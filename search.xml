<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Robust DRL学习笔记（1）</title>
    <url>/2022/02/23/Robust-DRL-1/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks</strong></p>
<span id="more"></span>
<p>Richard Cheng Richard M. Murray Joel W. Burdick  California Institute of Technology </p>
<p>AAAI2019</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>在无模型的DRL上通过基于模型信息构建的CBF控制器来修正RL决策（这里作者考虑了CBF“仅仅修正RL的决策动作”和“既考虑修正DRL，又考虑影响DRL的更新”两部分）。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>这篇文章的定位是<strong>可验证的“safe-RL”</strong>，作者认为RL在探索过程中侧重于最大化长期回报，这可能会在学习过程中探索到不安全的行为，这在实际系统中是不允许的</p>
<p>针对safe-RL研究而言，作者从model-free &amp; model-based角度进行现状分析：</p>
<p><strong>（1）model-free</strong>：文献都较老，主要方法有reward-shaping, policy optimization with constraints or teacher advice，并且无法保证在训练初期也能保证DRL决策的安全性。</p>
<p><strong>（2）model-based</strong>: Lyapunov-based or MPC or 备用控制器（实际上是引入了一个风险感知算法，在探索阶段 如果感知到该动作会引起较差的响应，则采用备用控制器的安全动作）。前两种方法没有解决探索和性能优化的问题，最后一种方法对探索过程限制过强</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><h4 id="模型获取（model-based-CBF的需求）"><a href="#模型获取（model-based-CBF的需求）" class="headerlink" title="模型获取（model-based CBF的需求）"></a>模型获取（model-based CBF的需求）</h4><p>给定$S \rightarrow S$的动力学方程：</p>
<script type="math/tex; mode=display">
s_{t+1} = f(s_t) + g(s_t)a_t + d(s_t)</script><p>其中f()代表与a无关的状态，g()中的状态与a有关，d()是不确定项（例如摩擦力和关节柔性）</p>
<p>由于CBF控制器是model-based的，但精准知道模型是很困难的，尤其是d()。故作者采用<strong>Gaussian Process(GP)</strong>估计d()。</p>
<p><img src="https://pic4.zhimg.com/v2-3c25a927c217f13a055794377635faaf_r.jpg" alt="Fig.1 高斯过程更新的图（随着数据点的扩充，高斯过程的均值会越来越接近真实值）"></p>
<p><em>Fig.1 高斯过程更新的图（随着数据点的扩充，高斯过程的均值会越来越接近真实值）</em></p>
<p>理论上我们用于估计d()的数据越多越好，但由于在高斯过程的$\mu$和$\sigma$的求解过程中涉及到求逆的操作，所以为了实时性考虑，实际上只取最新的1000个数据来训练GP model。</p>
<h4 id="CBF控制器设计"><a href="#CBF控制器设计" class="headerlink" title="CBF控制器设计"></a>CBF控制器设计</h4><p>定义安全集合$C$：</p>
<script type="math/tex; mode=display">
\mathcal{C}:\left\{s \in \mathbb{R}^{n}: h(s) \geq 0\right\}</script><p>CBF就是利用李雅普诺夫函数的推广，给出了<strong>在受控动力学下保证安全集C的前向不变性的充分条件</strong>。</p>
<p><strong>定义1</strong>：给定安全集合$C$，一个连续可微的函数$h(\cdot)$作为CBF函数，若存在$\eta \in [0, 1]$使得对所有$s_t$，均有：</p>
<script type="math/tex; mode=display">
\sup _{a_{t} \in A}\left[h\left(f\left(s_{t}\right)+g\left(s_{t}\right) a_{t}+d\left(s_{t}\right)\right)+(\eta-1) h\left(s_{t}\right)\right] \geq 0</script><p>$\eta$就是CBF区别于LF的主要一项，该变量指的是$a_t$将状态$s_{t+1}$推向安全集合的强度。$h(\cdot)$作者定义为一个基本的势垒函数$h=p^{T} s+q$（<em>though our methodology could support more general barrier functions. This  restriction means the set C is composed of intersecting half spaces (i.e.  polytopes).</em>这段不太理解）</p>
<p>代入$h(\cdot)$和$d(\cdot)$后，我们可以得到CBF控制器的目标：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left(a_{t}, \epsilon\right)=\underset{a_{t}, \epsilon}{\operatorname{argmin}} &\left\|a_{t}\right\|_{2}+K_{\epsilon} \epsilon \\
\text { s.t. } \quad & p^{T} f\left(s_{t}\right)+p^{T} g\left(s_{t}\right) a_{t}+p^{T} \mu_{d}\left(s_{t}\right)-\\
& k_{\delta}|p|^{T} \sigma_{d}\left(s_{t}\right)+q \geq(1-\eta) h\left(s_{t}\right)-\epsilon \\
& a_{\text {low }}^{i} \leq a_{t}^{i} \leq a_{\text {high }}^{i} \text { for } i=1, \ldots, M
\end{aligned}</script><p>其中$\epsilon$是松弛变量（<em>松弛变量的引入常常是为了便于在<strong>更大的可行域</strong>内求解。若为0，则收敛到原有状态，若大于零，则约束松弛。</em>）</p>
<p><strong>引理1</strong>：若对于任何$s \in C$，式（4）存在$\epsilon = 0$的解，则该控制器有（1-$\delta$）的概率使得安全集合C是前向不变的（即始终在安全集合内）。若存在$\epsilon=\epsilon^{\max }&gt;0$，则该控制器有（1-$\delta$）的概率使得$\mathcal{C}_{\epsilon}:\left\{s \in \mathbb{R}^{n}: h(s) \geq-\frac{\epsilon}{\eta}\right\}$是前向不变（即始终在限制集合内）。</p>
<h4 id="基于CBF的强化学习补偿控制"><a href="#基于CBF的强化学习补偿控制" class="headerlink" title="基于CBF的强化学习补偿控制"></a>基于CBF的强化学习补偿控制</h4><script type="math/tex; mode=display">
u_{k}(s)=u_{\theta_{k}}^{R L}(s)+u_{k}^{C B F}\left(s, u_{\theta_{k}}^{R L}\right)</script><p><img src="/2022/02/23/Robust-DRL-1/image-20220223093018926.png" alt="Fig2 CBF仅补偿RL控制动作"></p>
<p><em>Fig2 CBF仅补偿RL控制动作</em></p>
<p>CBF并不影响DRL的policy network的更新，只是做一个调控作用，和背景介绍里的“备用控制器”其实比较相似。</p>
<h4 id="基于CBF的强化学习引导控制"><a href="#基于CBF的强化学习引导控制" class="headerlink" title="基于CBF的强化学习引导控制"></a>基于CBF的强化学习引导控制</h4><p><img src="/2022/02/23/Robust-DRL-1/image-20220223093216487.png" alt="Fig3 策略迭代过程"></p>
<p><em>Fig3 策略迭代过程</em></p>
<p>如果仅利用CBF进行补偿，其控制结果如上图a，RL在第k个更新周期里的策略$\pi_{\theta_{k}}^{RL}$可能是很差的，但实际采取的策略是$\pi_{k}$，RL更新如果从$\pi_{k}$处进行更新（每次更新必然也只是在上次策略的一个小邻域内选择最接近$\pi_{opt}$的动作），$\pi_{\theta_{k+1}}^{RL}$会越来越好，逐渐摆脱CBF的补偿，这才是作者想要的效果。</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093310526.png" alt="Fig4 CBF指导强化学习更新"></p>
<p><em>这里利用历史CBF的求和是因为RL的policy更新如今也是依赖于之前的CBF控制值（在上一次的控制值基础上调节）</em></p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{k}(s)=& u_{\theta_{k}}^{R L}(s)+\sum_{j=0}^{k-1} u_{j}^{C B F}\left(s, u_{\theta_{0}}^{R L}, \ldots, u_{\theta_{j-1}}^{R L}\right) \\
&+u_{k}^{C B F}\left(s, u_{\theta_{k}}^{R L}+\sum_{j=0}^{k-1} u_{j}^{C B F}\right)
\end{aligned}</script><p>RL-CBF整体伪代码：</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093421936.png" alt="image-20220223093421936"></p>
<p><em>Tips：k是RL policy更新和GP更新的迭代次数</em></p>
<h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h3><p>倒立摆：</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093700054.png" alt="*左图表示即使在agent学习的过程中也不会违反安全限制 右图表示最终CBF在实际一个episode里基本不需要补偿*"></p>
<p><em>左图表示即使在agent学习的过程中也不会违反安全限制 右图表示最终CBF在实际一个episode里基本不需要补偿</em></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
</search>
