<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Robust DRL学习笔记（2）</title>
    <url>/2022/02/27/Robust%20DRL-2/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>Safe Model-based Reinforcement Learning with Stability Guarantees</strong><br><span id="more"></span></p>
<p>Felix Berkenkamp Matteo Turchetta Angela P. Schoellig  Angela P. Schoellig ETH Zurich</p>
<p>NIPS2017</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>作者为DRL策略学习和更新过程中提供了高概率安全保证，展示了如何从初始的安全策略开始，通过收集安全区域内的数据来扩展对吸引域的估计，并调整策略以增加吸引域和提高控制效果。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p><strong>safe-RL:</strong></p>
<p>model-free的方法主要集中于构建风险敏感型RL。例如：(1)在奖励函数中指定避险行为(2)安全探索MDP;</p>
<p>model-based 方法主要集中在状态约束方面的安全性，例如：对不确定的环境约束进行建模；</p>
<p><strong>稳定性证明:</strong></p>
<p>（1）对于known system的稳定性可以使用李亚普诺夫函数来验证</p>
<p>（2）通过对未知的系统部分利用DP进行估计建模，也可用李亚普诺夫函数来验证</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><p>研究对象的状态方程：</p>
<script type="math/tex; mode=display">
\mathbf{x}_{t+1}=f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=h\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)+g\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)</script><p>x：状态 u：控制动作 h()已知的系统动态 g()未知的模型误差</p>
<p>$\pi$：$X \rightarrow U$ 最终目标设定为x转为0</p>
<p>$r(x, u)$：作者用该变量定义该状态下执行该动作的<strong>cost</strong>（正），故$r(0,0)=0$</p>
<p>目标：从交互数据中安全地了解系统的状态方程（因为有unknown model），并调整策略使discounted costs变小。</p>
<p><strong>假设1（连续性）：</strong>h() g() $\pi$满足李普希茨连续条件，其李普希茨常数依次为$L_h,L_g,L_\pi$</p>
<p><em>李普希茨条件的通俗含义：存在一个实数L，使得对于函数 f（x）上的每对点，连接它们的线的斜率的绝对值不大于这个实数L。最小的L称为该函数的Lipschitz常数。（满足李普希茨条件的函数是一个比满足一致连续更光滑的连续函数）</em></p>
<p><strong>假设2（校准良好的模型）：</strong>假设对n个噪声数据的测量的均值和方差为$\mu_{n}$和$\sigma_{n}$，存在一个常数$\beta_{n}&gt;0$，使得对于所有的n&gt;0,x,u，以$1-\delta$的置信度满足：$\left|f(\mathbf{x}, \mathbf{u})-\mu_{n}(\mathbf{x}, \mathbf{u})\right|_{1} \leq \beta_{n} \sigma_{n}(\mathbf{x}, \mathbf{u})$</p>
<p>Lyapunov函数：$v(0)=0,v(x)&gt;0$</p>
<p><strong>理论1：</strong>$L,f$分别表示李雅普诺夫方差和满足李普希茨连续条件的系统状态方程，如果对于所有在吸引域$V(c)$中的状态x，均有$v(f(\mathbf{x}, \pi(\mathbf{x})))<v(\mathbf{x})$。其中，$\mathcal{V}(c)=\{\mathbf{x} \in \mathcal{x} \backslash\{\mathbf{0}\} \mid v(\mathbf{x}) \leq c\}, c>0$，则系统是稳定的。</v(\mathbf{x})$。其中，$\mathcal{V}(c)=\{\mathbf{x}></p>
<p><em>相当于利用李雅普诺夫的递减条件取代了复杂的稳定检验（状态收敛到 0的检验）</em></p>
<p>对于DRL，由于价值函数满足：$v(\mathbf{x})=r(\mathbf{x}, \pi(\mathbf{x}))+v(f(\mathbf{x}, \pi(\mathbf{x})) \leq v(f(\mathbf{x}, \pi(\mathbf{x})))$，本身就是一个很好的候选李雅普诺夫函数。</p>
<p><em>Q1：所以说 作者前面定义cost用错符号了？</em></p>
<p>初始安全策略：假设有个初始策略，可以使得原点在某个很小的状态集$S_0^x$内渐近稳定</p>
<p>定义$v(f(\cdot))$的上界和下界分别是$u_n(x)$和$l_n(x)$。则稳定性保证可以变为$v(f(\mathbf{x}, \mathbf{u})) \leq u_{n}(\mathbf{x})&lt;v(\mathbf{x}) \text { for all } \mathbf{x} \in \mathcal{V}(c)$</p>
<p>作者认为理论1是针对连续x而言的，实际验证起来很困难，所以扩展到了离散域。</p>
<p><strong>理论2：</strong>将状态空间$X$扩展到离散域$X_{\tau}$，离散域里的点和连续域的点之间的L1范数很小 $\left|\mathbf{x}-[\mathbf{x}]_{\tau}\right|_{1} \leq \tau$，对于所有既属于该离散域又属于吸引域的状态x而言，若满足$u_{n}(\mathbf{x}, \mathbf{u})&lt;v(\mathbf{x})-L_{\Delta v} \tau$，则该域内以$1-\delta$的置信度满足$v(f(\mathbf{x}, \pi(\mathbf{x})))&lt;v(\mathbf{x})$，其中，$L_{\Delta v}:=L_{v} L_{f}\left(L_{\pi}+1\right)+L_{v}$。</p>
<p>定义满足递减条件的所有状态-动作集合：</p>
<p>$\mathcal{D}_{n}=\left\{(\mathbf{x}, \mathbf{u}) \in \mathcal{X}_{\tau} \times \mathcal{U} \mid u_{n}(\mathbf{x}, \mathbf{u})-v(\mathbf{x})&lt;-L_{\Delta v} \tau\right\}$</p>
<p>RL策略目标是找到一个<strong>尽可能大的吸引域</strong>，使得其中所有的$(x, \pi_x)$均在$D$内：</p>
<script type="math/tex; mode=display">
\pi_{n}, c_{n}=\underset{\pi \in \Pi_{L}, c \in \mathbb{R}_{>0}}{\operatorname{argmax}} c, \quad \text { such that for all } \mathbf{x} \in \mathcal{V}(c) \cap \mathcal{X}_{\tau}:(\mathbf{x}, \pi(\mathbf{x})) \in \mathcal{D}_{n}</script><p><strong>理论3：</strong>假设实际的吸引域为$R_{\pi_n}$，则在策略$\pi_n$下，对于$\delta \in (0,1)$，至少有$（1-\delta）$的置信度认为$\mathcal{V}\left(c_{n}\right) \subseteq \mathcal{R}_{\pi_{n}}$ ($n&gt;0$)。 </p>
<p>除此之外，作者提出不仅需要将当前状态限制在$V(C)$中，对于该状态下采取控制动作后转移的次态也要在$V(C)$中：</p>
<script type="math/tex; mode=display">
\mathcal{S}_{n}=\bigcup_{\mathbf{z} \subset \mathcal{S_{n-1}}}\left\{\mathbf{z}^{\prime} \in \mathcal{V}\left(c_{n}\right) \cap \mathcal{X}_{\tau} \times \mathcal{U}_{\tau} \mid u_{n}(\mathbf{z})+L_{v} L_{f}\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|_{1} \leq c_{n}\right\}</script><p>其算法伪代码为：</p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226150644884.png" alt="伪代码"></p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151031162.png" alt="image-20220226151031162"></p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151118226.png" alt="image-20220226151118226"></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151236997.png" alt="image-20220226151236997"></p>
<p><em>吸引域本身在变大 同时更多状态落在吸引域中</em></p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151322970.png" alt="image-20220226151322970"></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust DRL学习笔记（3）</title>
    <url>/2022/03/05/Robust%20DRL-3/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>Reinforcement learning control of constrained dynamic systems with uniformly ultimate boundedness stability guarantee</strong><br><span id="more"></span></p>
<p>Minghao Han(HIT) Yuan Tian WeiPan(Delft University of Technology, Netherlands)</p>
<p>Automatica</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>为了解决RL控制的安全问题，传统的方法大多需要了解系统状态方程的全部或部分信息，作者提出了一种方法，不需要了解模型，只从数据出发就能完成UUB稳定性保证。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p><strong>UUB</strong></p>
<p>UUB保证：轨迹将在有限时间内进入稳定的邻域，并且此后不会从该集合中逃脱</p>
<p>该稳定性保证以往用于解决自适应动态规划（Adaptive Dynamic Programming），但不可避免的会用到模型结构和模型参数。</p>
<p><strong>RL with safety constraints</strong></p>
<p>​    <strong>（1）Safety Constrained Policy Optimization(CPO)</strong></p>
<p>该方法在一个初始安全的策略下使后续的安全约束得以满足，但受限于on-policy，采样效率低下。（无stability guarantees）</p>
<p>​    <strong>（2） Lyapunov-based approach</strong></p>
<p>通过构造李雅普诺夫函数求解带约束的控制问题。但仅保证设计的约束函数的累积保持在阈值以下。（无 stability guarantees）</p>
<p>​    <strong>（3）model-based RL in Lipschitz continuous deterministic nonlinear systems</strong></p>
<p>17年nips那篇，笔记2解读过。问题出在其安全性保证是针对状态空间子集中离散点而言的，故限制在低维、简单的系统中。</p>
<p>​    <strong>（4）model-based CBF controller</strong></p>
<p>19年aaai那篇，笔记1解读过。问题在于需要一个nominal model。</p>
<p>​    <strong>（5）expert knowledge+RL</strong></p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><p><strong>UUB稳定性相关概念</strong></p>
<p><strong>定义1：</strong>如果存在正的常数$b$和$\eta$，对于任意$\epsilon<b$，存在$T(\epsilon, \eta)$，使得：$\left\|s_{t_0}\right\| < \epsilon \rightarrow\left\|s_{t}\right\|<\eta, \forall t>t_{0}+T$，该系统被称为满足UUB，且以$\eta$为界限。若$\epsilon$可以任意大，则该系统满足全局UUB。</b$，存在$T(\epsilon,></p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227112538366.png" alt="image-20220227112538366" style="zoom: 50%;"></p>
<p>上述定义的安全约束仅仅限制在状态的l1范数上，但实际系统的约束函数不一定是采用该约束方法，故作者将定义1更新为：</p>
<p><strong>定义2：</strong>如果存在正的常数$b$和$\eta$，对于任意$\epsilon&lt;b$，存在$T(\epsilon, \eta)$，使得：$c_{\pi}(s_1) \leq \epsilon \Rightarrow c_{\pi}(s_t)\leq \eta, \forall t \geq T$，该系统被称为满足UUB，且以$c_\pi(\cdot)$为界限。若$\epsilon$可以任意大，则该系统满足全局UUB。</p>
<p>其中$c_{\pi}(s) \triangleq \mathbb{E}_{a \sim \pi} c(s, a)$。</p>
<p><strong>Constrained Markov Decision Process(CMDP)：</strong></p>
<p>$\max _{\pi} J(\pi)$ s.t. $\mathbb{E}_{s_{t}} c_{\pi}\left(s_{t}\right) \leq d, \forall t \in[1, \infty)$</p>
<p>$J(\pi)$是MDP的目标，加上了安全约束就变成了CMDP。</p>
<p>假设初始状态$S_1$（其概率密度函数定义为$\rho\left(S_{1}\right)$）是在安全域内，即：初始状态从$\Gamma$域内进行选择：$\Gamma \triangleq\left\{s \mid c_{\pi}(s) \leq b\right\}$</p>
<p><strong>安全集合</strong>：$\mathcal{B} \triangleq\left\{s \mid c_{\pi}(s)&lt;\eta\right\}$</p>
<p><strong>边缘集合</strong>：$\Delta \triangleq\left\{s \mid c_{\pi}(s) \geq \eta\right\}$</p>
<p><strong>假设1</strong>：$c(s, a)$非负且存在一个可积函数$g(s, a)$使得对于任意在状态空间和动作空间内的（s，a）而言，$c(s, a) &lt; g(s, a)$。</p>
<p><strong>理论1</strong>：如果存在一个李氏方程L(s)和正常数$\alpha_1,\alpha_2,\alpha_3,\eta$，使得：</p>
<script type="math/tex; mode=display">
\alpha_1 c_\pi(s) \leq L(s) \leq \alpha_2 c_\pi(s), \forall s \in S</script><p>且：（能量递减UUB条件）</p>
<script type="math/tex; mode=display">
\mathbb{E}_{s \sim \mu_{N}}\left(\mathbb{E}_{s^{\prime} \sim P_{\pi}} L\left(s^{\prime}\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)-L(s) \mathbb{1}_{\Delta}(s)\right) <-\alpha_{3} \mathbb{E}_{s \sim \mu_{N}} c_{\pi}(s) \mathbb{1}_{\Delta}(s)</script><p>其中$\mu_{N}(s)$代表了状态s在有限N个时间步长上的分布：$\mu_{N}(s) \doteq \frac{1}{N} \sum_{t=1}^{N} p(s \mid \rho, \pi, t)$</p>
<p>N是处于边缘集合的概率&gt;0的最后一个时刻</p>
<p>$\mathbb{1}_{\Delta}(s)$表示该状态是否在边缘集合内：$\mathbb{1}_{\Delta}(s)= \begin{cases}1 &amp; s \in \Delta \\ 0 &amp; s \notin \Delta\end{cases}$</p>
<p><em>作者的这种方法需要采用历史数据（$\mu_{N}(s)$），但并不需要了解模型信息</em></p>
<p><strong>注意1</strong>：如果系统满足UUB且以$\frac{\alpha_{2} b}{\alpha_{3}}+\eta&lt;d$为界限，则可以保证系统满足安全性约束。</p>
<hr>
<p><strong>Lyapunov-based Soft Actor–Critic(LSAC)——off-policy</strong></p>
<p>critic函数包括两部分，其一是标准SAC的目标（评估该（s,a）的价值），用$Q(s,a)$来评判；其二是评估UUB条件，用李雅普诺夫critic函数$L_c(s,a)$来评判。</p>
<p><em>为什么用$L_c(s,a)$而不是$L(s)$？</em></p>
<p><em>因为$L(s)$不能直接用于critic函数，critic更新的时候是对$\pi$而言的，而$L(s)$与$\pi$无关。</em></p>
<p>二者之间的关系为：$L(s)=\mathbb{E}_{a \sim \pi}L_c(s,a)$</p>
<p>$L_c(s,a)$使用一个参数化的全连接层DNN来描述。该网络的目的是让$L_c(s,a)$靠近$L_{target}(s,a)$，$L_{target}(s,a)$指的就是真实的满足李雅普诺夫条件的函数（作者选择了价值函数和约束函数来做候选，因为这俩都是非负的且一阶导是半负定的）：</p>
<script type="math/tex; mode=display">
J(\phi)=\mathbb{E}_{\mathcal{D}}\left[\frac{1}{2}\left(L_{c}(s, a)-L_{\text {target }}(s, a)\right)^{2}\right]</script><p>D是$\pi$下的transition：$\mathcal{D}=\{(s,a,s^{\prime},r,c)\}$，$J(\phi)$即后文伪代码中的$J(L_c)$。</p>
<p>同critic函数里$Q_{target}$的思路，$L_{target}$定义为：</p>
<script type="math/tex; mode=display">
L_{target}(s,a) = c(s,a) + \text{max}_{a^{\prime}} \gamma L_c^{\prime}(s^{\prime}, a^{\prime})</script><p>对于LSAC而言，一方面SAC在最大化Q（s,a）的同时希望策略熵尽可能大，另一方面安全约束希望能量递减尽可能大（后-前越小越好），故利用拉格朗日法，该多约束最优化问题可以转变为下式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J(\pi)=& \mathbb{E}_{\mathcal{D}}\left[-Q\left(s, f_{\theta}(s, \epsilon)\right)+\beta \log \pi_{\theta}\left(f_{\theta}(s, \epsilon) \mid s\right)\right] \\
+& \lambda \mathbb{E}_{\mathcal{D}_{\Delta}}\left[L_{c}\left(s^{\prime}, f_{\theta}\left(s^{\prime}, \epsilon\right)\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)\right.\\
-&\left.\left(L_{c}(s, a)-\alpha_{3} c\right) \mathbb{1}_{\Delta}(s)\right]
\end{aligned}</script><p>$\beta$和$\lambda$是正的拉格朗日乘数。$D_{\Delta}$指的是在$\mu_{N}$中的transition（edge buffer）$D$即常规的replay buffer。</p>
<p>对于$Q$、$\beta$和$\lambda$，其对应的损失函数依次为：</p>
<p>$J\left(Q_{i}\right)=\mathbb{E}_{\mathcal{D}} \frac{1}{2}\left[r+\gamma Q_{i}^{\prime}\left(s^{\prime}, f_{\theta}\left(s^{\prime}, \epsilon\right)\right)-Q_{i}(s, a)\right]^{2}, i \in\{1,2\}$</p>
<p>$J(\beta)=\beta \mathbb{E}_{\mathcal{D}}\left[\log \pi_{\theta}(a \mid s)+\mathcal{H}_{t}\right]$</p>
<p>$J(\lambda)=\lambda \mathbb{E}_{\mathcal{D}_{\Delta}}\left[L_{c}\left(s^{\prime}, f_{\theta}\left(s^{\prime}, \epsilon\right)\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)-\left(L_{c}(s, a)-\alpha_{3} c\right) \mathbb{1}_{\Delta}(s)\right]$</p>
<p>最终LSAC的伪代码如下：</p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227171257691.png" alt="image-20220227171257691" style="zoom: 67%;"></p>
<hr>
<p><strong>Lyapunov-based Constrained Policy Optimization (LCPO)——off-policy</strong></p>
<p>CPO+安全约束的原问题如下：</p>
<script type="math/tex; mode=display">
\theta_{k+1}=\underset{\theta}{\arg \max } \underset{\theta \sim_{\Delta \sim}}{\mathbb{E}} Q_{\pi_{k}}(s, a)</script><script type="math/tex; mode=display">
\begin{aligned}\text { s.t. } &\mathbb{E}_{s \sim \mathcal{D}_{\Delta} \atop a \sim \pi}\left[L\left(s^{\prime}\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)-\left(L(s)-\alpha_{3} c\right) \mathbb{1}_{\Delta}(s)\right] \leq 0 \\ &\mathbb{E}_{\mathcal{D}} D_{\mathrm{KL}}\left(\pi_{\theta} \mid \pi_{k}\right) \leq \delta \end{aligned}</script><p>作者使用泰勒二阶展开将其近似为如下优化问题：</p>
<script type="math/tex; mode=display">
\begin{aligned} \theta_{k+1}=& \arg \max _{\theta} g_{Q}^{\top}\left(\theta-\theta_{k}\right) \\ \text { s.t. } & g_{L}^{\top}\left(\theta-\theta_{k}\right)+h \leq 0 \\ & \frac{1}{2}\left(\theta-\theta_{k}\right)^{\top} H\left(\theta-\theta_{k}\right) \leq \delta \end{aligned}</script><p>$g_Q$、$g_L$分别是目标函数和安全约束函数相对于网络参数$\theta$的梯度。$h$是安全约束函数值，$H$是Fisher信息矩阵。</p>
<p>则针对上述凸优化问题，其对偶问题为：</p>
<script type="math/tex; mode=display">
\max _{\lambda, \beta \geq 0} \frac{1}{2 \beta}\left(\mathrm{g}_{Q}^{T} H^{-1} g_{Q}-2 \lambda \mathcal{Z}+\lambda^{2} \mathcal{N}\right)+\lambda h-\frac{\beta \delta}{2}</script><p>其中$\mathcal{Z}=g^T_QH^{-1}g_L$，$\mathcal{N}=g^{T}_LH^{-1}g_L$。</p>
<p>假设原问题有解且$\lambda$和$\beta$是对偶问题的解，则原问题的最优解为：</p>
<script type="math/tex; mode=display">
\theta_{k+1}=\theta_{k}+\frac{1}{\beta^{*}} H^{-1}\left(g_{Q}-\lambda^{*} g_{L}\right)</script><p>若原问题无解，则需要尽快恢复到安全策略：</p>
<script type="math/tex; mode=display">
\begin{aligned} \theta_{k+1} &=\arg \min _{\theta} g_{L}^{\top}\left(\theta-\theta_{k}\right)+h \\ \text { s.t. } & \frac{1}{2}\left(\theta-\theta_{k}\right)^{\top} H\left(\theta-\theta_{k}\right) \leq \delta \end{aligned}</script><p><em>相当于在小范围策略更新的情况下找到离约束最近的策略</em></p>
<p>该解为：</p>
<script type="math/tex; mode=display">
\theta^{*}=\theta_{k}-\sqrt{\frac{2 \delta}{g_{L}^{\top} H^{-1 g_{L}}}} H^{-} 1 g_{L}</script><p>​    LCPO的伪代码如下：</p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227193544818.png" alt="image-20220227193544818"></p>
<p>作者认为<strong>可以先用LSAC在仿真环境中学习初始策略，再将其应用到LCPO接受进一步的在线训练</strong>。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2022/03/05/Robust%20DRL-3/image-20220227200041471.png" alt="image-20220227200041471"></p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227200051457.png" alt="image-20220227200051457"></p>
<p><em>第一行是return 第二行是cost函数的值 第三行是超越安全界限的数量</em></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust DRL学习笔记（1）</title>
    <url>/2022/02/23/Robust-DRL-1/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks</strong></p>
<span id="more"></span>
<p>Richard Cheng Richard M. Murray Joel W. Burdick  California Institute of Technology </p>
<p>AAAI2019</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>在无模型的DRL上通过基于模型信息构建的CBF控制器来修正RL决策（这里作者考虑了CBF“仅仅修正RL的决策动作”和“既考虑修正DRL，又考虑影响DRL的更新”两部分）。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>这篇文章的定位是<strong>可验证的“safe-RL”</strong>，作者认为RL在探索过程中侧重于最大化长期回报，这可能会在学习过程中探索到不安全的行为，这在实际系统中是不允许的</p>
<p>针对safe-RL研究而言，作者从model-free &amp; model-based角度进行现状分析：</p>
<p><strong>（1）model-free</strong>：文献都较老，主要方法有reward-shaping, policy optimization with constraints or teacher advice，并且无法保证在训练初期也能保证DRL决策的安全性。</p>
<p><strong>（2）model-based</strong>: Lyapunov-based or MPC or 备用控制器（实际上是引入了一个风险感知算法，在探索阶段 如果感知到该动作会引起较差的响应，则采用备用控制器的安全动作）。前两种方法没有解决探索和性能优化的问题，最后一种方法对探索过程限制过强</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><h4 id="模型获取（model-based-CBF的需求）"><a href="#模型获取（model-based-CBF的需求）" class="headerlink" title="模型获取（model-based CBF的需求）"></a>模型获取（model-based CBF的需求）</h4><p>给定$S \rightarrow S$的动力学方程：</p>
<script type="math/tex; mode=display">
s_{t+1} = f(s_t) + g(s_t)a_t + d(s_t)</script><p>其中f()代表与a无关的状态，g()中的状态与a有关，d()是不确定项（例如摩擦力和关节柔性）</p>
<p>由于CBF控制器是model-based的，但精准知道模型是很困难的，尤其是d()。故作者采用<strong>Gaussian Process(GP)</strong>估计d()。</p>
<p><img src="https://pic4.zhimg.com/v2-3c25a927c217f13a055794377635faaf_r.jpg" alt="Fig.1 高斯过程更新的图（随着数据点的扩充，高斯过程的均值会越来越接近真实值）"></p>
<p><em>Fig.1 高斯过程更新的图（随着数据点的扩充，高斯过程的均值会越来越接近真实值）</em></p>
<p>理论上我们用于估计d()的数据越多越好，但由于在高斯过程的$\mu$和$\sigma$的求解过程中涉及到求逆的操作，所以为了实时性考虑，实际上只取最新的1000个数据来训练GP model。</p>
<h4 id="CBF控制器设计"><a href="#CBF控制器设计" class="headerlink" title="CBF控制器设计"></a>CBF控制器设计</h4><p>定义安全集合$C$：</p>
<script type="math/tex; mode=display">
\mathcal{C}:\left\{s \in \mathbb{R}^{n}: h(s) \geq 0\right\}</script><p>CBF就是利用李雅普诺夫函数的推广，给出了<strong>在受控动力学下保证安全集C的前向不变性的充分条件</strong>。</p>
<p><strong>定义1</strong>：给定安全集合$C$，一个连续可微的函数$h(\cdot)$作为CBF函数，若存在$\eta \in [0, 1]$使得对所有$s_t$，均有：</p>
<script type="math/tex; mode=display">
\sup _{a_{t} \in A}\left[h\left(f\left(s_{t}\right)+g\left(s_{t}\right) a_{t}+d\left(s_{t}\right)\right)+(\eta-1) h\left(s_{t}\right)\right] \geq 0</script><p>$\eta$就是CBF区别于LF的主要一项，该变量指的是$a_t$将状态$s_{t+1}$推向安全集合的强度。$h(\cdot)$作者定义为一个基本的势垒函数$h=p^{T} s+q$（<em>though our methodology could support more general barrier functions. This  restriction means the set C is composed of intersecting half spaces (i.e.  polytopes).</em>这段不太理解）</p>
<p>代入$h(\cdot)$和$d(\cdot)$后，我们可以得到CBF控制器的目标：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left(a_{t}, \epsilon\right)=\underset{a_{t}, \epsilon}{\operatorname{argmin}} &\left\|a_{t}\right\|_{2}+K_{\epsilon} \epsilon \\
\text { s.t. } \quad & p^{T} f\left(s_{t}\right)+p^{T} g\left(s_{t}\right) a_{t}+p^{T} \mu_{d}\left(s_{t}\right)-\\
& k_{\delta}|p|^{T} \sigma_{d}\left(s_{t}\right)+q \geq(1-\eta) h\left(s_{t}\right)-\epsilon \\
& a_{\text {low }}^{i} \leq a_{t}^{i} \leq a_{\text {high }}^{i} \text { for } i=1, \ldots, M
\end{aligned}</script><p>其中$\epsilon$是松弛变量（<em>松弛变量的引入常常是为了便于在<strong>更大的可行域</strong>内求解。若为0，则收敛到原有状态，若大于零，则约束松弛。</em>）</p>
<p><strong>引理1</strong>：若对于任何$s \in C$，式（4）存在$\epsilon = 0$的解，则该控制器有（1-$\delta$）的概率使得安全集合C是前向不变的（即始终在安全集合内）。若存在$\epsilon=\epsilon^{\max }&gt;0$，则该控制器有（1-$\delta$）的概率使得$\mathcal{C}_{\epsilon}:\left\{s \in \mathbb{R}^{n}: h(s) \geq-\frac{\epsilon}{\eta}\right\}$是前向不变（即始终在限制集合内）。</p>
<h4 id="基于CBF的强化学习补偿控制"><a href="#基于CBF的强化学习补偿控制" class="headerlink" title="基于CBF的强化学习补偿控制"></a>基于CBF的强化学习补偿控制</h4><script type="math/tex; mode=display">
u_{k}(s)=u_{\theta_{k}}^{R L}(s)+u_{k}^{C B F}\left(s, u_{\theta_{k}}^{R L}\right)</script><p><img src="/2022/02/23/Robust-DRL-1/image-20220223093018926.png" alt="Fig2 CBF仅补偿RL控制动作"></p>
<p><em>Fig2 CBF仅补偿RL控制动作</em></p>
<p>CBF并不影响DRL的policy network的更新，只是做一个调控作用，和背景介绍里的“备用控制器”其实比较相似。</p>
<h4 id="基于CBF的强化学习引导控制"><a href="#基于CBF的强化学习引导控制" class="headerlink" title="基于CBF的强化学习引导控制"></a>基于CBF的强化学习引导控制</h4><p><img src="/2022/02/23/Robust-DRL-1/image-20220223093216487.png" alt="Fig3 策略迭代过程"></p>
<p><em>Fig3 策略迭代过程</em></p>
<p>如果仅利用CBF进行补偿，其控制结果如上图a，RL在第k个更新周期里的策略$\pi_{\theta_{k}}^{RL}$可能是很差的，但实际采取的策略是$\pi_{k}$，RL更新如果从$\pi_{k}$处进行更新（每次更新必然也只是在上次策略的一个小邻域内选择最接近$\pi_{opt}$的动作），$\pi_{\theta_{k+1}}^{RL}$会越来越好，逐渐摆脱CBF的补偿，这才是作者想要的效果。</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093310526.png" alt="Fig4 CBF指导强化学习更新"></p>
<p><em>这里利用历史CBF的求和是因为RL的policy更新如今也是依赖于之前的CBF控制值（在上一次的控制值基础上调节）</em></p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{k}(s)=& u_{\theta_{k}}^{R L}(s)+\sum_{j=0}^{k-1} u_{j}^{C B F}\left(s, u_{\theta_{0}}^{R L}, \ldots, u_{\theta_{j-1}}^{R L}\right) \\
&+u_{k}^{C B F}\left(s, u_{\theta_{k}}^{R L}+\sum_{j=0}^{k-1} u_{j}^{C B F}\right)
\end{aligned}</script><p>RL-CBF整体伪代码：</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093421936.png" alt="image-20220223093421936"></p>
<p><em>Tips：k是RL policy更新和GP更新的迭代次数</em></p>
<h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h3><p>倒立摆：</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093700054.png" alt="*左图表示即使在agent学习的过程中也不会违反安全限制 右图表示最终CBF在实际一个episode里基本不需要补偿*"></p>
<p><em>左图表示即使在agent学习的过程中也不会违反安全限制 右图表示最终CBF在实际一个episode里基本不需要补偿</em></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust DRL学习笔记（4）</title>
    <url>/2022/03/11/Robust%20DRL-4/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>Constrained Policy Optimization</strong></p>
<span id="more"></span>
<p>Joshua Achiam David Held Aviv Tamar Pieter Abbeel (UC Berkeley)<br>       PMLR2017</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>第一个用于约束强化学习的<strong>通用（可以用于DRL）</strong>策略搜索算法，在每次迭代中<strong>保证</strong>近似约束满足。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>CMDP：增加了约束条件，限制了该MDP允许的策略集。</p>
<p><em>“A constrained Markov decision process (CMDP) is an MDP augmented with  constraints that restrict the set of allowable policies for that MDP”</em></p>
<p>已有利用启发式算法或是基于原始对偶方法的方法进行CMDP的研究，但无法保证每个学习过程中都能满足约束，存在保证了约束的工作，但假设条件太多，不通用。</p>
<p>其他与之前笔记重复内容不再赘述。</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><p><strong>CMDP：</strong></p>
<script type="math/tex; mode=display">
C_{i}: J_{C_{i}}(\pi)=\mathrm{E}\left[\sum_{t=0}^{\infty} \gamma^{t} C_{i}\left(s_{t}, a_{t}, s_{t+1}\right)\right]\\
\Pi_{C} \doteq\left\{\pi \in \Pi: \forall i, J_{C_{i}}(\pi) \leq d_{i}\right\}\\
\pi^{*}=\arg \max _{\pi \in \Pi_{C}} J(\pi)</script><p>$J(\pi)$就是折扣return的期望，$J_{C_i}(\pi)$是折扣约束的期望，目标是在满足$J_{C_i}(\pi)&lt;d_i$的情况下最大化$J(\pi)$。</p>
<p><strong>Constrained Policy Optimization(CPO):</strong></p>
<p>对于大型的MDP而言，若想得出最优策略是十分复杂的，因此利用神经网络等策略搜索算法逐步解决该问题：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\pi_{k+1}=\arg \max _{\pi \in \Pi_{\theta}}  J(\pi) \\
& \text { s.t. } D\left(\pi, \pi_{k}\right) \leq \delta
\end{aligned}</script><p>$D$是度量新旧策略差异的度量函数，每次策略更新是在很小的邻域内。</p>
<p>所以，对于CMDP而言，其更新可以看作：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{k+1}=&\arg \max _{\pi \in \Pi_{\theta}}  J(\pi) \\
\text { s.t. } & J_{C_{i}}(\pi) \leq d_{i} \quad i=1, \ldots, m \\
& D\left(\pi, \pi_{k}\right) \leq \delta .
\end{aligned}</script><p>如果完全按照上式进行更新，则也是计算复杂的，所以作者使用了 Trust Region Optimization(TRO)近似计算上式。</p>
<hr>
<p>作者首先证明了两个随机策略的returns的差的界限；之后证明了在每次更新时最坏情况下的性能下降保证。</p>
<p><strong>Policy Performance Bounds</strong></p>
<p>已有证明表示：</p>
<script type="math/tex; mode=display">
J\left(\pi^{\prime}\right)-J(\pi)=\frac{1}{1-\gamma} \underset{s \sim d^{\pi^{\prime} \atop} \atop a \sim \pi^{\prime}}{\mathrm{E}}\left[A^{\pi}(s, a)\right]</script><p>其中$d^{\pi}(s)=(1-\gamma) \sum_{t=0}^{\infty} \gamma^{t} P\left(s_{t}=s \mid \pi\right)$表示discounted 未来状态的分布。（该证明没懂）</p>
<p><strong>理论1：</strong>对于任意一个关于S的函数f，任意的策略$\pi$和$\pi^{\prime}$，定义估计与真实值的误差为$\delta_{f}\left(s, a, s^{\prime}\right) \doteq R\left(s, a, s^{\prime}\right)+\gamma f\left(s^{\prime}\right)-f(s)$，</p>
<p>同时定义：</p>
<script type="math/tex; mode=display">
\epsilon_{f}^{\pi^{\prime}} \doteq \max _{s}\left|\mathrm{E}_{a \sim \pi^{\prime}, s^{\prime} \sim P}\left[\delta_{f}\left(s, a, s^{\prime}\right)\right]\right|</script><script type="math/tex; mode=display">
L_{\pi, f}\left(\pi^{\prime}\right) \doteq \underset{s \sim d^{\pi} ,a \sim \pi  \atop s^{\prime} \sim P}{\mathrm{E}}\left[\left(\frac{\pi^{\prime}(a \mid s)}{\pi(a \mid s)}-1\right) \delta_{f}\left(s, a, s^{\prime}\right)\right]</script><script type="math/tex; mode=display">
D_{\pi, f}^{\pm}\left(\pi^{\prime}\right) \doteq \frac{L_{\pi, f}\left(\pi^{\prime}\right)}{1-\gamma} \pm \frac{2 \gamma \epsilon_{f}^{\pi^{\prime}}}{(1-\gamma)^{2}} \underset{s \sim d^{\pi}}{\mathrm{E}}\left[D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right]</script><p>其中$D_{TV}\left(\pi^{\prime} | \pi\right)[s]=(1 / 2) \sum_{a}\left|\pi^{\prime}(a \mid s)-\pi(a \mid s)\right|$。则以下界限存在：</p>
<script type="math/tex; mode=display">
D_{\pi, f}^{+}\left(\pi^{\prime}\right) \geq J\left(\pi^{\prime}\right)-J(\pi) \geq D_{\pi, f}^{-}\left(\pi^{\prime}\right)</script><p><strong>推论1：</strong>对任意策略$\pi^{\prime}$,$\pi$，有$\epsilon^{\pi^{\prime}} \doteq \max _{s}\left|\mathrm{E}_{a \sim \pi^{\prime}}\left[A^{\pi}(s, a)\right]\right|$ ，则以下界限存在：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&J\left(\pi^{\prime}\right)-J(\pi) \\
&\geq \frac{1}{1-\gamma} \underset{s \sim d^{\pi} \atop a \sim \pi^{\prime}}{\mathrm{E}}\left[A^{\pi}(s, a)-\frac{2 \gamma \epsilon^{\pi^{\prime}}}{1-\gamma} D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right]
\end{aligned}</script><p><strong>推论2：</strong>对任意策略$\pi^{\prime}$,$\pi$，任意代价函数$C_i$，且$\epsilon_{C_{i}}^{\pi^{\prime}} \doteq \max _{s}\left|\mathrm{E}_{a \sim \pi^{\prime}}\left[A_{C_{i}}^{\pi}(s, a)\right]\right|$，则以下界限成立：</p>
<script type="math/tex; mode=display">
J_{C_{i}}\left(\pi^{\prime}\right)-J_{C_{i}}(\pi)
\leq \frac{1}{1-\gamma} \underset{s \sim d^{\pi} \atop a \sim \pi^{\prime}}{\mathrm{E}}\left[A_{C_{i}}^{\pi}(s, a)+\frac{2 \gamma \epsilon_{C_{i}}^{\prime}}{1-\gamma} D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right]</script><p>上述理论和证明定义的界限都是针对<strong>TV散度</strong>而言的，而信域方法是利用<strong>KL散度</strong>，故通过Pinsker不等式将二者建立联系如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underset{s \sim d^{\pi}}{\mathrm{E}}\left[D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right] & \leq \underset{s \sim d^{\pi}}{\mathrm{E}}\left[\sqrt{\frac{1}{2} D_{K L}\left(\pi^{\prime} \| \pi\right)[s]}\right] \\
& \leq \sqrt{\frac{1}{2} \underset{s \sim d^{\pi}}{\mathrm{E}}\left[D_{K L}\left(\pi^{\prime} \| \pi\right)[s]\right]}
\end{aligned}</script><p>KL散度计算公式为：$K L(p | q)=\sum p(x) \log \frac{p(x)}{q(x)}$。</p>
<p><strong>推论3：</strong>将理论1和推论1、2中的TV散度换为KL散度即可得到一组和KL散度相关的边界条件。</p>
<p><strong>Trust Region Methods</strong></p>
<p>信赖域算法的策略更新采取以下形式：</p>
<script type="math/tex; mode=display">
\begin{array}{r}
\pi_{k+1}=\arg \max _{\pi \in \Pi_{\theta}} \underset{s \sim d^{\pi} k}{\mathrm{E}}\left[A^{\pi_{k}}(s, a)\right] \\
\text { s.t. } \bar{D}_{K L}\left(\pi \| \pi_{k}\right) \leq \delta
\end{array}</script><p>其中$D_{KL}$的约束被称为信赖域（策略更新前后的策略性能之差可用优势函数的折扣累加期望）。</p>
<p><strong>命题1（性能保证）：</strong>假设$\pi_{k}$按照式（12）进行更新，则策略更新前后的性能差异有：</p>
<script type="math/tex; mode=display">
J\left(\pi_{k+1}\right)-J\left(\pi_{k}\right) \geq \frac{-\sqrt{2 \delta} \gamma \epsilon^{\pi_{k+1}}}{(1-\gamma)^{2}}
\\
where \quad\epsilon^{\pi_{k+1}}=\max _{s}\left|\mathrm{E}_{a \sim \pi_{k+1}}\left[A^{\pi_{k}}(s, a)\right]\right|.</script><p>考虑信赖域的约束条件和安全约束条件，结合推论1、2、3，有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\pi_{k+1}=\arg \max _{\pi \in \Pi_{\theta}} \underset{s \sim d^{\pi_{k}} \atop a \sim \pi}{\mathrm{E}}\left[A^{\pi_{k}}(s, a)\right]-\alpha_{k} \sqrt{\bar{D}_{K L}\left(\pi \| \pi_{k}\right)} \\
&\text { s.t. } J_{C_{i}}\left(\pi_{k}\right)+\underset{s \sim d^{\pi_{k}} \atop a \sim \pi}{\mathrm{E}}\left[\frac{A_{C_{i}}^{\pi_{k}}(s, a)}{1-\gamma}\right]+\beta_{k}^{i} \sqrt{\bar{D}_{K L}\left(\pi \| \pi_{k}\right)} \leq d_{i}
\end{aligned}</script><p>基于此，作者提出<strong>CPO</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{k+1}=& \arg \max _{\pi \in \Pi_{\theta}} \underset{s \sim d^{\pi_{k}}}{\mathrm{E}}\left[A^{\pi_{k}}(s, a)\right] \\
\text { s.t. } &J_{C_{i}}\left(\pi_{k}\right)+\frac{1}{1-\gamma} \underset{s \sim d^{\pi_{k}}}{\mathrm{E}}\left[A_{C_{i}}^{\pi_{k}}(s, a)\right] \leq d_{i} \quad \forall i \\
& \bar{D}_{K L}\left(\pi \| \pi_{k}\right) \leq \delta .
\end{aligned}</script><p>​    因为这是一个信赖域方法，所以继承了命题1的性能保证。此外，根据推论2和3，又<strong>近似满足</strong>安全约束。故最终达到了满足安全约束的性能保证。</p>
<p><strong>命题2（CPO更新的最坏情况）：</strong>安全约束$C_i-return$的上界为：    </p>
<script type="math/tex; mode=display">
J_{C_{i}}\left(\pi_{k+1}\right) \leq d_{i}+\frac{\sqrt{2 \delta} \gamma \epsilon_{C_{i}}^{\pi_{k+1}}}{(1-\gamma)^{2}}
\\
where \quad\epsilon_{C_{i}}^{\pi_{k+1}}=\max _{s}\left|\mathrm{E}_{a \sim \pi_{k+1}}\left[A_{C_{i}}^{\pi_{k}}(s, a)\right]\right|.</script><p><strong>CPO如何求解？</strong></p>
<p>定义$c_{i}=J_{C_i}(\pi_k)-d$，则公式（15）可以近似为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{k+1}=\arg \max _{\theta} & g^{T}\left(\theta-\theta_{k}\right) \\
\text { s.t. } \quad & c_{i}+b_{i}^{T}\left(\theta-\theta_{k}\right) \leq 0 \quad i=1, \ldots, m \\
& \frac{1}{2}\left(\theta-\theta_{k}\right)^{T} H\left(\theta-\theta_{k}\right) \leq \delta
\end{aligned}</script><p>$H$是费希尔信息矩阵，总是半正定的且式（17）优化问题是<strong>凸的</strong>（上式是一个带线性和二次约束的线性目标函数凸优化问题(LQCLP)），故可以用拉格朗日对偶方法求解：</p>
<script type="math/tex; mode=display">
\max _{\lambda \geq 0 \atop \nu \succeq 0} \frac{-1}{2 \lambda}\left(g^{T} H^{-1} g-2 r^{T} \nu+\nu^{T} S \nu\right)+\nu^{T} c-\frac{\lambda \delta}{2}</script><p>其中$r \doteq g^{T} H^{-1} B, S \doteq B^{T} H^{-1} B$。若$\lambda^<em>，v^</em>$是式（18）的解，则原问题的解为：</p>
<script type="math/tex; mode=display">
\theta^{*}=\theta_{k}+\frac{1}{\lambda^{*}} H^{-1}\left(g-B \nu^{*}\right)</script><p>由于近似是存在误差的，有时利用式（17）是安全有效的，但对于不可行的情况，作者利用下式更新：</p>
<script type="math/tex; mode=display">
\theta^{*}=\theta_{k}-\sqrt{\frac{2 \delta}{b^{T} H^{-1} b}} H^{-1} b</script><p>该更新用于信赖域和约束域的交集为空集时采取的更新措施。</p>
<p>伪代码：</p>
<p><img src="/2022/03/11/Robust%20DRL-4/image-20220307152139547.png" alt="image-20220307152139547"></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2022/03/11/Robust%20DRL-4/image-20220307152526198.png" alt="image-20220307152526198"></p>
<p><em>CPO保持在限制内（感觉都在限制值那块聚集？）更多 TRPO无效</em></p>
<p><img src="/2022/03/11/Robust%20DRL-4/image-20220307152950704.png" alt="image-20220307152950704"></p>
<p><em>CPO+cost shaping 更有效</em></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Barrier Function学习笔记</title>
    <url>/2022/05/01/QP-1/</url>
    <content><![CDATA[<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>Barrier Function最先用于优化问题，现在也有研究利用障碍函数将约束最优控制方法转化为无约束最优控制。</p>
<span id="more"></span>
<p>通常使用与集合C相关联的障碍函数的两个概念：</p>
<p><strong>（1）Reciprocal Barrier Function(RBF)</strong></p>
<p>$x$在集合$C$的边界处，障碍函数$B(x)$无界：$B(x) \rightarrow \infty$ 当 $x \rightarrow \partial \mathcal{C}$</p>
<p><strong>（2）Zeroing Barrier Function(ZBF)</strong></p>
<p>$x$在集合$C$的边界处，障碍函数$B(x)$为0：$h(x) \rightarrow 0$ 当 $x \rightarrow \partial \mathcal{C}$</p>
<p>对于这两种情况，若$B$或$h$满足Lyapunov-like条件，则在集合C内的前向不变性是可以保证的。</p>
<h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><p>$\operatorname{Int}(\mathcal{C})$ 和$\partial \mathcal{C}$分别表示集合$C$的内部和边界。</p>
<p>定义一个中心在0处的开球： $B_{\varepsilon}=\{x \in$ $\left.\mathbb{R}^{n} \mid|x|&lt;\varepsilon\right\}$, $\varepsilon \in \mathbb{R}^{+}$</p>
<p>若一个连续函数定义域和值域均为非负数，且严格单调递增，在0处取0，则该函数属于$K$类：</p>
<p>$\beta_{1}:[0, a) \rightarrow[0, \infty)$ , $a&gt;0$ 且 $\beta_{1}(0)=0$.</p>
<p>若一个连续函数定义域和值域可以取负数，且严格单调递增，在0处取0，则该函数属于扩展$K$类：</p>
<p>$\alpha:(-b, a) \rightarrow(-\infty, \infty)$， $a, b&gt;0$ 且$\alpha(0)=0$.</p>
<p>sup：上确界, inf：下确界</p>
<h3 id="RBF-amp-ZBF"><a href="#RBF-amp-ZBF" class="headerlink" title="RBF &amp; ZBF"></a>RBF &amp; ZBF</h3><p>考虑一个非线性系统：</p>
<script type="math/tex; mode=display">
\begin{equation}
\dot{x}=f(x)
\end{equation}</script><h4 id="RBF"><a href="#RBF" class="headerlink" title="RBF"></a>RBF</h4><p>集合C满足：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\mathcal{C} &=\left\{x \in \mathbb{R}^{n}: h(x) \geq 0\right\} \\
\partial \mathcal{C} &=\left\{x \in \mathbb{R}^{n}: h(x)=0\right\} \\
\operatorname{Int}(\mathcal{C}) &=\left\{x \in \mathbb{R}^{n}: h(x)>0\right\}
\end{aligned}
\end{equation}</script><p>满足Lyapunov-like条件的BF候选函数有：</p>
<script type="math/tex; mode=display">
\begin{equation}
B(x)=-\log \left(\frac{h(x)}{1+h(x)}\right)
\end{equation}</script><p>该函数满足以下两条基本条件：</p>
<script type="math/tex; mode=display">
\begin{equation}
\inf _{x \in \operatorname{Int}(\mathcal{C})} B(x) \geq 0, \quad \lim _{x \rightarrow \partial \mathcal{C}} B(x)=\infty
\end{equation}</script><p>那么问题来了，设计什么条件才能使$B(x)$在$\operatorname{Int}(\mathcal{C})$内是前向不变的呢？</p>
<p>理论上应该设计条件$\dot{B} \leq 0$，但这样是没必要的，我们只需要在越接近$C$的边界处设置该标准即可。</p>
<p>故设计条件：</p>
<script type="math/tex; mode=display">
\begin{equation}
\dot{B} \leq \frac{\gamma}{B}
\end{equation}</script><p>使得$B$在远离边界时可以增大，越接近边界增大速率越接近于0（因为B在边界处趋于无穷，上述条件使其导数趋于0）。</p>
<p>除此之外，另一个可能的候选函数为：</p>
<script type="math/tex; mode=display">
\begin{equation}
B(x) = \frac{1}{h(x)}
\end{equation}</script><p>RBF更一般的形式：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
&\frac{1}{\alpha_{1}(h(x))} \leq B(x) \leq \frac{1}{\alpha_{2}(h(x))}\\
&L_{f} B(x) \leq \alpha_{3}(h(x)) .
\end{aligned}
\end{equation}</script><p>其中，α1，α2，α3是上述定义的K类函数，$h(x)$是连续可微函数。</p>
<p><strong>定理1</strong>：给定动态系统（1）和一个由连续可微函数$h$定义的集合$C$，如果存在$B$是一个RBF，那么在$\operatorname{Int}(\mathcal{C})$上是前向不变的。</p>
<h4 id="ZBF"><a href="#ZBF" class="headerlink" title="ZBF"></a>ZBF</h4><p>在RBF定义中，当其自变量接近C的边界时，RBF趋向于无穷大，然而，当考虑real-time/embedded implementations（不理解）时，这可能是不希望被达到的。故设计了ZBF。</p>
<p>一般的ZBF形式如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
L_{f} h(x) \geq-\alpha(h(x))
\end{equation}</script><p>其中$\alpha$是扩展K函数。</p>
<p><em>注意：将$h$定义在一个比$C$ 大的集合$D$上可以考虑模型扰动的影响。</em></p>
<p>和定理1类似，ZBF的存在可以保证$\operatorname{Int}(\mathcal C)$的前向不变性。</p>
<p>且即使初始位置在集合$\mathcal C$之外，也有$x$渐进收敛于$\mathcal C$。</p>
<p>互斥控制屏障函数。</p>
<p><strong>二者与前向不变性的关系</strong>：</p>
<p>ZBF和RBF均为$\operatorname{int}(\mathcal C)$前向不变性的充分条件</p>
<p><img src="/2022/05/01/QP-1/image-20220423164317970.png" alt="image-20220423164317970"></p>
<h3 id="利用RBF-amp-ZBF设计CBF"><a href="#利用RBF-amp-ZBF设计CBF" class="headerlink" title="利用RBF&amp;ZBF设计CBF"></a>利用RBF&amp;ZBF设计CBF</h3><p>虽然障碍函数是验证集合不变性的重要工具，但它们不能直接用于设计强制不变性的控制器。</p>
<h4 id="RCBF"><a href="#RCBF" class="headerlink" title="RCBF"></a>RCBF</h4><p>考虑一个仿射控制系统：</p>
<script type="math/tex; mode=display">
\begin{equation}
\dot{x}=f(x)+g(x) u
\end{equation}</script><p>$f$ 和 $g$ 是局部李普希茨连续，$x \in \mathbb{R}^{n}$ 且$u \in U \subset \mathbb{R}^{m}$，其中$U$定义为：</p>
<script type="math/tex; mode=display">
\begin{equation}
U=\left\{u \in \mathbb{R}^{m} \mid A_{0} u \leq b_{0}\right\}
\end{equation}</script><p>其中$A_0$是$p \times m$矩阵，$b_0$是$p \times 1$向量，在无约束的系统动态下，$\operatorname{Int}(\mathcal{C})$不是前向不变的。</p>
<p>那么根据RCBF的定理1，若存在$\mathcal K$类函数α1，α2，α3，使得：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{gathered}
\frac{1}{\alpha_{1}(h(x))} \leq B(x) \leq \frac{1}{\alpha_{2}(h(x))} \\
\inf _{u \in U}\left[L_{f} B(x)+L_{g} B(x) u-\alpha_{3}(h(x))\right] \leq 0
\end{gathered}
\end{equation}</script><p>即若我们选取的控制动作u满足下述集合：</p>
<script type="math/tex; mode=display">
\begin{equation}
K_{\mathrm{rcbf}}(x)=\left\{u \in U: L_{f} B(x)+L_{g} B(x) u-\alpha_{3}(h(x)) \leq 0\right\}
\end{equation}</script><p>则可以保证C的前向不变性，其中，$h(x)$是之前公式3定义的连续可微函数。</p>
<h4 id="ZCBF"><a href="#ZCBF" class="headerlink" title="ZCBF"></a>ZCBF</h4><p>同理，若存在扩展$\mathcal K$类函数$\alpha$，使得：</p>
<script type="math/tex; mode=display">
\begin{equation}
\sup _{u \in U}\left[L_{f} h(x)+L_{g} h(x) u+\alpha(h(x))\right] \geq 0, \forall x \in \mathcal{D}
\end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
K_{\mathrm{zcbf}}(x)=\left\{u \in U: L_{f} h(x)+L_{g} h(x) u+\alpha(h(x)) \geq 0\right\}
\end{equation}</script><p>其中，$D$是比$C$大一些的集合。</p>
<h3 id="CLF-CBF-QP"><a href="#CLF-CBF-QP" class="headerlink" title="CLF-CBF-QP"></a>CLF-CBF-QP</h3><p>目的是利用QP来协调控制效果和安全约束。</p>
<p>首先写出系统动态：</p>
<script type="math/tex; mode=display">
\begin{equation}
\left(\begin{array}{c}
\dot{x}_{1} \\
\dot{x}_{2}
\end{array}\right)=\underbrace{\left(\begin{array}{c}
f_{1}\left(x_{1}, x_{2}\right) \\
f_{2}\left(x_{1}, x_{2}\right)
\end{array}\right)}_{f(x)}+\underbrace{\left(\begin{array}{c}
g_{1}\left(x_{1}, x_{2}\right) \\
0
\end{array}\right)}_{g(x)} u
\end{equation}</script><p>可以看出状态$x_1$是受控状态，$x_2$是不受控状态。</p>
<p>指数稳定控制李亚普诺夫函数<strong>Exponentially stabilizing Control Lyapunov Function(CLF)</strong>的一般形式如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{gathered}
c_{1}\left\|x_{1}\right\|^{2} \leq V(x) \leq c_{2}\left\|x_{1}\right\|^{2} \\
\inf _{u \in U}\left[L_{f} V(x)+L_{g} V(x) u+c_{3} V(x)\right] \leq 0
\end{gathered}
\end{equation}</script><p>因此，我们可以利用CLF作为控制性能目标的约束，CBF作为安全目标的约束设立QP，通过放松CLF条件（式17）的约束，并调整松弛参数上的权重，QP可以在保证安全性的情况下协调性能和安全性之间的权衡。具体而言：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\mathbf{u}^{*}(x)=& \underset{\mathbf{u}=(u, \delta) \in \mathbb{R}^{m} \times \mathbb{R}}{\operatorname{argmin}} \frac{1}{2} \mathbf{u}^{\top} H(x) \mathbf{u}+F(x)^{\top} \mathbf{u} \\
\text { s.t. } & L_{f} V(x)+L_{g} V(x) u+c_{3} V(x)-\delta \leq 0, \\
& L_{f} B(x)+L_{g} B(x) u-\alpha(h(x)) \leq 0,
\end{aligned}
\end{equation}</script><p>其中 $c_{3}&gt;0$ 且是常数， $\alpha$ 属于类别 $\mathcal{K}, H(x) \in$ $\mathbb{R}^{(m+1) \times(m+1)}$ 正定， $F(x) \in \mathbb{R}^{m+1}$。</p>
<h3 id="Eg-Lane-Keeping-Via-QPs"><a href="#Eg-Lane-Keeping-Via-QPs" class="headerlink" title="Eg: Lane Keeping Via QPs"></a>Eg: Lane Keeping Via QPs</h3><p>使用基于CBF的QP来考虑车道保持(LK)问题，该QP旨在使车辆在可能弯曲的车道上保持“中心”。假设车辆具有恒定的纵向速度，这样我们只需要关注车辆的横向运动。</p>
<p>安全约束：</p>
<p>$|y| \leq y_{\max }$，车辆距车道中心的位移小于给定的常量$y_{max}$</p>
<p>$|\ddot{y}| \leq a_{\max }$，为了驾驶员的舒适性，侧向加速度需要有界</p>
<p>要结合CBF，主要需要找两个函数，一个$h_F(x)$满足式3，一个BF函数，可选为RBF/ZBF如上式4、7、12等。</p>
<p>例如此题选择$h_F(x)$：</p>
<script type="math/tex; mode=display">
\begin{equation}
h_{F}(x)=\left(y_{\max }-\operatorname{sgn}(\dot{y}) y\right)-\frac{1}{2} \frac{\dot{y}^{2}}{a_{\max }}
\end{equation}</script><p>则有$\mathcal{C}_{F}:=\left\{x \mid h_{F}(x) \geq 0\right\}$。</p>
<p>选择$B_{F}(x)$：</p>
<script type="math/tex; mode=display">
\begin{equation}
B_{F}(x)=\frac{1}{h_F(x)}
\end{equation}</script><p>则对于$x \in \operatorname{Int}\left(\mathcal{C}_{F}\right)$，若总存在控制动作$u$：</p>
<script type="math/tex; mode=display">
\begin{equation}
L_{f} B_{F}(x)+L_{g} B_{F}(x) u-\frac{\gamma}{B_{F}(x)} \leq 0
\end{equation}</script><p>则$\operatorname{Int}\left(\mathcal{C}_{F}\right)$是前向不变的。</p>
<p>这样，我们就可以利用QP去解出最优的$u$。</p>
<p>结果：</p>
<p><img src="/2022/05/01/QP-1/image-20220423194127174.png" alt="image-20220423194127174"></p>
<p>可以看到无论是横行位移还是横向加速度都在限定范围内。</p>
]]></content>
      <categories>
        <category>数学基础</category>
      </categories>
      <tags>
        <tag>CBF</tag>
      </tags>
  </entry>
</search>
