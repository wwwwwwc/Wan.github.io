<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Robust DRL学习笔记（2）</title>
    <url>/2022/02/27/Robust%20DRL-2/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>Safe Model-based Reinforcement Learning with Stability Guarantees</strong><br><span id="more"></span></p>
<p>Felix Berkenkamp Matteo Turchetta Angela P. Schoellig  Angela P. Schoellig ETH Zurich</p>
<p>NIPS2017</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>作者为DRL策略学习和更新过程中提供了高概率安全保证，展示了如何从初始的安全策略开始，通过收集安全区域内的数据来扩展对吸引域的估计，并调整策略以增加吸引域和提高控制效果。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p><strong>safe-RL:</strong></p>
<p>model-free的方法主要集中于构建风险敏感型RL。例如：(1)在奖励函数中指定避险行为(2)安全探索MDP;</p>
<p>model-based 方法主要集中在状态约束方面的安全性，例如：对不确定的环境约束进行建模；</p>
<p><strong>稳定性证明:</strong></p>
<p>（1）对于known system的稳定性可以使用李亚普诺夫函数来验证</p>
<p>（2）通过对未知的系统部分利用DP进行估计建模，也可用李亚普诺夫函数来验证</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><p>研究对象的状态方程：</p>
<script type="math/tex; mode=display">
\mathbf{x}_{t+1}=f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=h\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)+g\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)</script><p>x：状态 u：控制动作 h()已知的系统动态 g()未知的模型误差</p>
<p>$\pi$：$X \rightarrow U$ 最终目标设定为x转为0</p>
<p>$r(x, u)$：作者用该变量定义该状态下执行该动作的<strong>cost</strong>（正），故$r(0,0)=0$</p>
<p>目标：从交互数据中安全地了解系统的状态方程（因为有unknown model），并调整策略使discounted costs变小。</p>
<p><strong>假设1（连续性）：</strong>h() g() $\pi$满足李普希茨连续条件，其李普希茨常数依次为$L_h,L_g,L_\pi$</p>
<p><em>李普希茨条件的通俗含义：存在一个实数L，使得对于函数 f（x）上的每对点，连接它们的线的斜率的绝对值不大于这个实数L。最小的L称为该函数的Lipschitz常数。（满足李普希茨条件的函数是一个比满足一致连续更光滑的连续函数）</em></p>
<p><strong>假设2（校准良好的模型）：</strong>假设对n个噪声数据的测量的均值和方差为$\mu_{n}$和$\sigma_{n}$，存在一个常数$\beta_{n}&gt;0$，使得对于所有的n&gt;0,x,u，以$1-\delta$的置信度满足：$\left|f(\mathbf{x}, \mathbf{u})-\mu_{n}(\mathbf{x}, \mathbf{u})\right|_{1} \leq \beta_{n} \sigma_{n}(\mathbf{x}, \mathbf{u})$</p>
<p>Lyapunov函数：$v(0)=0,v(x)&gt;0$</p>
<p><strong>理论1：</strong>$L,f$分别表示李雅普诺夫方差和满足李普希茨连续条件的系统状态方程，如果对于所有在吸引域$V(c)$中的状态x，均有$v(f(\mathbf{x}, \pi(\mathbf{x})))<v(\mathbf{x})$。其中，$\mathcal{V}(c)=\{\mathbf{x} \in \mathcal{x} \backslash\{\mathbf{0}\} \mid v(\mathbf{x}) \leq c\}, c>0$，则系统是稳定的。</v(\mathbf{x})$。其中，$\mathcal{V}(c)=\{\mathbf{x}></p>
<p><em>相当于利用李雅普诺夫的递减条件取代了复杂的稳定检验（状态收敛到 0的检验）</em></p>
<p>对于DRL，由于价值函数满足：$v(\mathbf{x})=r(\mathbf{x}, \pi(\mathbf{x}))+v(f(\mathbf{x}, \pi(\mathbf{x})) \leq v(f(\mathbf{x}, \pi(\mathbf{x})))$，本身就是一个很好的候选李雅普诺夫函数。</p>
<p><em>Q1：所以说 作者前面定义cost用错符号了？</em></p>
<p>初始安全策略：假设有个初始策略，可以使得原点在某个很小的状态集$S_0^x$内渐近稳定</p>
<p>定义$v(f(\cdot))$的上界和下界分别是$u_n(x)$和$l_n(x)$。则稳定性保证可以变为$v(f(\mathbf{x}, \mathbf{u})) \leq u_{n}(\mathbf{x})&lt;v(\mathbf{x}) \text { for all } \mathbf{x} \in \mathcal{V}(c)$</p>
<p>作者认为理论1是针对连续x而言的，实际验证起来很困难，所以扩展到了离散域。</p>
<p><strong>理论2：</strong>将状态空间$X$扩展到离散域$X_{\tau}$，离散域里的点和连续域的点之间的L1范数很小 $\left|\mathbf{x}-[\mathbf{x}]_{\tau}\right|_{1} \leq \tau$，对于所有既属于该离散域又属于吸引域的状态x而言，若满足$u_{n}(\mathbf{x}, \mathbf{u})&lt;v(\mathbf{x})-L_{\Delta v} \tau$，则该域内以$1-\delta$的置信度满足$v(f(\mathbf{x}, \pi(\mathbf{x})))&lt;v(\mathbf{x})$，其中，$L_{\Delta v}:=L_{v} L_{f}\left(L_{\pi}+1\right)+L_{v}$。</p>
<p>定义满足递减条件的所有状态-动作集合：</p>
<p>$\mathcal{D}_{n}=\left\{(\mathbf{x}, \mathbf{u}) \in \mathcal{X}_{\tau} \times \mathcal{U} \mid u_{n}(\mathbf{x}, \mathbf{u})-v(\mathbf{x})&lt;-L_{\Delta v} \tau\right\}$</p>
<p>RL策略目标是找到一个<strong>尽可能大的吸引域</strong>，使得其中所有的$(x, \pi_x)$均在$D$内：</p>
<script type="math/tex; mode=display">
\pi_{n}, c_{n}=\underset{\pi \in \Pi_{L}, c \in \mathbb{R}_{>0}}{\operatorname{argmax}} c, \quad \text { such that for all } \mathbf{x} \in \mathcal{V}(c) \cap \mathcal{X}_{\tau}:(\mathbf{x}, \pi(\mathbf{x})) \in \mathcal{D}_{n}</script><p><strong>理论3：</strong>假设实际的吸引域为$R_{\pi_n}$，则在策略$\pi_n$下，对于$\delta \in (0,1)$，至少有$（1-\delta）$的置信度认为$\mathcal{V}\left(c_{n}\right) \subseteq \mathcal{R}_{\pi_{n}}$ ($n&gt;0$)。 </p>
<p>除此之外，作者提出不仅需要将当前状态限制在$V(C)$中，对于该状态下采取控制动作后转移的次态也要在$V(C)$中：</p>
<script type="math/tex; mode=display">
\mathcal{S}_{n}=\bigcup_{\mathbf{z} \subset \mathcal{S_{n-1}}}\left\{\mathbf{z}^{\prime} \in \mathcal{V}\left(c_{n}\right) \cap \mathcal{X}_{\tau} \times \mathcal{U}_{\tau} \mid u_{n}(\mathbf{z})+L_{v} L_{f}\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|_{1} \leq c_{n}\right\}</script><p>其算法伪代码为：</p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226150644884.png" alt="伪代码"></p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151031162.png" alt="image-20220226151031162"></p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151118226.png" alt="image-20220226151118226"></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151236997.png" alt="image-20220226151236997"></p>
<p><em>吸引域本身在变大 同时更多状态落在吸引域中</em></p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151322970.png" alt="image-20220226151322970"></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust DRL学习笔记（3）</title>
    <url>/2022/03/05/Robust%20DRL-3/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>Reinforcement learning control of constrained dynamic systems with uniformly ultimate boundedness stability guarantee</strong><br><span id="more"></span></p>
<p>Minghao Han(HIT) Yuan Tian WeiPan(Delft University of Technology, Netherlands)</p>
<p>Automatica</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>为了解决RL控制的安全问题，传统的方法大多需要了解系统状态方程的全部或部分信息，作者提出了一种方法，不需要了解模型，只从数据出发就能完成UUB稳定性保证。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p><strong>UUB</strong></p>
<p>UUB保证：轨迹将在有限时间内进入稳定的邻域，并且此后不会从该集合中逃脱</p>
<p>该稳定性保证以往用于解决自适应动态规划（Adaptive Dynamic Programming），但不可避免的会用到模型结构和模型参数。</p>
<p><strong>RL with safety constraints</strong></p>
<p>​    <strong>（1）Safety Constrained Policy Optimization(CPO)</strong></p>
<p>该方法在一个初始安全的策略下使后续的安全约束得以满足，但受限于on-policy，采样效率低下。（无stability guarantees）</p>
<p>​    <strong>（2） Lyapunov-based approach</strong></p>
<p>通过构造李雅普诺夫函数求解带约束的控制问题。但仅保证设计的约束函数的累积保持在阈值以下。（无 stability guarantees）</p>
<p>​    <strong>（3）model-based RL in Lipschitz continuous deterministic nonlinear systems</strong></p>
<p>17年nips那篇，笔记2解读过。问题出在其安全性保证是针对状态空间子集中离散点而言的，故限制在低维、简单的系统中。</p>
<p>​    <strong>（4）model-based CBF controller</strong></p>
<p>19年aaai那篇，笔记1解读过。问题在于需要一个nominal model。</p>
<p>​    <strong>（5）expert knowledge+RL</strong></p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><p><strong>UUB稳定性相关概念</strong></p>
<p><strong>定义1：</strong>如果存在正的常数$b$和$\eta$，对于任意$\epsilon<b$，存在$T(\epsilon, \eta)$，使得：$\left\|s_{t_0}\right\| < \epsilon \rightarrow\left\|s_{t}\right\|<\eta, \forall t>t_{0}+T$，该系统被称为满足UUB，且以$\eta$为界限。若$\epsilon$可以任意大，则该系统满足全局UUB。</b$，存在$T(\epsilon,></p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227112538366.png" alt="image-20220227112538366" style="zoom: 50%;"></p>
<p>上述定义的安全约束仅仅限制在状态的l1范数上，但实际系统的约束函数不一定是采用该约束方法，故作者将定义1更新为：</p>
<p><strong>定义2：</strong>如果存在正的常数$b$和$\eta$，对于任意$\epsilon&lt;b$，存在$T(\epsilon, \eta)$，使得：$c_{\pi}(s_1) \leq \epsilon \Rightarrow c_{\pi}(s_t)\leq \eta, \forall t \geq T$，该系统被称为满足UUB，且以$c_\pi(\cdot)$为界限。若$\epsilon$可以任意大，则该系统满足全局UUB。</p>
<p>其中$c_{\pi}(s) \triangleq \mathbb{E}_{a \sim \pi} c(s, a)$。</p>
<p><strong>Constrained Markov Decision Process(CMDP)：</strong></p>
<p>$\max _{\pi} J(\pi)$ s.t. $\mathbb{E}_{s_{t}} c_{\pi}\left(s_{t}\right) \leq d, \forall t \in[1, \infty)$</p>
<p>$J(\pi)$是MDP的目标，加上了安全约束就变成了CMDP。</p>
<p>假设初始状态$S_1$（其概率密度函数定义为$\rho\left(S_{1}\right)$）是在安全域内，即：初始状态从$\Gamma$域内进行选择：$\Gamma \triangleq\left\{s \mid c_{\pi}(s) \leq b\right\}$</p>
<p><strong>安全集合</strong>：$\mathcal{B} \triangleq\left\{s \mid c_{\pi}(s)&lt;\eta\right\}$</p>
<p><strong>边缘集合</strong>：$\Delta \triangleq\left\{s \mid c_{\pi}(s) \geq \eta\right\}$</p>
<p><strong>假设1</strong>：$c(s, a)$非负且存在一个可积函数$g(s, a)$使得对于任意在状态空间和动作空间内的（s，a）而言，$c(s, a) &lt; g(s, a)$。</p>
<p><strong>理论1</strong>：如果存在一个李氏方程L(s)和正常数$\alpha_1,\alpha_2,\alpha_3,\eta$，使得：</p>
<script type="math/tex; mode=display">
\alpha_1 c_\pi(s) \leq L(s) \leq \alpha_2 c_\pi(s), \forall s \in S</script><p>且：（能量递减UUB条件）</p>
<script type="math/tex; mode=display">
\mathbb{E}_{s \sim \mu_{N}}\left(\mathbb{E}_{s^{\prime} \sim P_{\pi}} L\left(s^{\prime}\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)-L(s) \mathbb{1}_{\Delta}(s)\right) <-\alpha_{3} \mathbb{E}_{s \sim \mu_{N}} c_{\pi}(s) \mathbb{1}_{\Delta}(s)</script><p>其中$\mu_{N}(s)$代表了状态s在有限N个时间步长上的分布：$\mu_{N}(s) \doteq \frac{1}{N} \sum_{t=1}^{N} p(s \mid \rho, \pi, t)$</p>
<p>N是处于边缘集合的概率&gt;0的最后一个时刻</p>
<p>$\mathbb{1}_{\Delta}(s)$表示该状态是否在边缘集合内：$\mathbb{1}_{\Delta}(s)= \begin{cases}1 &amp; s \in \Delta \\ 0 &amp; s \notin \Delta\end{cases}$</p>
<p><em>作者的这种方法需要采用历史数据（$\mu_{N}(s)$），但并不需要了解模型信息</em></p>
<p><strong>注意1</strong>：如果系统满足UUB且以$\frac{\alpha_{2} b}{\alpha_{3}}+\eta&lt;d$为界限，则可以保证系统满足安全性约束。</p>
<hr>
<p><strong>Lyapunov-based Soft Actor–Critic(LSAC)——off-policy</strong></p>
<p>critic函数包括两部分，其一是标准SAC的目标（评估该（s,a）的价值），用$Q(s,a)$来评判；其二是评估UUB条件，用李雅普诺夫critic函数$L_c(s,a)$来评判。</p>
<p><em>为什么用$L_c(s,a)$而不是$L(s)$？</em></p>
<p><em>因为$L(s)$不能直接用于critic函数，critic更新的时候是对$\pi$而言的，而$L(s)$与$\pi$无关。</em></p>
<p>二者之间的关系为：$L(s)=\mathbb{E}_{a \sim \pi}L_c(s,a)$</p>
<p>$L_c(s,a)$使用一个参数化的全连接层DNN来描述。该网络的目的是让$L_c(s,a)$靠近$L_{target}(s,a)$，$L_{target}(s,a)$指的就是真实的满足李雅普诺夫条件的函数（作者选择了价值函数和约束函数来做候选，因为这俩都是非负的且一阶导是半负定的）：</p>
<script type="math/tex; mode=display">
J(\phi)=\mathbb{E}_{\mathcal{D}}\left[\frac{1}{2}\left(L_{c}(s, a)-L_{\text {target }}(s, a)\right)^{2}\right]</script><p>D是$\pi$下的transition：$\mathcal{D}=\{(s,a,s^{\prime},r,c)\}$，$J(\phi)$即后文伪代码中的$J(L_c)$。</p>
<p>同critic函数里$Q_{target}$的思路，$L_{target}$定义为：</p>
<script type="math/tex; mode=display">
L_{target}(s,a) = c(s,a) + \text{max}_{a^{\prime}} \gamma L_c^{\prime}(s^{\prime}, a^{\prime})</script><p>对于LSAC而言，一方面SAC在最大化Q（s,a）的同时希望策略熵尽可能大，另一方面安全约束希望能量递减尽可能大（后-前越小越好），故利用拉格朗日法，该多约束最优化问题可以转变为下式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J(\pi)=& \mathbb{E}_{\mathcal{D}}\left[-Q\left(s, f_{\theta}(s, \epsilon)\right)+\beta \log \pi_{\theta}\left(f_{\theta}(s, \epsilon) \mid s\right)\right] \\
+& \lambda \mathbb{E}_{\mathcal{D}_{\Delta}}\left[L_{c}\left(s^{\prime}, f_{\theta}\left(s^{\prime}, \epsilon\right)\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)\right.\\
-&\left.\left(L_{c}(s, a)-\alpha_{3} c\right) \mathbb{1}_{\Delta}(s)\right]
\end{aligned}</script><p>$\beta$和$\lambda$是正的拉格朗日乘数。$D_{\Delta}$指的是在$\mu_{N}$中的transition（edge buffer）$D$即常规的replay buffer。</p>
<p>对于$Q$、$\beta$和$\lambda$，其对应的损失函数依次为：</p>
<p>$J\left(Q_{i}\right)=\mathbb{E}_{\mathcal{D}} \frac{1}{2}\left[r+\gamma Q_{i}^{\prime}\left(s^{\prime}, f_{\theta}\left(s^{\prime}, \epsilon\right)\right)-Q_{i}(s, a)\right]^{2}, i \in\{1,2\}$</p>
<p>$J(\beta)=\beta \mathbb{E}_{\mathcal{D}}\left[\log \pi_{\theta}(a \mid s)+\mathcal{H}_{t}\right]$</p>
<p>$J(\lambda)=\lambda \mathbb{E}_{\mathcal{D}_{\Delta}}\left[L_{c}\left(s^{\prime}, f_{\theta}\left(s^{\prime}, \epsilon\right)\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)-\left(L_{c}(s, a)-\alpha_{3} c\right) \mathbb{1}_{\Delta}(s)\right]$</p>
<p>最终LSAC的伪代码如下：</p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227171257691.png" alt="image-20220227171257691" style="zoom: 67%;"></p>
<hr>
<p><strong>Lyapunov-based Constrained Policy Optimization (LCPO)——off-policy</strong></p>
<p>CPO+安全约束的原问题如下：</p>
<script type="math/tex; mode=display">
\theta_{k+1}=\underset{\theta}{\arg \max } \underset{\theta \sim_{\Delta \sim}}{\mathbb{E}} Q_{\pi_{k}}(s, a)</script><script type="math/tex; mode=display">
\begin{aligned}\text { s.t. } &\mathbb{E}_{s \sim \mathcal{D}_{\Delta} \atop a \sim \pi}\left[L\left(s^{\prime}\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)-\left(L(s)-\alpha_{3} c\right) \mathbb{1}_{\Delta}(s)\right] \leq 0 \\ &\mathbb{E}_{\mathcal{D}} D_{\mathrm{KL}}\left(\pi_{\theta} \mid \pi_{k}\right) \leq \delta \end{aligned}</script><p>作者使用泰勒二阶展开将其近似为如下优化问题：</p>
<script type="math/tex; mode=display">
\begin{aligned} \theta_{k+1}=& \arg \max _{\theta} g_{Q}^{\top}\left(\theta-\theta_{k}\right) \\ \text { s.t. } & g_{L}^{\top}\left(\theta-\theta_{k}\right)+h \leq 0 \\ & \frac{1}{2}\left(\theta-\theta_{k}\right)^{\top} H\left(\theta-\theta_{k}\right) \leq \delta \end{aligned}</script><p>$g_Q$、$g_L$分别是目标函数和安全约束函数相对于网络参数$\theta$的梯度。$h$是安全约束函数值，$H$是Fisher信息矩阵。</p>
<p>则针对上述凸优化问题，其对偶问题为：</p>
<script type="math/tex; mode=display">
\max _{\lambda, \beta \geq 0} \frac{1}{2 \beta}\left(\mathrm{g}_{Q}^{T} H^{-1} g_{Q}-2 \lambda \mathcal{Z}+\lambda^{2} \mathcal{N}\right)+\lambda h-\frac{\beta \delta}{2}</script><p>其中$\mathcal{Z}=g^T_QH^{-1}g_L$，$\mathcal{N}=g^{T}_LH^{-1}g_L$。</p>
<p>假设原问题有解且$\lambda$和$\beta$是对偶问题的解，则原问题的最优解为：</p>
<script type="math/tex; mode=display">
\theta_{k+1}=\theta_{k}+\frac{1}{\beta^{*}} H^{-1}\left(g_{Q}-\lambda^{*} g_{L}\right)</script><p>若原问题无解，则需要尽快恢复到安全策略：</p>
<script type="math/tex; mode=display">
\begin{aligned} \theta_{k+1} &=\arg \min _{\theta} g_{L}^{\top}\left(\theta-\theta_{k}\right)+h \\ \text { s.t. } & \frac{1}{2}\left(\theta-\theta_{k}\right)^{\top} H\left(\theta-\theta_{k}\right) \leq \delta \end{aligned}</script><p><em>相当于在小范围策略更新的情况下找到离约束最近的策略</em></p>
<p>该解为：</p>
<script type="math/tex; mode=display">
\theta^{*}=\theta_{k}-\sqrt{\frac{2 \delta}{g_{L}^{\top} H^{-1 g_{L}}}} H^{-} 1 g_{L}</script><p>​    LCPO的伪代码如下：</p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227193544818.png" alt="image-20220227193544818"></p>
<p>作者认为<strong>可以先用LSAC在仿真环境中学习初始策略，再将其应用到LCPO接受进一步的在线训练</strong>。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2022/03/05/Robust%20DRL-3/image-20220227200041471.png" alt="image-20220227200041471"></p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227200051457.png" alt="image-20220227200051457"></p>
<p><em>第一行是return 第二行是cost函数的值 第三行是超越安全界限的数量</em></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust DRL学习笔记（1）</title>
    <url>/2022/02/23/Robust-DRL-1/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks</strong></p>
<span id="more"></span>
<p>Richard Cheng Richard M. Murray Joel W. Burdick  California Institute of Technology </p>
<p>AAAI2019</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>在无模型的DRL上通过基于模型信息构建的CBF控制器来修正RL决策（这里作者考虑了CBF“仅仅修正RL的决策动作”和“既考虑修正DRL，又考虑影响DRL的更新”两部分）。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>这篇文章的定位是<strong>可验证的“safe-RL”</strong>，作者认为RL在探索过程中侧重于最大化长期回报，这可能会在学习过程中探索到不安全的行为，这在实际系统中是不允许的</p>
<p>针对safe-RL研究而言，作者从model-free &amp; model-based角度进行现状分析：</p>
<p><strong>（1）model-free</strong>：文献都较老，主要方法有reward-shaping, policy optimization with constraints or teacher advice，并且无法保证在训练初期也能保证DRL决策的安全性。</p>
<p><strong>（2）model-based</strong>: Lyapunov-based or MPC or 备用控制器（实际上是引入了一个风险感知算法，在探索阶段 如果感知到该动作会引起较差的响应，则采用备用控制器的安全动作）。前两种方法没有解决探索和性能优化的问题，最后一种方法对探索过程限制过强</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><h4 id="模型获取（model-based-CBF的需求）"><a href="#模型获取（model-based-CBF的需求）" class="headerlink" title="模型获取（model-based CBF的需求）"></a>模型获取（model-based CBF的需求）</h4><p>给定$S \rightarrow S$的动力学方程：</p>
<script type="math/tex; mode=display">
s_{t+1} = f(s_t) + g(s_t)a_t + d(s_t)</script><p>其中f()代表与a无关的状态，g()中的状态与a有关，d()是不确定项（例如摩擦力和关节柔性）</p>
<p>由于CBF控制器是model-based的，但精准知道模型是很困难的，尤其是d()。故作者采用<strong>Gaussian Process(GP)</strong>估计d()。</p>
<p><img src="https://pic4.zhimg.com/v2-3c25a927c217f13a055794377635faaf_r.jpg" alt="Fig.1 高斯过程更新的图（随着数据点的扩充，高斯过程的均值会越来越接近真实值）"></p>
<p><em>Fig.1 高斯过程更新的图（随着数据点的扩充，高斯过程的均值会越来越接近真实值）</em></p>
<p>理论上我们用于估计d()的数据越多越好，但由于在高斯过程的$\mu$和$\sigma$的求解过程中涉及到求逆的操作，所以为了实时性考虑，实际上只取最新的1000个数据来训练GP model。</p>
<h4 id="CBF控制器设计"><a href="#CBF控制器设计" class="headerlink" title="CBF控制器设计"></a>CBF控制器设计</h4><p>定义安全集合$C$：</p>
<script type="math/tex; mode=display">
\mathcal{C}:\left\{s \in \mathbb{R}^{n}: h(s) \geq 0\right\}</script><p>CBF就是利用李雅普诺夫函数的推广，给出了<strong>在受控动力学下保证安全集C的前向不变性的充分条件</strong>。</p>
<p><strong>定义1</strong>：给定安全集合$C$，一个连续可微的函数$h(\cdot)$作为CBF函数，若存在$\eta \in [0, 1]$使得对所有$s_t$，均有：</p>
<script type="math/tex; mode=display">
\sup _{a_{t} \in A}\left[h\left(f\left(s_{t}\right)+g\left(s_{t}\right) a_{t}+d\left(s_{t}\right)\right)+(\eta-1) h\left(s_{t}\right)\right] \geq 0</script><p>$\eta$就是CBF区别于LF的主要一项，该变量指的是$a_t$将状态$s_{t+1}$推向安全集合的强度。$h(\cdot)$作者定义为一个基本的势垒函数$h=p^{T} s+q$（<em>though our methodology could support more general barrier functions. This  restriction means the set C is composed of intersecting half spaces (i.e.  polytopes).</em>这段不太理解）</p>
<p>代入$h(\cdot)$和$d(\cdot)$后，我们可以得到CBF控制器的目标：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left(a_{t}, \epsilon\right)=\underset{a_{t}, \epsilon}{\operatorname{argmin}} &\left\|a_{t}\right\|_{2}+K_{\epsilon} \epsilon \\
\text { s.t. } \quad & p^{T} f\left(s_{t}\right)+p^{T} g\left(s_{t}\right) a_{t}+p^{T} \mu_{d}\left(s_{t}\right)-\\
& k_{\delta}|p|^{T} \sigma_{d}\left(s_{t}\right)+q \geq(1-\eta) h\left(s_{t}\right)-\epsilon \\
& a_{\text {low }}^{i} \leq a_{t}^{i} \leq a_{\text {high }}^{i} \text { for } i=1, \ldots, M
\end{aligned}</script><p>其中$\epsilon$是松弛变量（<em>松弛变量的引入常常是为了便于在<strong>更大的可行域</strong>内求解。若为0，则收敛到原有状态，若大于零，则约束松弛。</em>）</p>
<p><strong>引理1</strong>：若对于任何$s \in C$，式（4）存在$\epsilon = 0$的解，则该控制器有（1-$\delta$）的概率使得安全集合C是前向不变的（即始终在安全集合内）。若存在$\epsilon=\epsilon^{\max }&gt;0$，则该控制器有（1-$\delta$）的概率使得$\mathcal{C}_{\epsilon}:\left\{s \in \mathbb{R}^{n}: h(s) \geq-\frac{\epsilon}{\eta}\right\}$是前向不变（即始终在限制集合内）。</p>
<h4 id="基于CBF的强化学习补偿控制"><a href="#基于CBF的强化学习补偿控制" class="headerlink" title="基于CBF的强化学习补偿控制"></a>基于CBF的强化学习补偿控制</h4><script type="math/tex; mode=display">
u_{k}(s)=u_{\theta_{k}}^{R L}(s)+u_{k}^{C B F}\left(s, u_{\theta_{k}}^{R L}\right)</script><p><img src="/2022/02/23/Robust-DRL-1/image-20220223093018926.png" alt="Fig2 CBF仅补偿RL控制动作"></p>
<p><em>Fig2 CBF仅补偿RL控制动作</em></p>
<p>CBF并不影响DRL的policy network的更新，只是做一个调控作用，和背景介绍里的“备用控制器”其实比较相似。</p>
<h4 id="基于CBF的强化学习引导控制"><a href="#基于CBF的强化学习引导控制" class="headerlink" title="基于CBF的强化学习引导控制"></a>基于CBF的强化学习引导控制</h4><p><img src="/2022/02/23/Robust-DRL-1/image-20220223093216487.png" alt="Fig3 策略迭代过程"></p>
<p><em>Fig3 策略迭代过程</em></p>
<p>如果仅利用CBF进行补偿，其控制结果如上图a，RL在第k个更新周期里的策略$\pi_{\theta_{k}}^{RL}$可能是很差的，但实际采取的策略是$\pi_{k}$，RL更新如果从$\pi_{k}$处进行更新（每次更新必然也只是在上次策略的一个小邻域内选择最接近$\pi_{opt}$的动作），$\pi_{\theta_{k+1}}^{RL}$会越来越好，逐渐摆脱CBF的补偿，这才是作者想要的效果。</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093310526.png" alt="Fig4 CBF指导强化学习更新"></p>
<p><em>这里利用历史CBF的求和是因为RL的policy更新如今也是依赖于之前的CBF控制值（在上一次的控制值基础上调节）</em></p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{k}(s)=& u_{\theta_{k}}^{R L}(s)+\sum_{j=0}^{k-1} u_{j}^{C B F}\left(s, u_{\theta_{0}}^{R L}, \ldots, u_{\theta_{j-1}}^{R L}\right) \\
&+u_{k}^{C B F}\left(s, u_{\theta_{k}}^{R L}+\sum_{j=0}^{k-1} u_{j}^{C B F}\right)
\end{aligned}</script><p>RL-CBF整体伪代码：</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093421936.png" alt="image-20220223093421936"></p>
<p><em>Tips：k是RL policy更新和GP更新的迭代次数</em></p>
<h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h3><p>倒立摆：</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093700054.png" alt="*左图表示即使在agent学习的过程中也不会违反安全限制 右图表示最终CBF在实际一个episode里基本不需要补偿*"></p>
<p><em>左图表示即使在agent学习的过程中也不会违反安全限制 右图表示最终CBF在实际一个episode里基本不需要补偿</em></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust DRL学习笔记（4）</title>
    <url>/2022/03/11/Robust%20DRL-4/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>Constrained Policy Optimization</strong></p>
<span id="more"></span>
<p>Joshua Achiam David Held Aviv Tamar Pieter Abbeel (UC Berkeley)<br>       PMLR2017</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>第一个用于约束强化学习的<strong>通用（可以用于DRL）</strong>策略搜索算法，在每次迭代中<strong>保证</strong>近似约束满足。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>CMDP：增加了约束条件，限制了该MDP允许的策略集。</p>
<p><em>“A constrained Markov decision process (CMDP) is an MDP augmented with  constraints that restrict the set of allowable policies for that MDP”</em></p>
<p>已有利用启发式算法或是基于原始对偶方法的方法进行CMDP的研究，但无法保证每个学习过程中都能满足约束，存在保证了约束的工作，但假设条件太多，不通用。</p>
<p>其他与之前笔记重复内容不再赘述。</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><p><strong>CMDP：</strong></p>
<script type="math/tex; mode=display">
C_{i}: J_{C_{i}}(\pi)=\mathrm{E}\left[\sum_{t=0}^{\infty} \gamma^{t} C_{i}\left(s_{t}, a_{t}, s_{t+1}\right)\right]\\
\Pi_{C} \doteq\left\{\pi \in \Pi: \forall i, J_{C_{i}}(\pi) \leq d_{i}\right\}\\
\pi^{*}=\arg \max _{\pi \in \Pi_{C}} J(\pi)</script><p>$J(\pi)$就是折扣return的期望，$J_{C_i}(\pi)$是折扣约束的期望，目标是在满足$J_{C_i}(\pi)&lt;d_i$的情况下最大化$J(\pi)$。</p>
<p><strong>Constrained Policy Optimization(CPO):</strong></p>
<p>对于大型的MDP而言，若想得出最优策略是十分复杂的，因此利用神经网络等策略搜索算法逐步解决该问题：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\pi_{k+1}=\arg \max _{\pi \in \Pi_{\theta}}  J(\pi) \\
& \text { s.t. } D\left(\pi, \pi_{k}\right) \leq \delta
\end{aligned}</script><p>$D$是度量新旧策略差异的度量函数，每次策略更新是在很小的邻域内。</p>
<p>所以，对于CMDP而言，其更新可以看作：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{k+1}=&\arg \max _{\pi \in \Pi_{\theta}}  J(\pi) \\
\text { s.t. } & J_{C_{i}}(\pi) \leq d_{i} \quad i=1, \ldots, m \\
& D\left(\pi, \pi_{k}\right) \leq \delta .
\end{aligned}</script><p>如果完全按照上式进行更新，则也是计算复杂的，所以作者使用了 Trust Region Optimization(TRO)近似计算上式。</p>
<hr>
<p>作者首先证明了两个随机策略的returns的差的界限；之后证明了在每次更新时最坏情况下的性能下降保证。</p>
<p><strong>Policy Performance Bounds</strong></p>
<p>已有证明表示：</p>
<script type="math/tex; mode=display">
J\left(\pi^{\prime}\right)-J(\pi)=\frac{1}{1-\gamma} \underset{s \sim d^{\pi^{\prime} \atop} \atop a \sim \pi^{\prime}}{\mathrm{E}}\left[A^{\pi}(s, a)\right]</script><p>其中$d^{\pi}(s)=(1-\gamma) \sum_{t=0}^{\infty} \gamma^{t} P\left(s_{t}=s \mid \pi\right)$表示discounted 未来状态的分布。（该证明没懂）</p>
<p><strong>理论1：</strong>对于任意一个关于S的函数f，任意的策略$\pi$和$\pi^{\prime}$，定义估计与真实值的误差为$\delta_{f}\left(s, a, s^{\prime}\right) \doteq R\left(s, a, s^{\prime}\right)+\gamma f\left(s^{\prime}\right)-f(s)$，</p>
<p>同时定义：</p>
<script type="math/tex; mode=display">
\epsilon_{f}^{\pi^{\prime}} \doteq \max _{s}\left|\mathrm{E}_{a \sim \pi^{\prime}, s^{\prime} \sim P}\left[\delta_{f}\left(s, a, s^{\prime}\right)\right]\right|</script><script type="math/tex; mode=display">
L_{\pi, f}\left(\pi^{\prime}\right) \doteq \underset{s \sim d^{\pi} ,a \sim \pi  \atop s^{\prime} \sim P}{\mathrm{E}}\left[\left(\frac{\pi^{\prime}(a \mid s)}{\pi(a \mid s)}-1\right) \delta_{f}\left(s, a, s^{\prime}\right)\right]</script><script type="math/tex; mode=display">
D_{\pi, f}^{\pm}\left(\pi^{\prime}\right) \doteq \frac{L_{\pi, f}\left(\pi^{\prime}\right)}{1-\gamma} \pm \frac{2 \gamma \epsilon_{f}^{\pi^{\prime}}}{(1-\gamma)^{2}} \underset{s \sim d^{\pi}}{\mathrm{E}}\left[D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right]</script><p>其中$D_{TV}\left(\pi^{\prime} | \pi\right)[s]=(1 / 2) \sum_{a}\left|\pi^{\prime}(a \mid s)-\pi(a \mid s)\right|$。则以下界限存在：</p>
<script type="math/tex; mode=display">
D_{\pi, f}^{+}\left(\pi^{\prime}\right) \geq J\left(\pi^{\prime}\right)-J(\pi) \geq D_{\pi, f}^{-}\left(\pi^{\prime}\right)</script><p><strong>推论1：</strong>对任意策略$\pi^{\prime}$,$\pi$，有$\epsilon^{\pi^{\prime}} \doteq \max _{s}\left|\mathrm{E}_{a \sim \pi^{\prime}}\left[A^{\pi}(s, a)\right]\right|$ ，则以下界限存在：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&J\left(\pi^{\prime}\right)-J(\pi) \\
&\geq \frac{1}{1-\gamma} \underset{s \sim d^{\pi} \atop a \sim \pi^{\prime}}{\mathrm{E}}\left[A^{\pi}(s, a)-\frac{2 \gamma \epsilon^{\pi^{\prime}}}{1-\gamma} D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right]
\end{aligned}</script><p><strong>推论2：</strong>对任意策略$\pi^{\prime}$,$\pi$，任意代价函数$C_i$，且$\epsilon_{C_{i}}^{\pi^{\prime}} \doteq \max _{s}\left|\mathrm{E}_{a \sim \pi^{\prime}}\left[A_{C_{i}}^{\pi}(s, a)\right]\right|$，则以下界限成立：</p>
<script type="math/tex; mode=display">
J_{C_{i}}\left(\pi^{\prime}\right)-J_{C_{i}}(\pi)
\leq \frac{1}{1-\gamma} \underset{s \sim d^{\pi} \atop a \sim \pi^{\prime}}{\mathrm{E}}\left[A_{C_{i}}^{\pi}(s, a)+\frac{2 \gamma \epsilon_{C_{i}}^{\prime}}{1-\gamma} D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right]</script><p>上述理论和证明定义的界限都是针对<strong>TV散度</strong>而言的，而信域方法是利用<strong>KL散度</strong>，故通过Pinsker不等式将二者建立联系如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underset{s \sim d^{\pi}}{\mathrm{E}}\left[D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right] & \leq \underset{s \sim d^{\pi}}{\mathrm{E}}\left[\sqrt{\frac{1}{2} D_{K L}\left(\pi^{\prime} \| \pi\right)[s]}\right] \\
& \leq \sqrt{\frac{1}{2} \underset{s \sim d^{\pi}}{\mathrm{E}}\left[D_{K L}\left(\pi^{\prime} \| \pi\right)[s]\right]}
\end{aligned}</script><p>KL散度计算公式为：$K L(p | q)=\sum p(x) \log \frac{p(x)}{q(x)}$。</p>
<p><strong>推论3：</strong>将理论1和推论1、2中的TV散度换为KL散度即可得到一组和KL散度相关的边界条件。</p>
<p><strong>Trust Region Methods</strong></p>
<p>信赖域算法的策略更新采取以下形式：</p>
<script type="math/tex; mode=display">
\begin{array}{r}
\pi_{k+1}=\arg \max _{\pi \in \Pi_{\theta}} \underset{s \sim d^{\pi} k}{\mathrm{E}}\left[A^{\pi_{k}}(s, a)\right] \\
\text { s.t. } \bar{D}_{K L}\left(\pi \| \pi_{k}\right) \leq \delta
\end{array}</script><p>其中$D_{KL}$的约束被称为信赖域（策略更新前后的策略性能之差可用优势函数的折扣累加期望）。</p>
<p><strong>命题1（性能保证）：</strong>假设$\pi_{k}$按照式（12）进行更新，则策略更新前后的性能差异有：</p>
<script type="math/tex; mode=display">
J\left(\pi_{k+1}\right)-J\left(\pi_{k}\right) \geq \frac{-\sqrt{2 \delta} \gamma \epsilon^{\pi_{k+1}}}{(1-\gamma)^{2}}
\\
where \quad\epsilon^{\pi_{k+1}}=\max _{s}\left|\mathrm{E}_{a \sim \pi_{k+1}}\left[A^{\pi_{k}}(s, a)\right]\right|.</script><p>考虑信赖域的约束条件和安全约束条件，结合推论1、2、3，有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\pi_{k+1}=\arg \max _{\pi \in \Pi_{\theta}} \underset{s \sim d^{\pi_{k}} \atop a \sim \pi}{\mathrm{E}}\left[A^{\pi_{k}}(s, a)\right]-\alpha_{k} \sqrt{\bar{D}_{K L}\left(\pi \| \pi_{k}\right)} \\
&\text { s.t. } J_{C_{i}}\left(\pi_{k}\right)+\underset{s \sim d^{\pi_{k}} \atop a \sim \pi}{\mathrm{E}}\left[\frac{A_{C_{i}}^{\pi_{k}}(s, a)}{1-\gamma}\right]+\beta_{k}^{i} \sqrt{\bar{D}_{K L}\left(\pi \| \pi_{k}\right)} \leq d_{i}
\end{aligned}</script><p>基于此，作者提出<strong>CPO</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{k+1}=& \arg \max _{\pi \in \Pi_{\theta}} \underset{s \sim d^{\pi_{k}}}{\mathrm{E}}\left[A^{\pi_{k}}(s, a)\right] \\
\text { s.t. } &J_{C_{i}}\left(\pi_{k}\right)+\frac{1}{1-\gamma} \underset{s \sim d^{\pi_{k}}}{\mathrm{E}}\left[A_{C_{i}}^{\pi_{k}}(s, a)\right] \leq d_{i} \quad \forall i \\
& \bar{D}_{K L}\left(\pi \| \pi_{k}\right) \leq \delta .
\end{aligned}</script><p>​    因为这是一个信赖域方法，所以继承了命题1的性能保证。此外，根据推论2和3，又<strong>近似满足</strong>安全约束。故最终达到了满足安全约束的性能保证。</p>
<p><strong>命题2（CPO更新的最坏情况）：</strong>安全约束$C_i-return$的上界为：    </p>
<script type="math/tex; mode=display">
J_{C_{i}}\left(\pi_{k+1}\right) \leq d_{i}+\frac{\sqrt{2 \delta} \gamma \epsilon_{C_{i}}^{\pi_{k+1}}}{(1-\gamma)^{2}}
\\
where \quad\epsilon_{C_{i}}^{\pi_{k+1}}=\max _{s}\left|\mathrm{E}_{a \sim \pi_{k+1}}\left[A_{C_{i}}^{\pi_{k}}(s, a)\right]\right|.</script><p><strong>CPO如何求解？</strong></p>
<p>定义$c_{i}=J_{C_i}(\pi_k)-d$，则公式（15）可以近似为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{k+1}=\arg \max _{\theta} & g^{T}\left(\theta-\theta_{k}\right) \\
\text { s.t. } \quad & c_{i}+b_{i}^{T}\left(\theta-\theta_{k}\right) \leq 0 \quad i=1, \ldots, m \\
& \frac{1}{2}\left(\theta-\theta_{k}\right)^{T} H\left(\theta-\theta_{k}\right) \leq \delta
\end{aligned}</script><p>$H$是费希尔信息矩阵，总是半正定的且式（17）优化问题是<strong>凸的</strong>（上式是一个带线性和二次约束的线性目标函数凸优化问题(LQCLP)），故可以用拉格朗日对偶方法求解：</p>
<script type="math/tex; mode=display">
\max _{\lambda \geq 0 \atop \nu \succeq 0} \frac{-1}{2 \lambda}\left(g^{T} H^{-1} g-2 r^{T} \nu+\nu^{T} S \nu\right)+\nu^{T} c-\frac{\lambda \delta}{2}</script><p>其中$r \doteq g^{T} H^{-1} B, S \doteq B^{T} H^{-1} B$。若$\lambda^<em>，v^</em>$是式（18）的解，则原问题的解为：</p>
<script type="math/tex; mode=display">
\theta^{*}=\theta_{k}+\frac{1}{\lambda^{*}} H^{-1}\left(g-B \nu^{*}\right)</script><p>由于近似是存在误差的，有时利用式（17）是安全有效的，但对于不可行的情况，作者利用下式更新：</p>
<script type="math/tex; mode=display">
\theta^{*}=\theta_{k}-\sqrt{\frac{2 \delta}{b^{T} H^{-1} b}} H^{-1} b</script><p>该更新用于信赖域和约束域的交集为空集时采取的更新措施。</p>
<p>伪代码：</p>
<p><img src="/2022/03/11/Robust%20DRL-4/image-20220307152139547.png" alt="image-20220307152139547"></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2022/03/11/Robust%20DRL-4/image-20220307152526198.png" alt="image-20220307152526198"></p>
<p><em>CPO保持在限制内（感觉都在限制值那块聚集？）更多 TRPO无效</em></p>
<p><img src="/2022/03/11/Robust%20DRL-4/image-20220307152950704.png" alt="image-20220307152950704"></p>
<p><em>CPO+cost shaping 更有效</em></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
</search>
