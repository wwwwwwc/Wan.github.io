<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Robust DRL学习笔记（2）</title>
    <url>/2022/02/27/Robust%20DRL-2/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>Safe Model-based Reinforcement Learning with Stability Guarantees</strong><br><span id="more"></span></p>
<p>Felix Berkenkamp Matteo Turchetta Angela P. Schoellig  Angela P. Schoellig ETH Zurich</p>
<p>NIPS2017</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>作者为DRL策略学习和更新过程中提供了高概率安全保证，展示了如何从初始的安全策略开始，通过收集安全区域内的数据来扩展对吸引域的估计，并调整策略以增加吸引域和提高控制效果。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p><strong>safe-RL:</strong></p>
<p>model-free的方法主要集中于构建风险敏感型RL。例如：(1)在奖励函数中指定避险行为(2)安全探索MDP;</p>
<p>model-based 方法主要集中在状态约束方面的安全性，例如：对不确定的环境约束进行建模；</p>
<p><strong>稳定性证明:</strong></p>
<p>（1）对于known system的稳定性可以使用李亚普诺夫函数来验证</p>
<p>（2）通过对未知的系统部分利用DP进行估计建模，也可用李亚普诺夫函数来验证</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><p>研究对象的状态方程：</p>
<script type="math/tex; mode=display">
\mathbf{x}_{t+1}=f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=h\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)+g\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)</script><p>x：状态 u：控制动作 h()已知的系统动态 g()未知的模型误差</p>
<p>$\pi$：$X \rightarrow U$ 最终目标设定为x转为0</p>
<p>$r(x, u)$：作者用该变量定义该状态下执行该动作的<strong>cost</strong>（正），故$r(0,0)=0$</p>
<p>目标：从交互数据中安全地了解系统的状态方程（因为有unknown model），并调整策略使discounted costs变小。</p>
<p><strong>假设1（连续性）：</strong>h() g() $\pi$满足李普希茨连续条件，其李普希茨常数依次为$L_h,L_g,L_\pi$</p>
<p><em>李普希茨条件的通俗含义：存在一个实数L，使得对于函数 f（x）上的每对点，连接它们的线的斜率的绝对值不大于这个实数L。最小的L称为该函数的Lipschitz常数。（满足李普希茨条件的函数是一个比满足一致连续更光滑的连续函数）</em></p>
<p><strong>假设2（校准良好的模型）：</strong>假设对n个噪声数据的测量的均值和方差为$\mu_{n}$和$\sigma_{n}$，存在一个常数$\beta_{n}&gt;0$，使得对于所有的n&gt;0,x,u，以$1-\delta$的置信度满足：$\left|f(\mathbf{x}, \mathbf{u})-\mu_{n}(\mathbf{x}, \mathbf{u})\right|_{1} \leq \beta_{n} \sigma_{n}(\mathbf{x}, \mathbf{u})$</p>
<p>Lyapunov函数：$v(0)=0,v(x)&gt;0$</p>
<p><strong>理论1：</strong>$L,f$分别表示李雅普诺夫方差和满足李普希茨连续条件的系统状态方程，如果对于所有在吸引域$V(c)$中的状态x，均有$v(f(\mathbf{x}, \pi(\mathbf{x})))<v(\mathbf{x})$。其中，$\mathcal{V}(c)=\{\mathbf{x} \in \mathcal{x} \backslash\{\mathbf{0}\} \mid v(\mathbf{x}) \leq c\}, c>0$，则系统是稳定的。</v(\mathbf{x})$。其中，$\mathcal{V}(c)=\{\mathbf{x}></p>
<p><em>相当于利用李雅普诺夫的递减条件取代了复杂的稳定检验（状态收敛到 0的检验）</em></p>
<p>对于DRL，由于价值函数满足：$v(\mathbf{x})=r(\mathbf{x}, \pi(\mathbf{x}))+v(f(\mathbf{x}, \pi(\mathbf{x})) \leq v(f(\mathbf{x}, \pi(\mathbf{x})))$，本身就是一个很好的候选李雅普诺夫函数。</p>
<p><em>Q1：所以说 作者前面定义cost用错符号了？</em></p>
<p>初始安全策略：假设有个初始策略，可以使得原点在某个很小的状态集$S_0^x$内渐近稳定</p>
<p>定义$v(f(\cdot))$的上界和下界分别是$u_n(x)$和$l_n(x)$。则稳定性保证可以变为$v(f(\mathbf{x}, \mathbf{u})) \leq u_{n}(\mathbf{x})&lt;v(\mathbf{x}) \text { for all } \mathbf{x} \in \mathcal{V}(c)$</p>
<p>作者认为理论1是针对连续x而言的，实际验证起来很困难，所以扩展到了离散域。</p>
<p><strong>理论2：</strong>将状态空间$X$扩展到离散域$X_{\tau}$，离散域里的点和连续域的点之间的L1范数很小 $\left|\mathbf{x}-[\mathbf{x}]_{\tau}\right|_{1} \leq \tau$，对于所有既属于该离散域又属于吸引域的状态x而言，若满足$u_{n}(\mathbf{x}, \mathbf{u})&lt;v(\mathbf{x})-L_{\Delta v} \tau$，则该域内以$1-\delta$的置信度满足$v(f(\mathbf{x}, \pi(\mathbf{x})))&lt;v(\mathbf{x})$，其中，$L_{\Delta v}:=L_{v} L_{f}\left(L_{\pi}+1\right)+L_{v}$。</p>
<p>定义满足递减条件的所有状态-动作集合：</p>
<p>$\mathcal{D}_{n}=\left\{(\mathbf{x}, \mathbf{u}) \in \mathcal{X}_{\tau} \times \mathcal{U} \mid u_{n}(\mathbf{x}, \mathbf{u})-v(\mathbf{x})&lt;-L_{\Delta v} \tau\right\}$</p>
<p>RL策略目标是找到一个<strong>尽可能大的吸引域</strong>，使得其中所有的$(x, \pi_x)$均在$D$内：</p>
<script type="math/tex; mode=display">
\pi_{n}, c_{n}=\underset{\pi \in \Pi_{L}, c \in \mathbb{R}_{>0}}{\operatorname{argmax}} c, \quad \text { such that for all } \mathbf{x} \in \mathcal{V}(c) \cap \mathcal{X}_{\tau}:(\mathbf{x}, \pi(\mathbf{x})) \in \mathcal{D}_{n}</script><p><strong>理论3：</strong>假设实际的吸引域为$R_{\pi_n}$，则在策略$\pi_n$下，对于$\delta \in (0,1)$，至少有$（1-\delta）$的置信度认为$\mathcal{V}\left(c_{n}\right) \subseteq \mathcal{R}_{\pi_{n}}$ ($n&gt;0$)。 </p>
<p>除此之外，作者提出不仅需要将当前状态限制在$V(C)$中，对于该状态下采取控制动作后转移的次态也要在$V(C)$中：</p>
<script type="math/tex; mode=display">
\mathcal{S}_{n}=\bigcup_{\mathbf{z} \subset \mathcal{S_{n-1}}}\left\{\mathbf{z}^{\prime} \in \mathcal{V}\left(c_{n}\right) \cap \mathcal{X}_{\tau} \times \mathcal{U}_{\tau} \mid u_{n}(\mathbf{z})+L_{v} L_{f}\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|_{1} \leq c_{n}\right\}</script><p>其算法伪代码为：</p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226150644884.png" alt="伪代码"></p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151031162.png" alt="image-20220226151031162"></p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151118226.png" alt="image-20220226151118226"></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151236997.png" alt="image-20220226151236997"></p>
<p><em>吸引域本身在变大 同时更多状态落在吸引域中</em></p>
<p><img src="/2022/02/27/Robust%20DRL-2/image-20220226151322970.png" alt="image-20220226151322970"></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust DRL学习笔记（3）</title>
    <url>/2022/03/05/Robust%20DRL-3/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>Reinforcement learning control of constrained dynamic systems with uniformly ultimate boundedness stability guarantee</strong><br><span id="more"></span></p>
<p>Minghao Han(HIT) Yuan Tian WeiPan(Delft University of Technology, Netherlands)</p>
<p>Automatica</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>为了解决RL控制的安全问题，传统的方法大多需要了解系统状态方程的全部或部分信息，作者提出了一种方法，不需要了解模型，只从数据出发就能完成UUB稳定性保证。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p><strong>UUB</strong></p>
<p>UUB保证：轨迹将在有限时间内进入稳定的邻域，并且此后不会从该集合中逃脱</p>
<p>该稳定性保证以往用于解决自适应动态规划（Adaptive Dynamic Programming），但不可避免的会用到模型结构和模型参数。</p>
<p><strong>RL with safety constraints</strong></p>
<p>​    <strong>（1）Safety Constrained Policy Optimization(CPO)</strong></p>
<p>该方法在一个初始安全的策略下使后续的安全约束得以满足，但受限于on-policy，采样效率低下。（无stability guarantees）</p>
<p>​    <strong>（2） Lyapunov-based approach</strong></p>
<p>通过构造李雅普诺夫函数求解带约束的控制问题。但仅保证设计的约束函数的累积保持在阈值以下。（无 stability guarantees）</p>
<p>​    <strong>（3）model-based RL in Lipschitz continuous deterministic nonlinear systems</strong></p>
<p>17年nips那篇，笔记2解读过。问题出在其安全性保证是针对状态空间子集中离散点而言的，故限制在低维、简单的系统中。</p>
<p>​    <strong>（4）model-based CBF controller</strong></p>
<p>19年aaai那篇，笔记1解读过。问题在于需要一个nominal model。</p>
<p>​    <strong>（5）expert knowledge+RL</strong></p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><p><strong>UUB稳定性相关概念</strong></p>
<p><strong>定义1：</strong>如果存在正的常数$b$和$\eta$，对于任意$\epsilon<b$，存在$T(\epsilon, \eta)$，使得：$\left\|s_{t_0}\right\| < \epsilon \rightarrow\left\|s_{t}\right\|<\eta, \forall t>t_{0}+T$，该系统被称为满足UUB，且以$\eta$为界限。若$\epsilon$可以任意大，则该系统满足全局UUB。</b$，存在$T(\epsilon,></p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227112538366.png" alt="image-20220227112538366" style="zoom: 50%;"></p>
<p>上述定义的安全约束仅仅限制在状态的l1范数上，但实际系统的约束函数不一定是采用该约束方法，故作者将定义1更新为：</p>
<p><strong>定义2：</strong>如果存在正的常数$b$和$\eta$，对于任意$\epsilon&lt;b$，存在$T(\epsilon, \eta)$，使得：$c_{\pi}(s_1) \leq \epsilon \Rightarrow c_{\pi}(s_t)\leq \eta, \forall t \geq T$，该系统被称为满足UUB，且以$c_\pi(\cdot)$为界限。若$\epsilon$可以任意大，则该系统满足全局UUB。</p>
<p>其中$c_{\pi}(s) \triangleq \mathbb{E}_{a \sim \pi} c(s, a)$。</p>
<p><strong>Constrained Markov Decision Process(CMDP)：</strong></p>
<p>$\max _{\pi} J(\pi)$ s.t. $\mathbb{E}_{s_{t}} c_{\pi}\left(s_{t}\right) \leq d, \forall t \in[1, \infty)$</p>
<p>$J(\pi)$是MDP的目标，加上了安全约束就变成了CMDP。</p>
<p>假设初始状态$S_1$（其概率密度函数定义为$\rho\left(S_{1}\right)$）是在安全域内，即：初始状态从$\Gamma$域内进行选择：$\Gamma \triangleq\left\{s \mid c_{\pi}(s) \leq b\right\}$</p>
<p><strong>安全集合</strong>：$\mathcal{B} \triangleq\left\{s \mid c_{\pi}(s)&lt;\eta\right\}$</p>
<p><strong>边缘集合</strong>：$\Delta \triangleq\left\{s \mid c_{\pi}(s) \geq \eta\right\}$</p>
<p><strong>假设1</strong>：$c(s, a)$非负且存在一个可积函数$g(s, a)$使得对于任意在状态空间和动作空间内的（s，a）而言，$c(s, a) &lt; g(s, a)$。</p>
<p><strong>理论1</strong>：如果存在一个李氏方程L(s)和正常数$\alpha_1,\alpha_2,\alpha_3,\eta$，使得：</p>
<script type="math/tex; mode=display">
\alpha_1 c_\pi(s) \leq L(s) \leq \alpha_2 c_\pi(s), \forall s \in S</script><p>且：（能量递减UUB条件）</p>
<script type="math/tex; mode=display">
\mathbb{E}_{s \sim \mu_{N}}\left(\mathbb{E}_{s^{\prime} \sim P_{\pi}} L\left(s^{\prime}\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)-L(s) \mathbb{1}_{\Delta}(s)\right) <-\alpha_{3} \mathbb{E}_{s \sim \mu_{N}} c_{\pi}(s) \mathbb{1}_{\Delta}(s)</script><p>其中$\mu_{N}(s)$代表了状态s在有限N个时间步长上的分布：$\mu_{N}(s) \doteq \frac{1}{N} \sum_{t=1}^{N} p(s \mid \rho, \pi, t)$</p>
<p>N是处于边缘集合的概率&gt;0的最后一个时刻</p>
<p>$\mathbb{1}_{\Delta}(s)$表示该状态是否在边缘集合内：$\mathbb{1}_{\Delta}(s)= \begin{cases}1 &amp; s \in \Delta \\ 0 &amp; s \notin \Delta\end{cases}$</p>
<p><em>作者的这种方法需要采用历史数据（$\mu_{N}(s)$），但并不需要了解模型信息</em></p>
<p><strong>注意1</strong>：如果系统满足UUB且以$\frac{\alpha_{2} b}{\alpha_{3}}+\eta&lt;d$为界限，则可以保证系统满足安全性约束。</p>
<hr>
<p><strong>Lyapunov-based Soft Actor–Critic(LSAC)——off-policy</strong></p>
<p>critic函数包括两部分，其一是标准SAC的目标（评估该（s,a）的价值），用$Q(s,a)$来评判；其二是评估UUB条件，用李雅普诺夫critic函数$L_c(s,a)$来评判。</p>
<p><em>为什么用$L_c(s,a)$而不是$L(s)$？</em></p>
<p><em>因为$L(s)$不能直接用于critic函数，critic更新的时候是对$\pi$而言的，而$L(s)$与$\pi$无关。</em></p>
<p>二者之间的关系为：$L(s)=\mathbb{E}_{a \sim \pi}L_c(s,a)$</p>
<p>$L_c(s,a)$使用一个参数化的全连接层DNN来描述。该网络的目的是让$L_c(s,a)$靠近$L_{target}(s,a)$，$L_{target}(s,a)$指的就是真实的满足李雅普诺夫条件的函数（作者选择了价值函数和约束函数来做候选，因为这俩都是非负的且一阶导是半负定的）：</p>
<script type="math/tex; mode=display">
J(\phi)=\mathbb{E}_{\mathcal{D}}\left[\frac{1}{2}\left(L_{c}(s, a)-L_{\text {target }}(s, a)\right)^{2}\right]</script><p>D是$\pi$下的transition：$\mathcal{D}=\{(s,a,s^{\prime},r,c)\}$，$J(\phi)$即后文伪代码中的$J(L_c)$。</p>
<p>同critic函数里$Q_{target}$的思路，$L_{target}$定义为：</p>
<script type="math/tex; mode=display">
L_{target}(s,a) = c(s,a) + \text{max}_{a^{\prime}} \gamma L_c^{\prime}(s^{\prime}, a^{\prime})</script><p>对于LSAC而言，一方面SAC在最大化Q（s,a）的同时希望策略熵尽可能大，另一方面安全约束希望能量递减尽可能大（后-前越小越好），故利用拉格朗日法，该多约束最优化问题可以转变为下式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J(\pi)=& \mathbb{E}_{\mathcal{D}}\left[-Q\left(s, f_{\theta}(s, \epsilon)\right)+\beta \log \pi_{\theta}\left(f_{\theta}(s, \epsilon) \mid s\right)\right] \\
+& \lambda \mathbb{E}_{\mathcal{D}_{\Delta}}\left[L_{c}\left(s^{\prime}, f_{\theta}\left(s^{\prime}, \epsilon\right)\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)\right.\\
-&\left.\left(L_{c}(s, a)-\alpha_{3} c\right) \mathbb{1}_{\Delta}(s)\right]
\end{aligned}</script><p>$\beta$和$\lambda$是正的拉格朗日乘数。$D_{\Delta}$指的是在$\mu_{N}$中的transition（edge buffer）$D$即常规的replay buffer。</p>
<p>对于$Q$、$\beta$和$\lambda$，其对应的损失函数依次为：</p>
<p>$J\left(Q_{i}\right)=\mathbb{E}_{\mathcal{D}} \frac{1}{2}\left[r+\gamma Q_{i}^{\prime}\left(s^{\prime}, f_{\theta}\left(s^{\prime}, \epsilon\right)\right)-Q_{i}(s, a)\right]^{2}, i \in\{1,2\}$</p>
<p>$J(\beta)=\beta \mathbb{E}_{\mathcal{D}}\left[\log \pi_{\theta}(a \mid s)+\mathcal{H}_{t}\right]$</p>
<p>$J(\lambda)=\lambda \mathbb{E}_{\mathcal{D}_{\Delta}}\left[L_{c}\left(s^{\prime}, f_{\theta}\left(s^{\prime}, \epsilon\right)\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)-\left(L_{c}(s, a)-\alpha_{3} c\right) \mathbb{1}_{\Delta}(s)\right]$</p>
<p>最终LSAC的伪代码如下：</p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227171257691.png" alt="image-20220227171257691" style="zoom: 67%;"></p>
<hr>
<p><strong>Lyapunov-based Constrained Policy Optimization (LCPO)——off-policy</strong></p>
<p>CPO+安全约束的原问题如下：</p>
<script type="math/tex; mode=display">
\theta_{k+1}=\underset{\theta}{\arg \max } \underset{\theta \sim_{\Delta \sim}}{\mathbb{E}} Q_{\pi_{k}}(s, a)</script><script type="math/tex; mode=display">
\begin{aligned}\text { s.t. } &\mathbb{E}_{s \sim \mathcal{D}_{\Delta} \atop a \sim \pi}\left[L\left(s^{\prime}\right) \mathbb{1}_{\Delta}\left(s^{\prime}\right)-\left(L(s)-\alpha_{3} c\right) \mathbb{1}_{\Delta}(s)\right] \leq 0 \\ &\mathbb{E}_{\mathcal{D}} D_{\mathrm{KL}}\left(\pi_{\theta} \mid \pi_{k}\right) \leq \delta \end{aligned}</script><p>作者使用泰勒二阶展开将其近似为如下优化问题：</p>
<script type="math/tex; mode=display">
\begin{aligned} \theta_{k+1}=& \arg \max _{\theta} g_{Q}^{\top}\left(\theta-\theta_{k}\right) \\ \text { s.t. } & g_{L}^{\top}\left(\theta-\theta_{k}\right)+h \leq 0 \\ & \frac{1}{2}\left(\theta-\theta_{k}\right)^{\top} H\left(\theta-\theta_{k}\right) \leq \delta \end{aligned}</script><p>$g_Q$、$g_L$分别是目标函数和安全约束函数相对于网络参数$\theta$的梯度。$h$是安全约束函数值，$H$是Fisher信息矩阵。</p>
<p>则针对上述凸优化问题，其对偶问题为：</p>
<script type="math/tex; mode=display">
\max _{\lambda, \beta \geq 0} \frac{1}{2 \beta}\left(\mathrm{g}_{Q}^{T} H^{-1} g_{Q}-2 \lambda \mathcal{Z}+\lambda^{2} \mathcal{N}\right)+\lambda h-\frac{\beta \delta}{2}</script><p>其中$\mathcal{Z}=g^T_QH^{-1}g_L$，$\mathcal{N}=g^{T}_LH^{-1}g_L$。</p>
<p>假设原问题有解且$\lambda$和$\beta$是对偶问题的解，则原问题的最优解为：</p>
<script type="math/tex; mode=display">
\theta_{k+1}=\theta_{k}+\frac{1}{\beta^{*}} H^{-1}\left(g_{Q}-\lambda^{*} g_{L}\right)</script><p>若原问题无解，则需要尽快恢复到安全策略：</p>
<script type="math/tex; mode=display">
\begin{aligned} \theta_{k+1} &=\arg \min _{\theta} g_{L}^{\top}\left(\theta-\theta_{k}\right)+h \\ \text { s.t. } & \frac{1}{2}\left(\theta-\theta_{k}\right)^{\top} H\left(\theta-\theta_{k}\right) \leq \delta \end{aligned}</script><p><em>相当于在小范围策略更新的情况下找到离约束最近的策略</em></p>
<p>该解为：</p>
<script type="math/tex; mode=display">
\theta^{*}=\theta_{k}-\sqrt{\frac{2 \delta}{g_{L}^{\top} H^{-1 g_{L}}}} H^{-} 1 g_{L}</script><p>​    LCPO的伪代码如下：</p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227193544818.png" alt="image-20220227193544818"></p>
<p>作者认为<strong>可以先用LSAC在仿真环境中学习初始策略，再将其应用到LCPO接受进一步的在线训练</strong>。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2022/03/05/Robust%20DRL-3/image-20220227200041471.png" alt="image-20220227200041471"></p>
<p><img src="/2022/03/05/Robust%20DRL-3/image-20220227200051457.png" alt="image-20220227200051457"></p>
<p><em>第一行是return 第二行是cost函数的值 第三行是超越安全界限的数量</em></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust DRL学习笔记（1）</title>
    <url>/2022/02/23/Robust-DRL-1/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks</strong></p>
<span id="more"></span>
<p>Richard Cheng Richard M. Murray Joel W. Burdick  California Institute of Technology </p>
<p>AAAI2019</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>在无模型的DRL上通过基于模型信息构建的CBF控制器来修正RL决策（这里作者考虑了CBF“仅仅修正RL的决策动作”和“既考虑修正DRL，又考虑影响DRL的更新”两部分）。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>这篇文章的定位是<strong>可验证的“safe-RL”</strong>，作者认为RL在探索过程中侧重于最大化长期回报，这可能会在学习过程中探索到不安全的行为，这在实际系统中是不允许的</p>
<p>针对safe-RL研究而言，作者从model-free &amp; model-based角度进行现状分析：</p>
<p><strong>（1）model-free</strong>：文献都较老，主要方法有reward-shaping, policy optimization with constraints or teacher advice，并且无法保证在训练初期也能保证DRL决策的安全性。</p>
<p><strong>（2）model-based</strong>: Lyapunov-based or MPC or 备用控制器（实际上是引入了一个风险感知算法，在探索阶段 如果感知到该动作会引起较差的响应，则采用备用控制器的安全动作）。前两种方法没有解决探索和性能优化的问题，最后一种方法对探索过程限制过强</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><h4 id="模型获取（model-based-CBF的需求）"><a href="#模型获取（model-based-CBF的需求）" class="headerlink" title="模型获取（model-based CBF的需求）"></a>模型获取（model-based CBF的需求）</h4><p>给定$S \rightarrow S$的动力学方程：</p>
<script type="math/tex; mode=display">
s_{t+1} = f(s_t) + g(s_t)a_t + d(s_t)</script><p>其中f()代表与a无关的状态，g()中的状态与a有关，d()是不确定项（例如摩擦力和关节柔性）</p>
<p>由于CBF控制器是model-based的，但精准知道模型是很困难的，尤其是d()。故作者采用<strong>Gaussian Process(GP)</strong>估计d()。</p>
<p><img src="https://pic4.zhimg.com/v2-3c25a927c217f13a055794377635faaf_r.jpg" alt="Fig.1 高斯过程更新的图（随着数据点的扩充，高斯过程的均值会越来越接近真实值）"></p>
<p><em>Fig.1 高斯过程更新的图（随着数据点的扩充，高斯过程的均值会越来越接近真实值）</em></p>
<p>理论上我们用于估计d()的数据越多越好，但由于在高斯过程的$\mu$和$\sigma$的求解过程中涉及到求逆的操作，所以为了实时性考虑，实际上只取最新的1000个数据来训练GP model。</p>
<h4 id="CBF控制器设计"><a href="#CBF控制器设计" class="headerlink" title="CBF控制器设计"></a>CBF控制器设计</h4><p>定义安全集合$C$：</p>
<script type="math/tex; mode=display">
\mathcal{C}:\left\{s \in \mathbb{R}^{n}: h(s) \geq 0\right\}</script><p>CBF就是利用李雅普诺夫函数的推广，给出了<strong>在受控动力学下保证安全集C的前向不变性的充分条件</strong>。</p>
<p><strong>定义1</strong>：给定安全集合$C$，一个连续可微的函数$h(\cdot)$作为CBF函数，若存在$\eta \in [0, 1]$使得对所有$s_t$，均有：</p>
<script type="math/tex; mode=display">
\sup _{a_{t} \in A}\left[h\left(f\left(s_{t}\right)+g\left(s_{t}\right) a_{t}+d\left(s_{t}\right)\right)+(\eta-1) h\left(s_{t}\right)\right] \geq 0</script><p>$\eta$就是CBF区别于LF的主要一项，该变量指的是$a_t$将状态$s_{t+1}$推向安全集合的强度。$h(\cdot)$作者定义为一个基本的势垒函数$h=p^{T} s+q$（<em>though our methodology could support more general barrier functions. This  restriction means the set C is composed of intersecting half spaces (i.e.  polytopes).</em>这段不太理解）</p>
<p>代入$h(\cdot)$和$d(\cdot)$后，我们可以得到CBF控制器的目标：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left(a_{t}, \epsilon\right)=\underset{a_{t}, \epsilon}{\operatorname{argmin}} &\left\|a_{t}\right\|_{2}+K_{\epsilon} \epsilon \\
\text { s.t. } \quad & p^{T} f\left(s_{t}\right)+p^{T} g\left(s_{t}\right) a_{t}+p^{T} \mu_{d}\left(s_{t}\right)-\\
& k_{\delta}|p|^{T} \sigma_{d}\left(s_{t}\right)+q \geq(1-\eta) h\left(s_{t}\right)-\epsilon \\
& a_{\text {low }}^{i} \leq a_{t}^{i} \leq a_{\text {high }}^{i} \text { for } i=1, \ldots, M
\end{aligned}</script><p>其中$\epsilon$是松弛变量（<em>松弛变量的引入常常是为了便于在<strong>更大的可行域</strong>内求解。若为0，则收敛到原有状态，若大于零，则约束松弛。</em>）</p>
<p><strong>引理1</strong>：若对于任何$s \in C$，式（4）存在$\epsilon = 0$的解，则该控制器有（1-$\delta$）的概率使得安全集合C是前向不变的（即始终在安全集合内）。若存在$\epsilon=\epsilon^{\max }&gt;0$，则该控制器有（1-$\delta$）的概率使得$\mathcal{C}_{\epsilon}:\left\{s \in \mathbb{R}^{n}: h(s) \geq-\frac{\epsilon}{\eta}\right\}$是前向不变（即始终在限制集合内）。</p>
<h4 id="基于CBF的强化学习补偿控制"><a href="#基于CBF的强化学习补偿控制" class="headerlink" title="基于CBF的强化学习补偿控制"></a>基于CBF的强化学习补偿控制</h4><script type="math/tex; mode=display">
u_{k}(s)=u_{\theta_{k}}^{R L}(s)+u_{k}^{C B F}\left(s, u_{\theta_{k}}^{R L}\right)</script><p><img src="/2022/02/23/Robust-DRL-1/image-20220223093018926.png" alt="Fig2 CBF仅补偿RL控制动作"></p>
<p><em>Fig2 CBF仅补偿RL控制动作</em></p>
<p>CBF并不影响DRL的policy network的更新，只是做一个调控作用，和背景介绍里的“备用控制器”其实比较相似。</p>
<h4 id="基于CBF的强化学习引导控制"><a href="#基于CBF的强化学习引导控制" class="headerlink" title="基于CBF的强化学习引导控制"></a>基于CBF的强化学习引导控制</h4><p><img src="/2022/02/23/Robust-DRL-1/image-20220223093216487.png" alt="Fig3 策略迭代过程"></p>
<p><em>Fig3 策略迭代过程</em></p>
<p>如果仅利用CBF进行补偿，其控制结果如上图a，RL在第k个更新周期里的策略$\pi_{\theta_{k}}^{RL}$可能是很差的，但实际采取的策略是$\pi_{k}$，RL更新如果从$\pi_{k}$处进行更新（每次更新必然也只是在上次策略的一个小邻域内选择最接近$\pi_{opt}$的动作），$\pi_{\theta_{k+1}}^{RL}$会越来越好，逐渐摆脱CBF的补偿，这才是作者想要的效果。</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093310526.png" alt="Fig4 CBF指导强化学习更新"></p>
<p><em>这里利用历史CBF的求和是因为RL的policy更新如今也是依赖于之前的CBF控制值（在上一次的控制值基础上调节）</em></p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{k}(s)=& u_{\theta_{k}}^{R L}(s)+\sum_{j=0}^{k-1} u_{j}^{C B F}\left(s, u_{\theta_{0}}^{R L}, \ldots, u_{\theta_{j-1}}^{R L}\right) \\
&+u_{k}^{C B F}\left(s, u_{\theta_{k}}^{R L}+\sum_{j=0}^{k-1} u_{j}^{C B F}\right)
\end{aligned}</script><p>RL-CBF整体伪代码：</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093421936.png" alt="image-20220223093421936"></p>
<p><em>Tips：k是RL policy更新和GP更新的迭代次数</em></p>
<h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h3><p>倒立摆：</p>
<p><img src="/2022/02/23/Robust-DRL-1/image-20220223093700054.png" alt="*左图表示即使在agent学习的过程中也不会违反安全限制 右图表示最终CBF在实际一个episode里基本不需要补偿*"></p>
<p><em>左图表示即使在agent学习的过程中也不会违反安全限制 右图表示最终CBF在实际一个episode里基本不需要补偿</em></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust DRL学习笔记（4）</title>
    <url>/2022/03/11/Robust%20DRL-4/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>Constrained Policy Optimization</strong></p>
<span id="more"></span>
<p>Joshua Achiam David Held Aviv Tamar Pieter Abbeel (UC Berkeley)<br>       PMLR2017</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>第一个用于约束强化学习的<strong>通用（可以用于DRL）</strong>策略搜索算法，在每次迭代中<strong>保证</strong>近似约束满足。</p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>CMDP：增加了约束条件，限制了该MDP允许的策略集。</p>
<p><em>“A constrained Markov decision process (CMDP) is an MDP augmented with  constraints that restrict the set of allowable policies for that MDP”</em></p>
<p>已有利用启发式算法或是基于原始对偶方法的方法进行CMDP的研究，但无法保证每个学习过程中都能满足约束，存在保证了约束的工作，但假设条件太多，不通用。</p>
<p>其他与之前笔记重复内容不再赘述。</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><p><strong>CMDP：</strong></p>
<script type="math/tex; mode=display">
C_{i}: J_{C_{i}}(\pi)=\mathrm{E}\left[\sum_{t=0}^{\infty} \gamma^{t} C_{i}\left(s_{t}, a_{t}, s_{t+1}\right)\right]\\
\Pi_{C} \doteq\left\{\pi \in \Pi: \forall i, J_{C_{i}}(\pi) \leq d_{i}\right\}\\
\pi^{*}=\arg \max _{\pi \in \Pi_{C}} J(\pi)</script><p>$J(\pi)$就是折扣return的期望，$J_{C_i}(\pi)$是折扣约束的期望，目标是在满足$J_{C_i}(\pi)&lt;d_i$的情况下最大化$J(\pi)$。</p>
<p><strong>Constrained Policy Optimization(CPO):</strong></p>
<p>对于大型的MDP而言，若想得出最优策略是十分复杂的，因此利用神经网络等策略搜索算法逐步解决该问题：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\pi_{k+1}=\arg \max _{\pi \in \Pi_{\theta}}  J(\pi) \\
& \text { s.t. } D\left(\pi, \pi_{k}\right) \leq \delta
\end{aligned}</script><p>$D$是度量新旧策略差异的度量函数，每次策略更新是在很小的邻域内。</p>
<p>所以，对于CMDP而言，其更新可以看作：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{k+1}=&\arg \max _{\pi \in \Pi_{\theta}}  J(\pi) \\
\text { s.t. } & J_{C_{i}}(\pi) \leq d_{i} \quad i=1, \ldots, m \\
& D\left(\pi, \pi_{k}\right) \leq \delta .
\end{aligned}</script><p>如果完全按照上式进行更新，则也是计算复杂的，所以作者使用了 Trust Region Optimization(TRO)近似计算上式。</p>
<hr>
<p>作者首先证明了两个随机策略的returns的差的界限；之后证明了在每次更新时最坏情况下的性能下降保证。</p>
<p><strong>Policy Performance Bounds</strong></p>
<p>已有证明表示：</p>
<script type="math/tex; mode=display">
J\left(\pi^{\prime}\right)-J(\pi)=\frac{1}{1-\gamma} \underset{s \sim d^{\pi^{\prime} \atop} \atop a \sim \pi^{\prime}}{\mathrm{E}}\left[A^{\pi}(s, a)\right]</script><p>其中$d^{\pi}(s)=(1-\gamma) \sum_{t=0}^{\infty} \gamma^{t} P\left(s_{t}=s \mid \pi\right)$表示discounted 未来状态的分布。（该证明没懂）</p>
<p><strong>理论1：</strong>对于任意一个关于S的函数f，任意的策略$\pi$和$\pi^{\prime}$，定义估计与真实值的误差为$\delta_{f}\left(s, a, s^{\prime}\right) \doteq R\left(s, a, s^{\prime}\right)+\gamma f\left(s^{\prime}\right)-f(s)$，</p>
<p>同时定义：</p>
<script type="math/tex; mode=display">
\epsilon_{f}^{\pi^{\prime}} \doteq \max _{s}\left|\mathrm{E}_{a \sim \pi^{\prime}, s^{\prime} \sim P}\left[\delta_{f}\left(s, a, s^{\prime}\right)\right]\right|</script><script type="math/tex; mode=display">
L_{\pi, f}\left(\pi^{\prime}\right) \doteq \underset{s \sim d^{\pi} ,a \sim \pi  \atop s^{\prime} \sim P}{\mathrm{E}}\left[\left(\frac{\pi^{\prime}(a \mid s)}{\pi(a \mid s)}-1\right) \delta_{f}\left(s, a, s^{\prime}\right)\right]</script><script type="math/tex; mode=display">
D_{\pi, f}^{\pm}\left(\pi^{\prime}\right) \doteq \frac{L_{\pi, f}\left(\pi^{\prime}\right)}{1-\gamma} \pm \frac{2 \gamma \epsilon_{f}^{\pi^{\prime}}}{(1-\gamma)^{2}} \underset{s \sim d^{\pi}}{\mathrm{E}}\left[D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right]</script><p>其中$D_{TV}\left(\pi^{\prime} | \pi\right)[s]=(1 / 2) \sum_{a}\left|\pi^{\prime}(a \mid s)-\pi(a \mid s)\right|$。则以下界限存在：</p>
<script type="math/tex; mode=display">
D_{\pi, f}^{+}\left(\pi^{\prime}\right) \geq J\left(\pi^{\prime}\right)-J(\pi) \geq D_{\pi, f}^{-}\left(\pi^{\prime}\right)</script><p><strong>推论1：</strong>对任意策略$\pi^{\prime}$,$\pi$，有$\epsilon^{\pi^{\prime}} \doteq \max _{s}\left|\mathrm{E}_{a \sim \pi^{\prime}}\left[A^{\pi}(s, a)\right]\right|$ ，则以下界限存在：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&J\left(\pi^{\prime}\right)-J(\pi) \\
&\geq \frac{1}{1-\gamma} \underset{s \sim d^{\pi} \atop a \sim \pi^{\prime}}{\mathrm{E}}\left[A^{\pi}(s, a)-\frac{2 \gamma \epsilon^{\pi^{\prime}}}{1-\gamma} D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right]
\end{aligned}</script><p><strong>推论2：</strong>对任意策略$\pi^{\prime}$,$\pi$，任意代价函数$C_i$，且$\epsilon_{C_{i}}^{\pi^{\prime}} \doteq \max _{s}\left|\mathrm{E}_{a \sim \pi^{\prime}}\left[A_{C_{i}}^{\pi}(s, a)\right]\right|$，则以下界限成立：</p>
<script type="math/tex; mode=display">
J_{C_{i}}\left(\pi^{\prime}\right)-J_{C_{i}}(\pi)
\leq \frac{1}{1-\gamma} \underset{s \sim d^{\pi} \atop a \sim \pi^{\prime}}{\mathrm{E}}\left[A_{C_{i}}^{\pi}(s, a)+\frac{2 \gamma \epsilon_{C_{i}}^{\prime}}{1-\gamma} D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right]</script><p>上述理论和证明定义的界限都是针对<strong>TV散度</strong>而言的，而信域方法是利用<strong>KL散度</strong>，故通过Pinsker不等式将二者建立联系如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underset{s \sim d^{\pi}}{\mathrm{E}}\left[D_{T V}\left(\pi^{\prime} \| \pi\right)[s]\right] & \leq \underset{s \sim d^{\pi}}{\mathrm{E}}\left[\sqrt{\frac{1}{2} D_{K L}\left(\pi^{\prime} \| \pi\right)[s]}\right] \\
& \leq \sqrt{\frac{1}{2} \underset{s \sim d^{\pi}}{\mathrm{E}}\left[D_{K L}\left(\pi^{\prime} \| \pi\right)[s]\right]}
\end{aligned}</script><p>KL散度计算公式为：$K L(p | q)=\sum p(x) \log \frac{p(x)}{q(x)}$。</p>
<p><strong>推论3：</strong>将理论1和推论1、2中的TV散度换为KL散度即可得到一组和KL散度相关的边界条件。</p>
<p><strong>Trust Region Methods</strong></p>
<p>信赖域算法的策略更新采取以下形式：</p>
<script type="math/tex; mode=display">
\begin{array}{r}
\pi_{k+1}=\arg \max _{\pi \in \Pi_{\theta}} \underset{s \sim d^{\pi} k}{\mathrm{E}}\left[A^{\pi_{k}}(s, a)\right] \\
\text { s.t. } \bar{D}_{K L}\left(\pi \| \pi_{k}\right) \leq \delta
\end{array}</script><p>其中$D_{KL}$的约束被称为信赖域（策略更新前后的策略性能之差可用优势函数的折扣累加期望）。</p>
<p><strong>命题1（性能保证）：</strong>假设$\pi_{k}$按照式（12）进行更新，则策略更新前后的性能差异有：</p>
<script type="math/tex; mode=display">
J\left(\pi_{k+1}\right)-J\left(\pi_{k}\right) \geq \frac{-\sqrt{2 \delta} \gamma \epsilon^{\pi_{k+1}}}{(1-\gamma)^{2}}
\\
where \quad\epsilon^{\pi_{k+1}}=\max _{s}\left|\mathrm{E}_{a \sim \pi_{k+1}}\left[A^{\pi_{k}}(s, a)\right]\right|.</script><p>考虑信赖域的约束条件和安全约束条件，结合推论1、2、3，有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\pi_{k+1}=\arg \max _{\pi \in \Pi_{\theta}} \underset{s \sim d^{\pi_{k}} \atop a \sim \pi}{\mathrm{E}}\left[A^{\pi_{k}}(s, a)\right]-\alpha_{k} \sqrt{\bar{D}_{K L}\left(\pi \| \pi_{k}\right)} \\
&\text { s.t. } J_{C_{i}}\left(\pi_{k}\right)+\underset{s \sim d^{\pi_{k}} \atop a \sim \pi}{\mathrm{E}}\left[\frac{A_{C_{i}}^{\pi_{k}}(s, a)}{1-\gamma}\right]+\beta_{k}^{i} \sqrt{\bar{D}_{K L}\left(\pi \| \pi_{k}\right)} \leq d_{i}
\end{aligned}</script><p>基于此，作者提出<strong>CPO</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi_{k+1}=& \arg \max _{\pi \in \Pi_{\theta}} \underset{s \sim d^{\pi_{k}}}{\mathrm{E}}\left[A^{\pi_{k}}(s, a)\right] \\
\text { s.t. } &J_{C_{i}}\left(\pi_{k}\right)+\frac{1}{1-\gamma} \underset{s \sim d^{\pi_{k}}}{\mathrm{E}}\left[A_{C_{i}}^{\pi_{k}}(s, a)\right] \leq d_{i} \quad \forall i \\
& \bar{D}_{K L}\left(\pi \| \pi_{k}\right) \leq \delta .
\end{aligned}</script><p>​    因为这是一个信赖域方法，所以继承了命题1的性能保证。此外，根据推论2和3，又<strong>近似满足</strong>安全约束。故最终达到了满足安全约束的性能保证。</p>
<p><strong>命题2（CPO更新的最坏情况）：</strong>安全约束$C_i-return$的上界为：    </p>
<script type="math/tex; mode=display">
J_{C_{i}}\left(\pi_{k+1}\right) \leq d_{i}+\frac{\sqrt{2 \delta} \gamma \epsilon_{C_{i}}^{\pi_{k+1}}}{(1-\gamma)^{2}}
\\
where \quad\epsilon_{C_{i}}^{\pi_{k+1}}=\max _{s}\left|\mathrm{E}_{a \sim \pi_{k+1}}\left[A_{C_{i}}^{\pi_{k}}(s, a)\right]\right|.</script><p><strong>CPO如何求解？</strong></p>
<p>定义$c_{i}=J_{C_i}(\pi_k)-d$，则公式（15）可以近似为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{k+1}=\arg \max _{\theta} & g^{T}\left(\theta-\theta_{k}\right) \\
\text { s.t. } \quad & c_{i}+b_{i}^{T}\left(\theta-\theta_{k}\right) \leq 0 \quad i=1, \ldots, m \\
& \frac{1}{2}\left(\theta-\theta_{k}\right)^{T} H\left(\theta-\theta_{k}\right) \leq \delta
\end{aligned}</script><p>$H$是费希尔信息矩阵，总是半正定的且式（17）优化问题是<strong>凸的</strong>（上式是一个带线性和二次约束的线性目标函数凸优化问题(LQCLP)），故可以用拉格朗日对偶方法求解：</p>
<script type="math/tex; mode=display">
\max _{\lambda \geq 0 \atop \nu \succeq 0} \frac{-1}{2 \lambda}\left(g^{T} H^{-1} g-2 r^{T} \nu+\nu^{T} S \nu\right)+\nu^{T} c-\frac{\lambda \delta}{2}</script><p>其中$r \doteq g^{T} H^{-1} B, S \doteq B^{T} H^{-1} B$。若$\lambda^<em>，v^</em>$是式（18）的解，则原问题的解为：</p>
<script type="math/tex; mode=display">
\theta^{*}=\theta_{k}+\frac{1}{\lambda^{*}} H^{-1}\left(g-B \nu^{*}\right)</script><p>由于近似是存在误差的，有时利用式（17）是安全有效的，但对于不可行的情况，作者利用下式更新：</p>
<script type="math/tex; mode=display">
\theta^{*}=\theta_{k}-\sqrt{\frac{2 \delta}{b^{T} H^{-1} b}} H^{-1} b</script><p>该更新用于信赖域和约束域的交集为空集时采取的更新措施。</p>
<p>伪代码：</p>
<p><img src="/2022/03/11/Robust%20DRL-4/image-20220307152139547.png" alt="image-20220307152139547"></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2022/03/11/Robust%20DRL-4/image-20220307152526198.png" alt="image-20220307152526198"></p>
<p><em>CPO保持在限制内（感觉都在限制值那块聚集？）更多 TRPO无效</em></p>
<p><img src="/2022/03/11/Robust%20DRL-4/image-20220307152950704.png" alt="image-20220307152950704"></p>
<p><em>CPO+cost shaping 更有效</em></p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Robust DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>Barrier Function学习笔记</title>
    <url>/2022/05/01/QP-1/</url>
    <content><![CDATA[<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>Barrier Function最先用于优化问题，现在也有研究利用障碍函数将约束最优控制方法转化为无约束最优控制。</p>
<span id="more"></span>
<p>通常使用与集合C相关联的障碍函数的两个概念：</p>
<p><strong>（1）Reciprocal Barrier Function(RBF)</strong></p>
<p>$x$在集合$C$的边界处，障碍函数$B(x)$无界：$B(x) \rightarrow \infty$ 当 $x \rightarrow \partial \mathcal{C}$</p>
<p><strong>（2）Zeroing Barrier Function(ZBF)</strong></p>
<p>$x$在集合$C$的边界处，障碍函数$B(x)$为0：$h(x) \rightarrow 0$ 当 $x \rightarrow \partial \mathcal{C}$</p>
<p>对于这两种情况，若$B$或$h$满足Lyapunov-like条件，则在集合C内的前向不变性是可以保证的。</p>
<h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><p>$\operatorname{Int}(\mathcal{C})$ 和$\partial \mathcal{C}$分别表示集合$C$的内部和边界。</p>
<p>定义一个中心在0处的开球： $B_{\varepsilon}=\{x \in$ $\left.\mathbb{R}^{n} \mid|x|&lt;\varepsilon\right\}$, $\varepsilon \in \mathbb{R}^{+}$</p>
<p>若一个连续函数定义域和值域均为非负数，且严格单调递增，在0处取0，则该函数属于$K$类：</p>
<p>$\beta_{1}:[0, a) \rightarrow[0, \infty)$ , $a&gt;0$ 且 $\beta_{1}(0)=0$.</p>
<p>若一个连续函数定义域和值域可以取负数，且严格单调递增，在0处取0，则该函数属于扩展$K$类：</p>
<p>$\alpha:(-b, a) \rightarrow(-\infty, \infty)$， $a, b&gt;0$ 且$\alpha(0)=0$.</p>
<p>sup：上确界, inf：下确界</p>
<h3 id="RBF-amp-ZBF"><a href="#RBF-amp-ZBF" class="headerlink" title="RBF &amp; ZBF"></a>RBF &amp; ZBF</h3><p>考虑一个非线性系统：</p>
<script type="math/tex; mode=display">
\begin{equation}
\dot{x}=f(x)
\end{equation}</script><h4 id="RBF"><a href="#RBF" class="headerlink" title="RBF"></a>RBF</h4><p>集合C满足：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\mathcal{C} &=\left\{x \in \mathbb{R}^{n}: h(x) \geq 0\right\} \\
\partial \mathcal{C} &=\left\{x \in \mathbb{R}^{n}: h(x)=0\right\} \\
\operatorname{Int}(\mathcal{C}) &=\left\{x \in \mathbb{R}^{n}: h(x)>0\right\}
\end{aligned}
\end{equation}</script><p>满足Lyapunov-like条件的BF候选函数有：</p>
<script type="math/tex; mode=display">
\begin{equation}
B(x)=-\log \left(\frac{h(x)}{1+h(x)}\right)
\end{equation}</script><p>该函数满足以下两条基本条件：</p>
<script type="math/tex; mode=display">
\begin{equation}
\inf _{x \in \operatorname{Int}(\mathcal{C})} B(x) \geq 0, \quad \lim _{x \rightarrow \partial \mathcal{C}} B(x)=\infty
\end{equation}</script><p>那么问题来了，设计什么条件才能使$B(x)$在$\operatorname{Int}(\mathcal{C})$内是前向不变的呢？</p>
<p>理论上应该设计条件$\dot{B} \leq 0$，但这样是没必要的，我们只需要在越接近$C$的边界处设置该标准即可。</p>
<p>故设计条件：</p>
<script type="math/tex; mode=display">
\begin{equation}
\dot{B} \leq \frac{\gamma}{B}
\end{equation}</script><p>使得$B$在远离边界时可以增大，越接近边界增大速率越接近于0（因为B在边界处趋于无穷，上述条件使其导数趋于0）。</p>
<p>除此之外，另一个可能的候选函数为：</p>
<script type="math/tex; mode=display">
\begin{equation}
B(x) = \frac{1}{h(x)}
\end{equation}</script><p>RBF更一般的形式：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
&\frac{1}{\alpha_{1}(h(x))} \leq B(x) \leq \frac{1}{\alpha_{2}(h(x))}\\
&L_{f} B(x) \leq \alpha_{3}(h(x)) .
\end{aligned}
\end{equation}</script><p>其中，α1，α2，α3是上述定义的K类函数，$h(x)$是连续可微函数。</p>
<p><strong>定理1</strong>：给定动态系统（1）和一个由连续可微函数$h$定义的集合$C$，如果存在$B$是一个RBF，那么在$\operatorname{Int}(\mathcal{C})$上是前向不变的。</p>
<h4 id="ZBF"><a href="#ZBF" class="headerlink" title="ZBF"></a>ZBF</h4><p>在RBF定义中，当其自变量接近C的边界时，RBF趋向于无穷大，然而，当考虑real-time/embedded implementations（不理解）时，这可能是不希望被达到的。故设计了ZBF。</p>
<p>一般的ZBF形式如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
L_{f} h(x) \geq-\alpha(h(x))
\end{equation}</script><p>其中$\alpha$是扩展K函数。</p>
<p><em>注意：将$h$定义在一个比$C$ 大的集合$D$上可以考虑模型扰动的影响。</em></p>
<p>和定理1类似，ZBF的存在可以保证$\operatorname{Int}(\mathcal C)$的前向不变性。</p>
<p>且即使初始位置在集合$\mathcal C$之外，也有$x$渐进收敛于$\mathcal C$。</p>
<p>互斥控制屏障函数。</p>
<p><strong>二者与前向不变性的关系</strong>：</p>
<p>ZBF和RBF均为$\operatorname{int}(\mathcal C)$前向不变性的充分条件</p>
<p><img src="/2022/05/01/QP-1/image-20220423164317970.png" alt="image-20220423164317970"></p>
<h3 id="利用RBF-amp-ZBF设计CBF"><a href="#利用RBF-amp-ZBF设计CBF" class="headerlink" title="利用RBF&amp;ZBF设计CBF"></a>利用RBF&amp;ZBF设计CBF</h3><p>虽然障碍函数是验证集合不变性的重要工具，但它们不能直接用于设计强制不变性的控制器。</p>
<h4 id="RCBF"><a href="#RCBF" class="headerlink" title="RCBF"></a>RCBF</h4><p>考虑一个仿射控制系统：</p>
<script type="math/tex; mode=display">
\begin{equation}
\dot{x}=f(x)+g(x) u
\end{equation}</script><p>$f$ 和 $g$ 是局部李普希茨连续，$x \in \mathbb{R}^{n}$ 且$u \in U \subset \mathbb{R}^{m}$，其中$U$定义为：</p>
<script type="math/tex; mode=display">
\begin{equation}
U=\left\{u \in \mathbb{R}^{m} \mid A_{0} u \leq b_{0}\right\}
\end{equation}</script><p>其中$A_0$是$p \times m$矩阵，$b_0$是$p \times 1$向量，在无约束的系统动态下，$\operatorname{Int}(\mathcal{C})$不是前向不变的。</p>
<p>那么根据RCBF的定理1，若存在$\mathcal K$类函数α1，α2，α3，使得：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{gathered}
\frac{1}{\alpha_{1}(h(x))} \leq B(x) \leq \frac{1}{\alpha_{2}(h(x))} \\
\inf _{u \in U}\left[L_{f} B(x)+L_{g} B(x) u-\alpha_{3}(h(x))\right] \leq 0
\end{gathered}
\end{equation}</script><p>即若我们选取的控制动作u满足下述集合：</p>
<script type="math/tex; mode=display">
\begin{equation}
K_{\mathrm{rcbf}}(x)=\left\{u \in U: L_{f} B(x)+L_{g} B(x) u-\alpha_{3}(h(x)) \leq 0\right\}
\end{equation}</script><p>则可以保证C的前向不变性，其中，$h(x)$是之前公式3定义的连续可微函数。</p>
<h4 id="ZCBF"><a href="#ZCBF" class="headerlink" title="ZCBF"></a>ZCBF</h4><p>同理，若存在扩展$\mathcal K$类函数$\alpha$，使得：</p>
<script type="math/tex; mode=display">
\begin{equation}
\sup _{u \in U}\left[L_{f} h(x)+L_{g} h(x) u+\alpha(h(x))\right] \geq 0, \forall x \in \mathcal{D}
\end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
K_{\mathrm{zcbf}}(x)=\left\{u \in U: L_{f} h(x)+L_{g} h(x) u+\alpha(h(x)) \geq 0\right\}
\end{equation}</script><p>其中，$D$是比$C$大一些的集合。</p>
<h3 id="CLF-CBF-QP"><a href="#CLF-CBF-QP" class="headerlink" title="CLF-CBF-QP"></a>CLF-CBF-QP</h3><p>目的是利用QP来协调控制效果和安全约束。</p>
<p>首先写出系统动态：</p>
<script type="math/tex; mode=display">
\begin{equation}
\left(\begin{array}{c}
\dot{x}_{1} \\
\dot{x}_{2}
\end{array}\right)=\underbrace{\left(\begin{array}{c}
f_{1}\left(x_{1}, x_{2}\right) \\
f_{2}\left(x_{1}, x_{2}\right)
\end{array}\right)}_{f(x)}+\underbrace{\left(\begin{array}{c}
g_{1}\left(x_{1}, x_{2}\right) \\
0
\end{array}\right)}_{g(x)} u
\end{equation}</script><p>可以看出状态$x_1$是受控状态，$x_2$是不受控状态。</p>
<p>指数稳定控制李亚普诺夫函数<strong>Exponentially stabilizing Control Lyapunov Function(CLF)</strong>的一般形式如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{gathered}
c_{1}\left\|x_{1}\right\|^{2} \leq V(x) \leq c_{2}\left\|x_{1}\right\|^{2} \\
\inf _{u \in U}\left[L_{f} V(x)+L_{g} V(x) u+c_{3} V(x)\right] \leq 0
\end{gathered}
\end{equation}</script><p>因此，我们可以利用CLF作为控制性能目标的约束，CBF作为安全目标的约束设立QP，通过放松CLF条件（式17）的约束，并调整松弛参数上的权重，QP可以在保证安全性的情况下协调性能和安全性之间的权衡。具体而言：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\mathbf{u}^{*}(x)=& \underset{\mathbf{u}=(u, \delta) \in \mathbb{R}^{m} \times \mathbb{R}}{\operatorname{argmin}} \frac{1}{2} \mathbf{u}^{\top} H(x) \mathbf{u}+F(x)^{\top} \mathbf{u} \\
\text { s.t. } & L_{f} V(x)+L_{g} V(x) u+c_{3} V(x)-\delta \leq 0, \\
& L_{f} B(x)+L_{g} B(x) u-\alpha(h(x)) \leq 0,
\end{aligned}
\end{equation}</script><p>其中 $c_{3}&gt;0$ 且是常数， $\alpha$ 属于类别 $\mathcal{K}, H(x) \in$ $\mathbb{R}^{(m+1) \times(m+1)}$ 正定， $F(x) \in \mathbb{R}^{m+1}$。</p>
<h3 id="Eg-Lane-Keeping-Via-QPs"><a href="#Eg-Lane-Keeping-Via-QPs" class="headerlink" title="Eg: Lane Keeping Via QPs"></a>Eg: Lane Keeping Via QPs</h3><p>使用基于CBF的QP来考虑车道保持(LK)问题，该QP旨在使车辆在可能弯曲的车道上保持“中心”。假设车辆具有恒定的纵向速度，这样我们只需要关注车辆的横向运动。</p>
<p>安全约束：</p>
<p>$|y| \leq y_{\max }$，车辆距车道中心的位移小于给定的常量$y_{max}$</p>
<p>$|\ddot{y}| \leq a_{\max }$，为了驾驶员的舒适性，侧向加速度需要有界</p>
<p>要结合CBF，主要需要找两个函数，一个$h_F(x)$满足式3，一个BF函数，可选为RBF/ZBF如上式4、7、12等。</p>
<p>例如此题选择$h_F(x)$：</p>
<script type="math/tex; mode=display">
\begin{equation}
h_{F}(x)=\left(y_{\max }-\operatorname{sgn}(\dot{y}) y\right)-\frac{1}{2} \frac{\dot{y}^{2}}{a_{\max }}
\end{equation}</script><p>则有$\mathcal{C}_{F}:=\left\{x \mid h_{F}(x) \geq 0\right\}$。</p>
<p>选择$B_{F}(x)$：</p>
<script type="math/tex; mode=display">
\begin{equation}
B_{F}(x)=\frac{1}{h_F(x)}
\end{equation}</script><p>则对于$x \in \operatorname{Int}\left(\mathcal{C}_{F}\right)$，若总存在控制动作$u$：</p>
<script type="math/tex; mode=display">
\begin{equation}
L_{f} B_{F}(x)+L_{g} B_{F}(x) u-\frac{\gamma}{B_{F}(x)} \leq 0
\end{equation}</script><p>则$\operatorname{Int}\left(\mathcal{C}_{F}\right)$是前向不变的。</p>
<p>这样，我们就可以利用QP去解出最优的$u$。</p>
<p>结果：</p>
<p><img src="/2022/05/01/QP-1/image-20220423194127174.png" alt="image-20220423194127174"></p>
<p>可以看到无论是横行位移还是横向加速度都在限定范围内。</p>
]]></content>
      <categories>
        <category>数学基础</category>
      </categories>
      <tags>
        <tag>CBF</tag>
      </tags>
  </entry>
  <entry>
    <title>模型推断攻击学习笔记（1）</title>
    <url>/2022/05/17/Property%20Inference-1/</url>
    <content><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p><strong>Property Inference from Poisoning</strong><br><span id="more"></span></p>
<p>Melissa Chase， Esha Ghosh， Saeed Mahloujifar</p>
<p>Microsoft Research Princeton University</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>攻击者可以恶意控制部分训练数据(中毒数据)以增加泄密性，从而进行属性推理。</p>
<p>结论： <strong>Poisoning Attacks Increasing Information Leakage！</strong></p>
<h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p><strong>推断攻击</strong>：攻击者试图通过检查训练集上的模型来推断训练集的敏感信息。推断攻击有两种主要形式：<strong>成员推理攻击（membership inference）和属性推理攻击（ property inference）</strong>。</p>
<p>在成员推断攻击中，对手试图推断用于训练给定模型的训练集中<strong>是否存在特殊实例</strong>。在属性推断攻击中，对手试图推断出关于整个训练集的一些<strong>聚合信息</strong>。被推断的属性不需要是训练集中的显式特征，也不需要与训练集标签明显相关。例如，我们将考虑对基于文本的模型(特别是垃圾邮件分类器)的属性推断攻击，该攻击试图学习训练数据集中文档的平均情感(正面或负面)。</p>
<p><strong>投毒攻击：</strong>训练数据(中毒数据)的某一部分是由希望使训练模型按照自己的兴趣行事的对手精心选择的。</p>
<p>全局属性推断攻击：从不打算共享的训练模型中推断训练数据集的敏感全局属性，但已有的会需要白盒访问模型，这意味着它被赋予神经网络中的所有权重。（作者的方法是毒化训练数据）</p>
<p>亚群攻击（subpopulation attack）：毒化部分训练数据，即只有对来自数据中某个亚群的输入的预测受到影响。</p>
<p>可保证的中毒攻击（poisoning attacks with provable guarantees）：中毒攻击对使用凸损失最小化的学习算法具有可证明的保证，作者的攻击对任何具有良好泛化能力的学习算法都是可证明的，并且不需要干净训练集的知识。</p>
<h3 id="详细内容"><a href="#详细内容" class="headerlink" title="详细内容"></a>详细内容</h3><p>模型假设</p>
<p>考虑一个攻击者：</p>
<p>（1）可以从样本集中采样并提交一组“中毒”数据，这些数据将与受害者数据集相结合，并用于训练模型。但攻击者不知道从其他来源采样的“干净”训练集；</p>
<p>（2）攻击者可以对经过训练的模型进行有限次数的black box queries。但仅限查询的样本label，它不会获得模型产生的置信度值，也不会从模型中获得任何内部信息；</p>
<p>（3）攻击者的目标是推断特定属性的平均值是高于还是低于特定阈值。</p>
<p>​        属性1：存在于数据集中的属性（例如人口普查数据集中的性别和种族）；</p>
<p>​        属性2：不作为特征出现在训练数据中，但是可以从那些特征中导出的属性（例如安然数据集中电子邮件中的负面情绪(由情绪分析确定)；</p>
<p>​        属性3：与训练数据或分类任务不相关的属性（为人口普查和安然数据中的每个数据条目添加了一个独立选择的随机二进制特征）</p>
<h4 id="属性推断是否构成重要的威胁模型？"><a href="#属性推断是否构成重要的威胁模型？" class="headerlink" title="属性推断是否构成重要的威胁模型？"></a>属性推断是否构成重要的威胁模型？</h4><p>例1：假设一家公司想要使用其内部电子邮件来训练垃圾邮件分类器。这种模型有望揭示哪些单词组合通常表示垃圾邮件，公司可能会乐于分享这些信息。然而，使用属性推断攻击，得到的模型也可能泄露关于公司中电子邮件的总体情绪的信息，这可能是非常敏感的。例如，如果公司电子邮件的情绪在临近财务季度时变得消极，这可能意味着公司的表现低于预期。</p>
<p>例2：一家金融公司可能愿意分享一个检测欺诈的模型，但可能不愿意透露各种类型的交易量。</p>
<p>例3：许多较小的公司可能愿意共享一个模型来实现产品价格调整，但是这些公司可能不愿意共享不同类型产品的具体销售数字。</p>
<h4 id="为什么可以通过投毒来进行属性推断？"><a href="#为什么可以通过投毒来进行属性推断？" class="headerlink" title="为什么可以通过投毒来进行属性推断？"></a>为什么可以通过投毒来进行属性推断？</h4><p>任何加密或硬件辅助解决方案都不会阻止训练数据的一些输入源的数据，在所有这些解决方案中，都没有切实可行的方法来保证输入的数据实际上是正确的。</p>
<h4 id="攻击模型设计"><a href="#攻击模型设计" class="headerlink" title="攻击模型设计"></a>攻击模型设计</h4><p>将数据集映射到某个假设的类别：</p>
<script type="math/tex; mode=display">
L:(\mathcal{X} \times \mathcal{Y})^{*} \rightarrow \mathcal{H}</script><script type="math/tex; mode=display">
f: \mathcal{X} \rightarrow\{0,1\}</script><p>攻击者的目标是从训练集及H中找到属性f的统计信息，即学习关于 $\hat{f}(\mathcal{T})$ 的信息：</p>
<script type="math/tex; mode=display">
\hat{f}=\mathbf{E}_{(x, y) \leftarrow \mathcal{T}}[f(x)]</script><p>将f=0的样本定义为负样本$D_-$，f=1的样本定义为正样本$D_+$，再以一定比例混合：</p>
<script type="math/tex; mode=display">
D_{t} \equiv t \cdot D_{+}+(1-t) \cdot D_{-}</script><h4 id="带有贝叶斯最优分类器可证明保证的理论攻击"><a href="#带有贝叶斯最优分类器可证明保证的理论攻击" class="headerlink" title="带有贝叶斯最优分类器可证明保证的理论攻击"></a>带有贝叶斯最优分类器可证明保证的理论攻击</h4><p>预测风险：（数据集中的预测类别和label不同的概率）</p>
<script type="math/tex; mode=display">
\operatorname{Risk}(h, D)=\operatorname{Pr}_{(x, y) \leftarrow D}[h(x) \neq y]</script><p>贝叶斯误差：</p>
<script type="math/tex; mode=display">
\operatorname{Bayes}(D)=\inf _{h: \mathcal{X} \rightarrow\{0,1\}} \operatorname{Risk}(h, D) .</script><p>贝叶斯最优分类器：</p>
<script type="math/tex; mode=display">
\forall x \in \mathcal{X}: h_{D}^{*}(x)=\underset{y \in\{0,1\}}{\operatorname{argmax}} \operatorname{Pr}[Y=y \mid X=x]</script><p>贝叶斯误差是分类器所能期望的最小误差。算法试图通过模仿贝叶斯最优分类器的行为来获得接近贝叶斯误差的错误率。下面，定义学习算法L，可以学习贝叶斯最优分类器的一类分布即使这样一,但即使对于高泛化程度的贝叶斯模型也是容易受到攻击。</p>
<p>(ε，δ)-贝叶斯最优学习算法：</p>
<p>给定一个数据集$\mathcal{T} \leftarrow D^{n}$与 从中采集的n 个样本，学习算法输出一个模型 h 使得有 $1-\delta(n)$ 的置信度满足：</p>
<script type="math/tex; mode=display">
\operatorname{Risk}(h, D) \leq \operatorname{Bayes}(D)+\varepsilon(n) .</script><p>Signed certainty：</p>
<p>训练集D中的每个点x的Signed certainty定义为：</p>
<script type="math/tex; mode=display">
\operatorname{crt}(x, D)=1-2 \operatorname{Pr}[Y=1 \mid X=x]</script><p><strong>定理</strong>：考虑一个学习算法是(ε，δ)-贝叶斯最优学习算法，对于任意$p,t_0&lt;t_1\in[0,1]$，若存在$\tau \in[0,1]$，满足：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\operatorname{Pr}_{x \leftarrow X}\left[\frac{p+2 \tau \cdot t_{1}}{t_{1}(1-p)}<\operatorname{crt}(x, D) \leq \frac{p-2 \tau \cdot t_{0}}{t_{0}(1-p)}\right. \\
\wedge \quad f(x)=1]>\frac{2 \epsilon(n)}{\tau}
\end{gathered}</script><p><strong>则攻击者至少有1-2δ (n)的概率赢得PIWP。</strong></p>
<p>如果在具有高不确定性的分布中存在足够多的点，且学习算法对于足够多的分布是贝叶斯最优的，那么作者的攻击就能成功地区分 $D_{t_0}$和 $D_{t_1}$。</p>
<p><img src="/2022/05/17/Property%20Inference-1/image-20220516151840452.png" alt="image-20220516151840452"></p>
<h4 id="评估攻击"><a href="#评估攻击" class="headerlink" title="评估攻击"></a>评估攻击</h4><p>（1）首先sample一些样本（p概率从投毒数据集中sample (1-p)从原始数据集中sample），假设投毒样本对应的Y都是1</p>
<p>（2）观察投毒对后验分布的影响：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{Pr}[\tilde{Y}=1 \mid \tilde{X}=x]=& \frac{p}{p+t(1-p)} \\
&+\frac{t(1-p)}{p+t(1-p)} \cdot \operatorname{Pr}[Y=1 \mid X=x]
\end{aligned}</script><p><img src="/2022/05/17/Property%20Inference-1/image-20220517101736190.png" alt="image-20220517101736190"></p>
<p>$t=\operatorname{Pr}[f(X)=1]$表示正常样本中属性为1对应的概率。</p>
<p>（3）观察对贝叶斯最优分类器的影响：</p>
<p><img src="/2022/05/17/Property%20Inference-1/image-20220517102517812.png" alt="image-20220517102517812"></p>
<p><img src="/2022/05/17/Property%20Inference-1/image-20220517102529000.png" alt="image-20220517102529000"></p>
<h4 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h4><p><img src="/2022/05/17/Property%20Inference-1/image-20220517105124915.png" alt="image-20220517105124915"></p>
<p><img src="/2022/05/17/Property%20Inference-1/image-20220517105726819.png" alt="image-20220517105726819"></p>
<p><img src="/2022/05/17/Property%20Inference-1/image-20220517105737425.png" alt="image-20220517105737425"></p>
<h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h3><p><img src="/2022/05/17/Property%20Inference-1/image-20220517111944829.png" alt="image-20220517111944829"></p>
<p>重点看推断属性的accuracy。</p>
]]></content>
      <categories>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Property Inference</tag>
      </tags>
  </entry>
  <entry>
    <title>SB中常见DRL算法的损失函数设置</title>
    <url>/2022/05/19/DRL-Loss/</url>
    <content><![CDATA[<h3 id="内容概述"><a href="#内容概述" class="headerlink" title="内容概述"></a>内容概述</h3><p>本文主要针对open-AI开源的SB 3（<a href="https://github.com/hill-a/stable-baselines）中部分DRL算法的损失函数进行分析与解读。">https://github.com/hill-a/stable-baselines）中部分DRL算法的损失函数进行分析与解读。</a><br><span id="more"></span></p>
<h3 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h3><p>SB中的DQN提供了普通DQN、DDQN、Dueling DQN、Prioritized Experience Replay四种形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.act, self._train_step, self.update_target, self.step_model = build_train(</span><br><span class="line">    q_func=partial(self.policy, **self.policy_kwargs),</span><br><span class="line">    ob_space=self.observation_space,</span><br><span class="line">    ac_space=self.action_space,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    gamma=self.gamma,</span><br><span class="line">    grad_norm_clipping=<span class="number">10</span>,</span><br><span class="line">    param_noise=self.param_noise,</span><br><span class="line">    sess=self.sess,</span><br><span class="line">    full_tensorboard_log=self.full_tensorboard_log,</span><br><span class="line">    double_q=self.double_q</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>该函数返回一个tuple，四个返回值均为function，self.act用于选择动作，self._train_step用于训练参数，self.update_target</p>
<p>用于target网络和eval网络的参数复制，self.step_model用于测试。</p>
<p>loss部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;loss&quot;</span>, reuse=reuse):</span><br><span class="line">    <span class="comment"># set up placeholders</span></span><br><span class="line">    act_t_ph = tf.placeholder(tf.int32, [<span class="literal">None</span>], name=<span class="string">&quot;action&quot;</span>)</span><br><span class="line">    rew_t_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">&quot;reward&quot;</span>)</span><br><span class="line">    done_mask_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">&quot;done&quot;</span>)</span><br><span class="line">    importance_weights_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">&quot;weight&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># q scores for actions which we know were selected in the given state.</span></span><br><span class="line">    q_t_selected = tf.reduce_sum(step_model.q_values * tf.one_hot(act_t_ph, n_actions), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute estimate of best possible value starting from state at t + 1</span></span><br><span class="line">    <span class="keyword">if</span> double_q:</span><br><span class="line">        q_tp1_best_using_online_net = tf.argmax(double_q_values, axis=<span class="number">1</span>)</span><br><span class="line">        q_tp1_best = tf.reduce_sum(target_policy.q_values * tf.one_hot(q_tp1_best_using_online_net, n_actions), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        q_tp1_best = tf.reduce_max(target_policy.q_values, axis=<span class="number">1</span>)</span><br><span class="line">    q_tp1_best_masked = (<span class="number">1.0</span> - done_mask_ph) * q_tp1_best</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute RHS of bellman equation</span></span><br><span class="line">    q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the error (potentially clipped)</span></span><br><span class="line">    td_error = q_t_selected - tf.stop_gradient(q_t_selected_target)</span><br><span class="line">    errors = tf_util.huber_loss(td_error)</span><br><span class="line">    weighted_error = tf.reduce_mean(importance_weights_ph * errors)</span><br><span class="line"></span><br><span class="line">    tf.summary.scalar(<span class="string">&quot;td_error&quot;</span>, tf.reduce_mean(td_error))</span><br><span class="line">    tf.summary.scalar(<span class="string">&quot;loss&quot;</span>, weighted_error)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> full_tensorboard_log:</span><br><span class="line">        tf.summary.histogram(<span class="string">&quot;td_error&quot;</span>, td_error)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update_target_fn will be called periodically to copy Q network to target Q network</span></span><br><span class="line">    update_target_expr = []</span><br><span class="line">    <span class="keyword">for</span> var, var_target <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">sorted</span>(q_func_vars, key=<span class="keyword">lambda</span> v: v.name),</span><br><span class="line">                               <span class="built_in">sorted</span>(target_q_func_vars, key=<span class="keyword">lambda</span> v: v.name)):</span><br><span class="line">        update_target_expr.append(var_target.assign(var))</span><br><span class="line">    update_target_expr = tf.group(*update_target_expr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute optimization op (potentially with gradient clipping)</span></span><br><span class="line">    gradients = optimizer.compute_gradients(weighted_error, var_list=q_func_vars)</span><br><span class="line">    <span class="keyword">if</span> grad_norm_clipping <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> i, (grad, var) <span class="keyword">in</span> <span class="built_in">enumerate</span>(gradients):</span><br><span class="line">            <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                gradients[i] = (tf.clip_by_norm(grad, grad_norm_clipping), var)</span><br></pre></td></tr></table></figure>
<p>注：</p>
<p>（1）<em>step_model = q_func(sess, ob_space, ac_space, 1, 1, None, reuse=True, obs_phs=obs_phs)</em>是eval Q network，q_t_selected用了reduce_sum是因为DQN是针对离散动作空间，所以value*onehot是个矩阵（类似[100, 0, 0, 0]）。</p>
<p>（2）<em>q_tp1_best_masked = (1.0 - done_mask_ph) \</em> q_tp1_best* 考虑了done，所以加了一层mask。</p>
<p>（3）计算误差包括三部分：首先是<em>q_t_selected - tf.stop_gradient(q_t_selected_target)</em> 的TD error，其次是Huber Loss 的形式，优点是能增强平方误差损失函数(MSE, mean square error)对离群点的鲁棒性：<em>tf_util.huber_loss(td_error)</em>：</p>
<script type="math/tex; mode=display">
L_{\delta}(a)= \begin{cases}\frac{1}{2} a^{2}, & \text { for }|a| \leq \delta \\ \delta \cdot\left(|a|-\frac{1}{2} \delta\right), & \text { otherwise }\end{cases}</script><p>可见对于Q与Q target差异较大的情况下，HuberLoss降低了对其的惩罚程度。</p>
<p>最后是<em>importance_weights_ph</em>，是针对Prioritized Experience Replay而言的。</p>
<p>（4）计算gradient的时候是针对加权后的loss而言的，并且可以设置gradient clipping。</p>
<h3 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_train_step</span>(<span class="params">self, learning_rate, cliprange, obs, returns, masks, actions, values, neglogpacs, update,</span></span><br><span class="line"><span class="params">                writer, states=<span class="literal">None</span>, cliprange_vf=<span class="literal">None</span></span>)</span><br><span class="line">    advs = returns - values</span><br><span class="line">    advs = (advs - advs.mean()) / (advs.std() + <span class="number">1e-8</span>)</span><br><span class="line">    td_map = &#123;self.train_model.obs_ph: obs, self.action_ph: actions,</span><br><span class="line">              self.advs_ph: advs, self.rewards_ph: returns,</span><br><span class="line">              self.learning_rate_ph: learning_rate, self.clip_range_ph: cliprange,</span><br><span class="line">              self.old_neglog_pac_ph: neglogpacs, self.old_vpred_ph: values&#125;</span><br><span class="line">    <span class="keyword">if</span> states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        td_map[self.train_model.states_ph] = states</span><br><span class="line">        td_map[self.train_model.dones_ph] = masks</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cliprange_vf <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> cliprange_vf &gt;= <span class="number">0</span>:</span><br><span class="line">        td_map[self.clip_range_vf_ph] = cliprange_vf</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        update_fac = <span class="built_in">max</span>(self.n_batch // self.nminibatches // self.noptepochs, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        update_fac = <span class="built_in">max</span>(self.n_batch // self.nminibatches // self.noptepochs // self.n_steps, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(</span><br><span class="line">            [self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train], td_map)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> policy_loss, value_loss, policy_entropy, approxkl, clipfrac</span><br></pre></td></tr></table></figure>
<p>返回的是策略梯度loss、价值函数loss，policy entropy，KL散度近似值，updated clipping range和training update operation。</p>
<p>loss部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;loss&quot;</span>, reuse=<span class="literal">False</span>):</span><br><span class="line">    self.action_ph = train_model.pdtype.sample_placeholder([<span class="literal">None</span>], name=<span class="string">&quot;action_ph&quot;</span>)</span><br><span class="line">    self.advs_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">&quot;advs_ph&quot;</span>)</span><br><span class="line">    self.rewards_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">&quot;rewards_ph&quot;</span>)</span><br><span class="line">    self.old_neglog_pac_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">&quot;old_neglog_pac_ph&quot;</span>)</span><br><span class="line">    self.old_vpred_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">&quot;old_vpred_ph&quot;</span>)</span><br><span class="line">    self.learning_rate_ph = tf.placeholder(tf.float32, [], name=<span class="string">&quot;learning_rate_ph&quot;</span>)</span><br><span class="line">    self.clip_range_ph = tf.placeholder(tf.float32, [], name=<span class="string">&quot;clip_range_ph&quot;</span>)</span><br><span class="line"></span><br><span class="line">    neglogpac = train_model.proba_distribution.neglogp(self.action_ph)</span><br><span class="line">    self.entropy = tf.reduce_mean(train_model.proba_distribution.entropy())</span><br><span class="line"></span><br><span class="line">    vpred = train_model.value_flat</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Value function clipping: not present in the original PPO</span></span><br><span class="line">    <span class="keyword">if</span> self.cliprange_vf <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Default behavior (legacy from OpenAI baselines):</span></span><br><span class="line">        <span class="comment"># use the same clipping as for the policy</span></span><br><span class="line">        self.clip_range_vf_ph = self.clip_range_ph</span><br><span class="line">        self.cliprange_vf = self.cliprange</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(self.cliprange_vf, (<span class="built_in">float</span>, <span class="built_in">int</span>)) <span class="keyword">and</span> self.cliprange_vf &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Original PPO implementation: no value function clipping</span></span><br><span class="line">        self.clip_range_vf_ph = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Last possible behavior: clipping range</span></span><br><span class="line">        <span class="comment"># specific to the value function</span></span><br><span class="line">        self.clip_range_vf_ph = tf.placeholder(tf.float32, [], name=<span class="string">&quot;clip_range_vf_ph&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.clip_range_vf_ph <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># No clipping</span></span><br><span class="line">        vpred_clipped = train_model.value_flat</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Clip the different between old and new value</span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> this depends on the reward scaling</span></span><br><span class="line">        vpred_clipped = self.old_vpred_ph + \</span><br><span class="line">            tf.clip_by_value(train_model.value_flat - self.old_vpred_ph,</span><br><span class="line">                             - self.clip_range_vf_ph, self.clip_range_vf_ph)</span><br><span class="line"></span><br><span class="line">    vf_losses1 = tf.square(vpred - self.rewards_ph)</span><br><span class="line">    vf_losses2 = tf.square(vpred_clipped - self.rewards_ph)</span><br><span class="line">    self.vf_loss = <span class="number">.5</span> * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))</span><br><span class="line"></span><br><span class="line">    ratio = tf.exp(self.old_neglog_pac_ph - neglogpac)</span><br><span class="line">    pg_losses = -self.advs_ph * ratio</span><br><span class="line">    pg_losses2 = -self.advs_ph * tf.clip_by_value(ratio, <span class="number">1.0</span> - self.clip_range_ph, <span class="number">1.0</span> +</span><br><span class="line">                                                  self.clip_range_ph)</span><br><span class="line">    self.pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))</span><br><span class="line">    self.approxkl = <span class="number">.5</span> * tf.reduce_mean(tf.square(neglogpac - self.old_neglog_pac_ph))</span><br><span class="line">    self.clipfrac = tf.reduce_mean(tf.cast(tf.greater(tf.<span class="built_in">abs</span>(ratio - <span class="number">1.0</span>),</span><br><span class="line">                                                      self.clip_range_ph), tf.float32))</span><br><span class="line">    loss = self.pg_loss - self.entropy * self.ent_coef + self.vf_loss * self.vf_coef</span><br><span class="line"></span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;entropy_loss&#x27;</span>, self.entropy)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;policy_gradient_loss&#x27;</span>, self.pg_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;value_function_loss&#x27;</span>, self.vf_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;approximate_kullback-leibler&#x27;</span>, self.approxkl)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;clip_factor&#x27;</span>, self.clipfrac)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;loss&#x27;</span>, loss)</span><br></pre></td></tr></table></figure>
<p>其中涉及到distribution的部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">proba_distribution_from_latent</span>(<span class="params">self, pi_latent_vector, vf_latent_vector, init_scale=<span class="number">1.0</span>, init_bias=<span class="number">0.0</span></span>):</span><br><span class="line">        mean = linear(pi_latent_vector, <span class="string">&#x27;pi&#x27;</span>, self.size, init_scale=init_scale, init_bias=init_bias)</span><br><span class="line">        logstd = tf.get_variable(name=<span class="string">&#x27;pi/logstd&#x27;</span>, shape=[<span class="number">1</span>, self.size], initializer=tf.zeros_initializer())</span><br><span class="line">        pdparam = tf.concat([mean, mean * <span class="number">0.0</span> + logstd], axis=<span class="number">1</span>)</span><br><span class="line">        q_values = linear(vf_latent_vector, <span class="string">&#x27;q&#x27;</span>, self.size, init_scale=init_scale, init_bias=init_bias)</span><br><span class="line">        <span class="keyword">return</span> self.proba_distribution_from_flat(pdparam), mean, q_values</span><br></pre></td></tr></table></figure>
<p>注：</p>
<p>（1）_train_step中计算advantage的时候做了归一化：<em>advs = (advs - advs.mean()) / (advs.std() + 1e-8)</em>。（因为这里的adv是包含了每个step下的adv，所以是个向量）</p>
<p>（2）计算c loss是用value network输出值 - discounted rewards，计算advantage是用discounted rewards - value network输出值，且PPO中没有target network，利用 GAE计算出来的discounted rewards就是target。</p>
<p>（3）在计算neglogpac和entropy的时候所用的proba_distribution里没有对mean和sigma进行归一化（说明需要在设计网络结构时加上tanh或者在这里将激活函数修改为tanh等），也没有限制logstd的大小。</p>
<p>（4）value func loss为<em>self.vf_loss = .5 \</em> tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))*，vf_losses2是对value进行clip后再减去target value，vf_losses1则为没有clip操作的loss。</p>
<p>（5）计算aloss的时候默认用的是clip，KL散度计算用的是<em>.5 \</em> tf.reduce_mean(tf.square(neglogpac - self.old_neglog_pac_ph))*</p>
<p>（6）计算梯度的时候用的是<em>loss = self.pg_loss - self.entropy \</em> self.ent_coef + self.vf_loss * self.vf_coef*，既包含了a loss，又包含了c loss，还包括了entropy，因为openAI默认的policy网络结中将actor和critic写在一起（用一个model表示），计算梯度各自只会去找和各自相关的变量（其他变量梯度也是0），所以不会冲突，回传的时候再依次传给对应网络的参数。</p>
<p>以mlp形式的FeedForwardPolicy为例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardPolicy</span>(<span class="title class_ inherited__">ActorCriticPolicy</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Policy object that implements actor critic, using a feed forward neural network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sess: (TensorFlow session) The current TensorFlow session</span></span><br><span class="line"><span class="string">    :param ob_space: (Gym Space) The observation space of the environment</span></span><br><span class="line"><span class="string">    :param ac_space: (Gym Space) The action space of the environment</span></span><br><span class="line"><span class="string">    :param n_env: (int) The number of environments to run</span></span><br><span class="line"><span class="string">    :param n_steps: (int) The number of steps to run for each environment</span></span><br><span class="line"><span class="string">    :param n_batch: (int) The number of batch to run (n_envs * n_steps)</span></span><br><span class="line"><span class="string">    :param reuse: (bool) If the policy is reusable or not</span></span><br><span class="line"><span class="string">    :param layers: ([int]) (deprecated, use net_arch instead) The size of the Neural network for the policy</span></span><br><span class="line"><span class="string">        (if None, default to [64, 64])</span></span><br><span class="line"><span class="string">    :param net_arch: (list) Specification of the actor-critic policy network architecture (see mlp_extractor</span></span><br><span class="line"><span class="string">        documentation for details).</span></span><br><span class="line"><span class="string">    :param act_fun: (tf.func) the activation function to use in the neural network.</span></span><br><span class="line"><span class="string">    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction</span></span><br><span class="line"><span class="string">    :param feature_extraction: (str) The feature extraction type (&quot;cnn&quot; or &quot;mlp&quot;)</span></span><br><span class="line"><span class="string">    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=<span class="literal">False</span>, layers=<span class="literal">None</span>, net_arch=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 act_fun=tf.tanh, cnn_extractor=nature_cnn, feature_extraction=<span class="string">&quot;cnn&quot;</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeedForwardPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse,</span><br><span class="line">                                                scale=(feature_extraction == <span class="string">&quot;cnn&quot;</span>))</span><br><span class="line"></span><br><span class="line">        self._kwargs_check(feature_extraction, kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> layers <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            warnings.warn(<span class="string">&quot;Usage of the `layers` parameter is deprecated! Use net_arch instead &quot;</span></span><br><span class="line">                          <span class="string">&quot;(it has a different semantics though).&quot;</span>, DeprecationWarning)</span><br><span class="line">            <span class="keyword">if</span> net_arch <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                warnings.warn(<span class="string">&quot;The new `net_arch` parameter overrides the deprecated `layers` parameter!&quot;</span>,</span><br><span class="line">                              DeprecationWarning)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> net_arch <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> layers <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                layers = [<span class="number">64</span>, <span class="number">64</span>]</span><br><span class="line">            net_arch = [<span class="built_in">dict</span>(vf=layers, pi=layers)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;model&quot;</span>, reuse=reuse):</span><br><span class="line">            <span class="keyword">if</span> feature_extraction == <span class="string">&quot;cnn&quot;</span>:</span><br><span class="line">                pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pi_latent, vf_latent = mlp_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)</span><br><span class="line"></span><br><span class="line">            self._value_fn = linear(vf_latent, <span class="string">&#x27;vf&#x27;</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            self._proba_distribution, self._policy, self.q_value = \</span><br><span class="line">                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self._setup_init()</span><br></pre></td></tr></table></figure>
<p>默认都是[64,64]的网络层，默认的激活函数是tanh。</p>
<p>PS1：PPO returns计算方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(mb_rewards))):</span><br><span class="line">    <span class="keyword">if</span> step == <span class="built_in">len</span>(mb_rewards) - <span class="number">1</span>:</span><br><span class="line">        nextnonterminal = <span class="number">1.0</span> - self.dones</span><br><span class="line">        nextvalues = last_values</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nextnonterminal = <span class="number">1.0</span> - mb_dones[step + <span class="number">1</span>]</span><br><span class="line">        nextvalues = mb_values[step + <span class="number">1</span>]</span><br><span class="line">    delta = mb_rewards[step] + self.gamma * nextvalues * nextnonterminal - mb_values[step]</span><br><span class="line">    mb_advs[step] = last_gae_lam = delta + self.gamma * self.lam * nextnonterminal * last_gae_lam</span><br><span class="line">mb_returns = mb_advs + mb_values</span><br></pre></td></tr></table></figure>
<p>注：</p>
<p>（1）因为没有target网络，所以计算Q target的时候也是用的当前网络的value：mb_values</p>
<p>（2）mb_values包含了当前网络参数下每个step的value func输出值。用了GAE计算出Advantage（r+gamma*next_V-V），再加上V即得到了TD（lambda）形式的value。</p>
<p><em>PS：关于上下文管理器中的reuse影响可见博客（<a href="https://blog.csdn.net/qq_29831163/article/details/90202649）">https://blog.csdn.net/qq_29831163/article/details/90202649）</a></em></p>
<h3 id="SAC"><a href="#SAC" class="headerlink" title="SAC"></a>SAC</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;loss&quot;</span>, reuse=<span class="literal">False</span>):</span><br><span class="line">    <span class="comment"># Take the min of the two Q-Values (Double-Q Learning)</span></span><br><span class="line">    min_qf_pi = tf.minimum(qf1_pi, qf2_pi)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Target for Q value regression</span></span><br><span class="line">    q_backup = tf.stop_gradient(</span><br><span class="line">        self.rewards_ph +</span><br><span class="line">        (<span class="number">1</span> - self.terminals_ph) * self.gamma * self.value_target</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Q-Function loss</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> test with huber loss (it would avoid too high values)</span></span><br><span class="line">    qf1_loss = <span class="number">0.5</span> * tf.reduce_mean((q_backup - qf1) ** <span class="number">2</span>)</span><br><span class="line">    qf2_loss = <span class="number">0.5</span> * tf.reduce_mean((q_backup - qf2) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the entropy temperature loss</span></span><br><span class="line">    <span class="comment"># it is used when the entropy coefficient is learned</span></span><br><span class="line">    ent_coef_loss, entropy_optimizer = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.ent_coef, <span class="built_in">float</span>):</span><br><span class="line">        ent_coef_loss = -tf.reduce_mean(</span><br><span class="line">            self.log_ent_coef * tf.stop_gradient(logp_pi + self.target_entropy))</span><br><span class="line">        entropy_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the policy loss</span></span><br><span class="line">    <span class="comment"># Alternative: policy_kl_loss = tf.reduce_mean(logp_pi - min_qf_pi)</span></span><br><span class="line">    policy_kl_loss = tf.reduce_mean(self.ent_coef * logp_pi - qf1_pi)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> in the original implementation, they have an additional</span></span><br><span class="line">    <span class="comment"># regularization loss for the Gaussian parameters</span></span><br><span class="line">    <span class="comment"># this is not used for now</span></span><br><span class="line">    <span class="comment"># policy_loss = (policy_kl_loss + policy_regularization_loss)</span></span><br><span class="line">    policy_loss = policy_kl_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Target for value fn regression</span></span><br><span class="line">    <span class="comment"># We update the vf towards the min of two Q-functions in order to</span></span><br><span class="line">    <span class="comment"># reduce overestimation bias from function approximation error.</span></span><br><span class="line">    v_backup = tf.stop_gradient(min_qf_pi - self.ent_coef * logp_pi)</span><br><span class="line">    value_loss = <span class="number">0.5</span> * tf.reduce_mean((value_fn - v_backup) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    values_losses = qf1_loss + qf2_loss + value_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Policy train op</span></span><br><span class="line">    <span class="comment"># (has to be separate from value train op, because min_qf_pi appears in policy_loss)</span></span><br><span class="line">    policy_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)</span><br><span class="line">    policy_train_op = policy_optimizer.minimize(policy_loss, var_list=tf_util.get_trainable_vars(<span class="string">&#x27;model/pi&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Value train op</span></span><br><span class="line">    value_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)</span><br><span class="line">    values_params = tf_util.get_trainable_vars(<span class="string">&#x27;model/values_fn&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    source_params = tf_util.get_trainable_vars(<span class="string">&quot;model/values_fn&quot;</span>)</span><br><span class="line">    target_params = tf_util.get_trainable_vars(<span class="string">&quot;target/values_fn&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Polyak averaging for target variables</span></span><br><span class="line">    self.target_update_op = [</span><br><span class="line">        tf.assign(target, (<span class="number">1</span> - self.tau) * target + self.tau * source)</span><br><span class="line">        <span class="keyword">for</span> target, source <span class="keyword">in</span> <span class="built_in">zip</span>(target_params, source_params)</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># Initializing target to match source variables</span></span><br><span class="line">    target_init_op = [</span><br><span class="line">        tf.assign(target, source)</span><br><span class="line">        <span class="keyword">for</span> target, source <span class="keyword">in</span> <span class="built_in">zip</span>(target_params, source_params)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Control flow is used because sess.run otherwise evaluates in nondeterministic order</span></span><br><span class="line">    <span class="comment"># and we first need to compute the policy action before computing q values losses</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([policy_train_op]):</span><br><span class="line">        train_values_op = value_optimizer.minimize(values_losses, var_list=values_params)</span><br><span class="line"></span><br><span class="line">        self.infos_names = [<span class="string">&#x27;policy_loss&#x27;</span>, <span class="string">&#x27;qf1_loss&#x27;</span>, <span class="string">&#x27;qf2_loss&#x27;</span>, <span class="string">&#x27;value_loss&#x27;</span>, <span class="string">&#x27;entropy&#x27;</span>]</span><br><span class="line">        <span class="comment"># All ops to call during one training step</span></span><br><span class="line">        self.step_ops = [policy_loss, qf1_loss, qf2_loss,</span><br><span class="line">                         value_loss, qf1, qf2, value_fn, logp_pi,</span><br><span class="line">                         self.entropy, policy_train_op, train_values_op]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add entropy coefficient optimization operation if needed</span></span><br><span class="line">        <span class="keyword">if</span> ent_coef_loss <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> tf.control_dependencies([train_values_op]):</span><br><span class="line">                ent_coef_op = entropy_optimizer.minimize(ent_coef_loss, var_list=self.log_ent_coef)</span><br><span class="line">                self.infos_names += [<span class="string">&#x27;ent_coef_loss&#x27;</span>, <span class="string">&#x27;ent_coef&#x27;</span>]</span><br><span class="line">                self.step_ops += [ent_coef_op, ent_coef_loss, self.ent_coef]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Monitor losses and entropy in tensorboard</span></span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;policy_loss&#x27;</span>, policy_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;qf1_loss&#x27;</span>, qf1_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;qf2_loss&#x27;</span>, qf2_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;value_loss&#x27;</span>, value_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;entropy&#x27;</span>, self.entropy)</span><br><span class="line">    <span class="keyword">if</span> ent_coef_loss <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;ent_coef_loss&#x27;</span>, ent_coef_loss)</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;ent_coef&#x27;</span>, self.ent_coef)</span><br><span class="line"></span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;learning_rate&#x27;</span>, tf.reduce_mean(self.learning_rate_ph))</span><br></pre></td></tr></table></figure>
<p>注：</p>
<p>（1）openAI的SAC版本的critic网络有1个Value network（需要target）和两个Q-value network（不需要target）。</p>
<p>（2）value loss是想要value_fn与当前最小q value的min_qf_pi对应的soft value——v_backup（即在min_qf_pi基础上加上当前时刻动作分布的entropy）接近：<em>value_loss = 0.5 \</em> tf.reduce_mean((value_fn - v_backup) <em>* 2)</em></p>
<p>（3）Q value loss是想qf接近Q target（通过r+gamma*V_target求出Q_target）：</p>
<p><em>qf1_loss = 0.5 \</em> tf.reduce_mean((q_backup - qf1) <em>* 2)</em></p>
<p>（3）这里默认的计算policy loss的方式是最大化Q network1对应的Q值同时最大化动作熵，<em>policy_kl_loss = tf.reduce_mean(self.ent_coef \</em> logp_pi - qf1_pi)* 。当然也可以选为最大化min_qf_pi对应的Q值同时最大化动作熵。</p>
<p>（4）计算value loss的梯度时采用的是所有和value相关的loss：<em>values_losses = qf1_loss + qf2_loss + value_loss</em></p>
<p>（5）soft更新target参数的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.target_update_op = [</span><br><span class="line">    tf.assign(target, (<span class="number">1</span> - self.tau) * target + self.tau * source)</span><br><span class="line">    <span class="keyword">for</span> target, source <span class="keyword">in</span> <span class="built_in">zip</span>(target_params, source_params)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>代码解读</category>
      </categories>
      <tags>
        <tag>Code</tag>
      </tags>
  </entry>
</search>
