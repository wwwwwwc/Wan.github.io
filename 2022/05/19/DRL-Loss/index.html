<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"waunx.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="内容概述 本文主要针对open-AI开源的SB 3（https:&#x2F;&#x2F;github.com&#x2F;hill-a&#x2F;stable-baselines）中部分DRL算法的损失函数进行分析与解读。">
<meta property="og:type" content="article">
<meta property="og:title" content="SB中常见DRL算法的损失函数设置">
<meta property="og:url" content="https://waunx.github.io/2022/05/19/DRL-Loss/index.html">
<meta property="og:site_name" content="Xu&#39;s notes">
<meta property="og:description" content="内容概述 本文主要针对open-AI开源的SB 3（https:&#x2F;&#x2F;github.com&#x2F;hill-a&#x2F;stable-baselines）中部分DRL算法的损失函数进行分析与解读。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-05-19T08:36:30.000Z">
<meta property="article:modified_time" content="2022-05-19T00:53:49.890Z">
<meta property="article:author" content="Xu Wan">
<meta property="article:tag" content="Code">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://waunx.github.io/2022/05/19/DRL-Loss/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>SB中常见DRL算法的损失函数设置 | Xu's notes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xu's notes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">AI & Security</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/waunx" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://waunx.github.io/2022/05/19/DRL-Loss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo.jpg">
      <meta itemprop="name" content="Xu Wan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xu's notes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SB中常见DRL算法的损失函数设置
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-05-19 08:36:30 / 修改时间：00:53:49" itemprop="dateCreated datePublished" datetime="2022-05-19T08:36:30Z">2022-05-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">代码解读</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>16k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="内容概述">内容概述</h3>
<p>本文主要针对open-AI开源的SB 3（https://github.com/hill-a/stable-baselines）中部分DRL算法的损失函数进行分析与解读。 <span id="more"></span></p>
<h3 id="dqn">DQN</h3>
<p>SB中的DQN提供了普通DQN、DDQN、Dueling DQN、Prioritized Experience Replay四种形式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">self.act, self._train_step, self.update_target, self.step_model = build_train(</span><br><span class="line">    q_func=partial(self.policy, **self.policy_kwargs),</span><br><span class="line">    ob_space=self.observation_space,</span><br><span class="line">    ac_space=self.action_space,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    gamma=self.gamma,</span><br><span class="line">    grad_norm_clipping=<span class="number">10</span>,</span><br><span class="line">    param_noise=self.param_noise,</span><br><span class="line">    sess=self.sess,</span><br><span class="line">    full_tensorboard_log=self.full_tensorboard_log,</span><br><span class="line">    double_q=self.double_q</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>该函数返回一个tuple，四个返回值均为function，self.act用于选择动作，self._train_step用于训练参数，self.update_target</p>
<p>用于target网络和eval网络的参数复制，self.step_model用于测试。</p>
<p>loss部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"loss"</span>, reuse=reuse):</span><br><span class="line">    <span class="comment"># set up placeholders</span></span><br><span class="line">    act_t_ph = tf.placeholder(tf.int32, [<span class="literal">None</span>], name=<span class="string">"action"</span>)</span><br><span class="line">    rew_t_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">"reward"</span>)</span><br><span class="line">    done_mask_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">"done"</span>)</span><br><span class="line">    importance_weights_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">"weight"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># q scores for actions which we know were selected in the given state.</span></span><br><span class="line">    q_t_selected = tf.reduce_sum(step_model.q_values * tf.one_hot(act_t_ph, n_actions), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute estimate of best possible value starting from state at t + 1</span></span><br><span class="line">    <span class="keyword">if</span> double_q:</span><br><span class="line">        q_tp1_best_using_online_net = tf.argmax(double_q_values, axis=<span class="number">1</span>)</span><br><span class="line">        q_tp1_best = tf.reduce_sum(target_policy.q_values * tf.one_hot(q_tp1_best_using_online_net, n_actions), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        q_tp1_best = tf.reduce_max(target_policy.q_values, axis=<span class="number">1</span>)</span><br><span class="line">    q_tp1_best_masked = (<span class="number">1.0</span> - done_mask_ph) * q_tp1_best</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute RHS of bellman equation</span></span><br><span class="line">    q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the error (potentially clipped)</span></span><br><span class="line">    td_error = q_t_selected - tf.stop_gradient(q_t_selected_target)</span><br><span class="line">    errors = tf_util.huber_loss(td_error)</span><br><span class="line">    weighted_error = tf.reduce_mean(importance_weights_ph * errors)</span><br><span class="line"></span><br><span class="line">    tf.summary.scalar(<span class="string">"td_error"</span>, tf.reduce_mean(td_error))</span><br><span class="line">    tf.summary.scalar(<span class="string">"loss"</span>, weighted_error)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> full_tensorboard_log:</span><br><span class="line">        tf.summary.histogram(<span class="string">"td_error"</span>, td_error)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update_target_fn will be called periodically to copy Q network to target Q network</span></span><br><span class="line">    update_target_expr = []</span><br><span class="line">    <span class="keyword">for</span> var, var_target <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">sorted</span>(q_func_vars, key=<span class="keyword">lambda</span> v: v.name),</span><br><span class="line">                               <span class="built_in">sorted</span>(target_q_func_vars, key=<span class="keyword">lambda</span> v: v.name)):</span><br><span class="line">        update_target_expr.append(var_target.assign(var))</span><br><span class="line">    update_target_expr = tf.group(*update_target_expr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute optimization op (potentially with gradient clipping)</span></span><br><span class="line">    gradients = optimizer.compute_gradients(weighted_error, var_list=q_func_vars)</span><br><span class="line">    <span class="keyword">if</span> grad_norm_clipping <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> i, (grad, var) <span class="keyword">in</span> <span class="built_in">enumerate</span>(gradients):</span><br><span class="line">            <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                gradients[i] = (tf.clip_by_norm(grad, grad_norm_clipping), var)</span><br></pre></td></tr></table></figure>
<p>注：</p>
<p>（1）<em>step_model = q_func(sess, ob_space, ac_space, 1, 1, None, reuse=True, obs_phs=obs_phs)</em>是eval Q network，q_t_selected用了reduce_sum是因为DQN是针对离散动作空间，所以value*onehot是个矩阵（类似[100, 0, 0, 0]）。</p>
<p>（2）<em>q_tp1_best_masked = (1.0 - done_mask_ph) * q_tp1_best</em> 考虑了done，所以加了一层mask。</p>
<p>（3）计算误差包括三部分：首先是<em>q_t_selected - tf.stop_gradient(q_t_selected_target)</em> 的TD error，其次是Huber Loss 的形式，优点是能增强平方误差损失函数(MSE, mean square error)对离群点的鲁棒性：<em>tf_util.huber_loss(td_error)</em>： <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.403ex" xmlns="http://www.w3.org/2000/svg" width="36.183ex" height="5.937ex" role="img" focusable="false" viewbox="0 -1562.2 15993 2624.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"/></g><g data-mml-node="TeXAtom" transform="translate(714,-153.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"/></g></g></g><g data-mml-node="mo" transform="translate(1078,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1467,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(1996,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(2662.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mrow" transform="translate(3718.5,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7B" d="M618 -943L612 -949H582L568 -943Q472 -903 411 -841T332 -703Q327 -682 327 -653T325 -350Q324 -28 323 -18Q317 24 301 61T264 124T221 171T179 205T147 225T132 234Q130 238 130 250Q130 255 130 258T131 264T132 267T134 269T139 272T144 275Q207 308 256 367Q310 436 323 519Q324 529 325 851Q326 1124 326 1154T332 1205Q369 1358 566 1443L582 1450H612L618 1444V1429Q618 1413 616 1411L608 1406Q599 1402 585 1393T552 1372T515 1343T479 1305T449 1257T429 1200Q425 1180 425 1152T423 851Q422 579 422 549T416 498Q407 459 388 424T346 364T297 318T250 284T214 264T197 254L188 251L205 242Q290 200 345 138T416 3Q421 -18 421 -48T423 -349Q423 -397 423 -472Q424 -677 428 -694Q429 -697 429 -699Q434 -722 443 -743T465 -782T491 -816T519 -845T548 -868T574 -886T595 -899T610 -908L616 -910Q618 -912 618 -928V-943Z"/></g><g data-mml-node="mtable" transform="translate(750,0)"><g data-mml-node="mtr" transform="translate(0,697.2)"><g data-mml-node="mtd"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(220,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g><rect width="553.6" height="60" x="120" y="220"/></g><g data-mml-node="msup" transform="translate(793.6,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="TeXAtom" transform="translate(562,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g><g data-mml-node="mo" transform="translate(1759.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g></g><g data-mml-node="mtd" transform="translate(6905.4,0)"><g data-mml-node="mtext"><path data-c="A0" d=""/><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(250,0)"/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(556,0)"/><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(1056,0)"/><path data-c="A0" d="" transform="translate(1448,0)"/></g><g data-mml-node="mo" transform="translate(1698,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(1976,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(2505,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mo" transform="translate(3060.8,0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"/></g><g data-mml-node="mi" transform="translate(4116.6,0)"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"/></g></g></g><g data-mml-node="mtr" transform="translate(0,-712.7)"><g data-mml-node="mtd"><g data-mml-node="mi"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"/></g><g data-mml-node="mo" transform="translate(666.2,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/></g><g data-mml-node="mrow" transform="translate(1166.4,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="28" d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z"/></g><g data-mml-node="mo" transform="translate(458,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(736,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(1265,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mo" transform="translate(1765.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mfrac" transform="translate(2765.4,0)"><g data-mml-node="mn" transform="translate(220,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g><rect width="553.6" height="60" x="120" y="220"/></g><g data-mml-node="mi" transform="translate(3559,0)"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"/></g><g data-mml-node="mo" transform="translate(4003,0) translate(0 -0.5)"><path data-c="29" d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z"/></g></g><g data-mml-node="mo" transform="translate(5627.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g></g><g data-mml-node="mtd" transform="translate(6905.4,0)"><g data-mml-node="mtext"><path data-c="A0" d=""/><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(250,0)"/><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(750,0)"/><path data-c="68" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1139,0)"/><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1695,0)"/><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(2139,0)"/><path data-c="77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z" transform="translate(2531,0)"/><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(3253,0)"/><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(3531,0)"/><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(3925,0)"/><path data-c="A0" d="" transform="translate(4369,0)"/></g></g></g></g><g data-mml-node="mo" transform="translate(12274.4,0) translate(0 250)"/></g></g></g></svg></mjx-container></span> 可见对于Q与Q target差异较大的情况下，HuberLoss降低了对其的惩罚程度。</p>
<p>最后是<em>importance_weights_ph</em>，是针对Prioritized Experience Replay而言的。</p>
<p>（4）计算gradient的时候是针对加权后的loss而言的，并且可以设置gradient clipping。</p>
<h3 id="ppo">PPO</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_train_step</span>(<span class="params">self, learning_rate, cliprange, obs, returns, masks, actions, values, neglogpacs, update,</span></span><br><span class="line"><span class="params">                writer, states=<span class="literal">None</span>, cliprange_vf=<span class="literal">None</span></span>)</span><br><span class="line">    advs = returns - values</span><br><span class="line">    advs = (advs - advs.mean()) / (advs.std() + <span class="number">1e-8</span>)</span><br><span class="line">    td_map = {self.train_model.obs_ph: obs, self.action_ph: actions,</span><br><span class="line">              self.advs_ph: advs, self.rewards_ph: returns,</span><br><span class="line">              self.learning_rate_ph: learning_rate, self.clip_range_ph: cliprange,</span><br><span class="line">              self.old_neglog_pac_ph: neglogpacs, self.old_vpred_ph: values}</span><br><span class="line">    <span class="keyword">if</span> states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        td_map[self.train_model.states_ph] = states</span><br><span class="line">        td_map[self.train_model.dones_ph] = masks</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cliprange_vf <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> cliprange_vf &gt;= <span class="number">0</span>:</span><br><span class="line">        td_map[self.clip_range_vf_ph] = cliprange_vf</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        update_fac = <span class="built_in">max</span>(self.n_batch // self.nminibatches // self.noptepochs, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        update_fac = <span class="built_in">max</span>(self.n_batch // self.nminibatches // self.noptepochs // self.n_steps, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(</span><br><span class="line">            [self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train], td_map)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> policy_loss, value_loss, policy_entropy, approxkl, clipfrac</span><br></pre></td></tr></table></figure>
<p>返回的是策略梯度loss、价值函数loss，policy entropy，KL散度近似值，updated clipping range和training update operation。</p>
<p>loss部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"loss"</span>, reuse=<span class="literal">False</span>):</span><br><span class="line">    self.action_ph = train_model.pdtype.sample_placeholder([<span class="literal">None</span>], name=<span class="string">"action_ph"</span>)</span><br><span class="line">    self.advs_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">"advs_ph"</span>)</span><br><span class="line">    self.rewards_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">"rewards_ph"</span>)</span><br><span class="line">    self.old_neglog_pac_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">"old_neglog_pac_ph"</span>)</span><br><span class="line">    self.old_vpred_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>], name=<span class="string">"old_vpred_ph"</span>)</span><br><span class="line">    self.learning_rate_ph = tf.placeholder(tf.float32, [], name=<span class="string">"learning_rate_ph"</span>)</span><br><span class="line">    self.clip_range_ph = tf.placeholder(tf.float32, [], name=<span class="string">"clip_range_ph"</span>)</span><br><span class="line"></span><br><span class="line">    neglogpac = train_model.proba_distribution.neglogp(self.action_ph)</span><br><span class="line">    self.entropy = tf.reduce_mean(train_model.proba_distribution.entropy())</span><br><span class="line"></span><br><span class="line">    vpred = train_model.value_flat</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Value function clipping: not present in the original PPO</span></span><br><span class="line">    <span class="keyword">if</span> self.cliprange_vf <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Default behavior (legacy from OpenAI baselines):</span></span><br><span class="line">        <span class="comment"># use the same clipping as for the policy</span></span><br><span class="line">        self.clip_range_vf_ph = self.clip_range_ph</span><br><span class="line">        self.cliprange_vf = self.cliprange</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(self.cliprange_vf, (<span class="built_in">float</span>, <span class="built_in">int</span>)) <span class="keyword">and</span> self.cliprange_vf &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Original PPO implementation: no value function clipping</span></span><br><span class="line">        self.clip_range_vf_ph = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Last possible behavior: clipping range</span></span><br><span class="line">        <span class="comment"># specific to the value function</span></span><br><span class="line">        self.clip_range_vf_ph = tf.placeholder(tf.float32, [], name=<span class="string">"clip_range_vf_ph"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.clip_range_vf_ph <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># No clipping</span></span><br><span class="line">        vpred_clipped = train_model.value_flat</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Clip the different between old and new value</span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> this depends on the reward scaling</span></span><br><span class="line">        vpred_clipped = self.old_vpred_ph + \</span><br><span class="line">            tf.clip_by_value(train_model.value_flat - self.old_vpred_ph,</span><br><span class="line">                             - self.clip_range_vf_ph, self.clip_range_vf_ph)</span><br><span class="line"></span><br><span class="line">    vf_losses1 = tf.square(vpred - self.rewards_ph)</span><br><span class="line">    vf_losses2 = tf.square(vpred_clipped - self.rewards_ph)</span><br><span class="line">    self.vf_loss = <span class="number">.5</span> * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))</span><br><span class="line"></span><br><span class="line">    ratio = tf.exp(self.old_neglog_pac_ph - neglogpac)</span><br><span class="line">    pg_losses = -self.advs_ph * ratio</span><br><span class="line">    pg_losses2 = -self.advs_ph * tf.clip_by_value(ratio, <span class="number">1.0</span> - self.clip_range_ph, <span class="number">1.0</span> +</span><br><span class="line">                                                  self.clip_range_ph)</span><br><span class="line">    self.pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))</span><br><span class="line">    self.approxkl = <span class="number">.5</span> * tf.reduce_mean(tf.square(neglogpac - self.old_neglog_pac_ph))</span><br><span class="line">    self.clipfrac = tf.reduce_mean(tf.cast(tf.greater(tf.<span class="built_in">abs</span>(ratio - <span class="number">1.0</span>),</span><br><span class="line">                                                      self.clip_range_ph), tf.float32))</span><br><span class="line">    loss = self.pg_loss - self.entropy * self.ent_coef + self.vf_loss * self.vf_coef</span><br><span class="line"></span><br><span class="line">    tf.summary.scalar(<span class="string">'entropy_loss'</span>, self.entropy)</span><br><span class="line">    tf.summary.scalar(<span class="string">'policy_gradient_loss'</span>, self.pg_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">'value_function_loss'</span>, self.vf_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">'approximate_kullback-leibler'</span>, self.approxkl)</span><br><span class="line">    tf.summary.scalar(<span class="string">'clip_factor'</span>, self.clipfrac)</span><br><span class="line">    tf.summary.scalar(<span class="string">'loss'</span>, loss)</span><br></pre></td></tr></table></figure>
<p>其中涉及到distribution的部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">proba_distribution_from_latent</span>(<span class="params">self, pi_latent_vector, vf_latent_vector, init_scale=<span class="number">1.0</span>, init_bias=<span class="number">0.0</span></span>):</span><br><span class="line">        mean = linear(pi_latent_vector, <span class="string">'pi'</span>, self.size, init_scale=init_scale, init_bias=init_bias)</span><br><span class="line">        logstd = tf.get_variable(name=<span class="string">'pi/logstd'</span>, shape=[<span class="number">1</span>, self.size], initializer=tf.zeros_initializer())</span><br><span class="line">        pdparam = tf.concat([mean, mean * <span class="number">0.0</span> + logstd], axis=<span class="number">1</span>)</span><br><span class="line">        q_values = linear(vf_latent_vector, <span class="string">'q'</span>, self.size, init_scale=init_scale, init_bias=init_bias)</span><br><span class="line">        <span class="keyword">return</span> self.proba_distribution_from_flat(pdparam), mean, q_values</span><br></pre></td></tr></table></figure>
<p>注：</p>
<p>（1）_train_step中计算advantage的时候做了归一化：<em>advs = (advs - advs.mean()) / (advs.std() + 1e-8)</em>。（因为这里的adv是包含了每个step下的adv，所以是个向量）</p>
<p>（2）计算c loss是用value network输出值 - discounted rewards，计算advantage是用discounted rewards - value network输出值，且PPO中没有target network，利用 GAE计算出来的discounted rewards就是target。</p>
<p>（3）在计算neglogpac和entropy的时候所用的proba_distribution里没有对mean和sigma进行归一化（说明需要在设计网络结构时加上tanh或者在这里将激活函数修改为tanh等），也没有限制logstd的大小。</p>
<p>（4）value func loss为<em>self.vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))</em>，vf_losses2是对value进行clip后再减去target value，vf_losses1则为没有clip操作的loss。</p>
<p>（5）计算aloss的时候默认用的是clip，KL散度计算用的是<em>.5 * tf.reduce_mean(tf.square(neglogpac - self.old_neglog_pac_ph))</em></p>
<p>（6）计算梯度的时候用的是<em>loss = self.pg_loss - self.entropy * self.ent_coef + self.vf_loss * self.vf_coef</em>，既包含了a loss，又包含了c loss，还包括了entropy，因为openAI默认的policy网络结中将actor和critic写在一起（用一个model表示），计算梯度各自只会去找和各自相关的变量（其他变量梯度也是0），所以不会冲突，回传的时候再依次传给对应网络的参数。</p>
<p>以mlp形式的FeedForwardPolicy为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardPolicy</span>(<span class="title class_ inherited__">ActorCriticPolicy</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Policy object that implements actor critic, using a feed forward neural network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sess: (TensorFlow session) The current TensorFlow session</span></span><br><span class="line"><span class="string">    :param ob_space: (Gym Space) The observation space of the environment</span></span><br><span class="line"><span class="string">    :param ac_space: (Gym Space) The action space of the environment</span></span><br><span class="line"><span class="string">    :param n_env: (int) The number of environments to run</span></span><br><span class="line"><span class="string">    :param n_steps: (int) The number of steps to run for each environment</span></span><br><span class="line"><span class="string">    :param n_batch: (int) The number of batch to run (n_envs * n_steps)</span></span><br><span class="line"><span class="string">    :param reuse: (bool) If the policy is reusable or not</span></span><br><span class="line"><span class="string">    :param layers: ([int]) (deprecated, use net_arch instead) The size of the Neural network for the policy</span></span><br><span class="line"><span class="string">        (if None, default to [64, 64])</span></span><br><span class="line"><span class="string">    :param net_arch: (list) Specification of the actor-critic policy network architecture (see mlp_extractor</span></span><br><span class="line"><span class="string">        documentation for details).</span></span><br><span class="line"><span class="string">    :param act_fun: (tf.func) the activation function to use in the neural network.</span></span><br><span class="line"><span class="string">    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction</span></span><br><span class="line"><span class="string">    :param feature_extraction: (str) The feature extraction type ("cnn" or "mlp")</span></span><br><span class="line"><span class="string">    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=<span class="literal">False</span>, layers=<span class="literal">None</span>, net_arch=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 act_fun=tf.tanh, cnn_extractor=nature_cnn, feature_extraction=<span class="string">"cnn"</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeedForwardPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse,</span><br><span class="line">                                                scale=(feature_extraction == <span class="string">"cnn"</span>))</span><br><span class="line"></span><br><span class="line">        self._kwargs_check(feature_extraction, kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> layers <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            warnings.warn(<span class="string">"Usage of the `layers` parameter is deprecated! Use net_arch instead "</span></span><br><span class="line">                          <span class="string">"(it has a different semantics though)."</span>, DeprecationWarning)</span><br><span class="line">            <span class="keyword">if</span> net_arch <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                warnings.warn(<span class="string">"The new `net_arch` parameter overrides the deprecated `layers` parameter!"</span>,</span><br><span class="line">                              DeprecationWarning)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> net_arch <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> layers <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                layers = [<span class="number">64</span>, <span class="number">64</span>]</span><br><span class="line">            net_arch = [<span class="built_in">dict</span>(vf=layers, pi=layers)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>, reuse=reuse):</span><br><span class="line">            <span class="keyword">if</span> feature_extraction == <span class="string">"cnn"</span>:</span><br><span class="line">                pi_latent = vf_latent = cnn_extractor(self.processed_obs, **kwargs)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pi_latent, vf_latent = mlp_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)</span><br><span class="line"></span><br><span class="line">            self._value_fn = linear(vf_latent, <span class="string">'vf'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            self._proba_distribution, self._policy, self.q_value = \</span><br><span class="line">                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self._setup_init()</span><br></pre></td></tr></table></figure>
<p>默认都是[64,64]的网络层，默认的激活函数是tanh。</p>
<p>PS1：PPO returns计算方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(mb_rewards))):</span><br><span class="line">    <span class="keyword">if</span> step == <span class="built_in">len</span>(mb_rewards) - <span class="number">1</span>:</span><br><span class="line">        nextnonterminal = <span class="number">1.0</span> - self.dones</span><br><span class="line">        nextvalues = last_values</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nextnonterminal = <span class="number">1.0</span> - mb_dones[step + <span class="number">1</span>]</span><br><span class="line">        nextvalues = mb_values[step + <span class="number">1</span>]</span><br><span class="line">    delta = mb_rewards[step] + self.gamma * nextvalues * nextnonterminal - mb_values[step]</span><br><span class="line">    mb_advs[step] = last_gae_lam = delta + self.gamma * self.lam * nextnonterminal * last_gae_lam</span><br><span class="line">mb_returns = mb_advs + mb_values</span><br></pre></td></tr></table></figure>
<p>注：</p>
<p>（1）因为没有target网络，所以计算Q target的时候也是用的当前网络的value：mb_values</p>
<p>（2）mb_values包含了当前网络参数下每个step的value func输出值。用了GAE计算出Advantage（r+gamma*next_V-V），再加上V即得到了TD（lambda）形式的value。</p>
<p><em>PS：关于上下文管理器中的reuse影响可见博客（https://blog.csdn.net/qq_29831163/article/details/90202649）</em></p>
<h3 id="sac">SAC</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"loss"</span>, reuse=<span class="literal">False</span>):</span><br><span class="line">    <span class="comment"># Take the min of the two Q-Values (Double-Q Learning)</span></span><br><span class="line">    min_qf_pi = tf.minimum(qf1_pi, qf2_pi)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Target for Q value regression</span></span><br><span class="line">    q_backup = tf.stop_gradient(</span><br><span class="line">        self.rewards_ph +</span><br><span class="line">        (<span class="number">1</span> - self.terminals_ph) * self.gamma * self.value_target</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Q-Function loss</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> test with huber loss (it would avoid too high values)</span></span><br><span class="line">    qf1_loss = <span class="number">0.5</span> * tf.reduce_mean((q_backup - qf1) ** <span class="number">2</span>)</span><br><span class="line">    qf2_loss = <span class="number">0.5</span> * tf.reduce_mean((q_backup - qf2) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the entropy temperature loss</span></span><br><span class="line">    <span class="comment"># it is used when the entropy coefficient is learned</span></span><br><span class="line">    ent_coef_loss, entropy_optimizer = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.ent_coef, <span class="built_in">float</span>):</span><br><span class="line">        ent_coef_loss = -tf.reduce_mean(</span><br><span class="line">            self.log_ent_coef * tf.stop_gradient(logp_pi + self.target_entropy))</span><br><span class="line">        entropy_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the policy loss</span></span><br><span class="line">    <span class="comment"># Alternative: policy_kl_loss = tf.reduce_mean(logp_pi - min_qf_pi)</span></span><br><span class="line">    policy_kl_loss = tf.reduce_mean(self.ent_coef * logp_pi - qf1_pi)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> in the original implementation, they have an additional</span></span><br><span class="line">    <span class="comment"># regularization loss for the Gaussian parameters</span></span><br><span class="line">    <span class="comment"># this is not used for now</span></span><br><span class="line">    <span class="comment"># policy_loss = (policy_kl_loss + policy_regularization_loss)</span></span><br><span class="line">    policy_loss = policy_kl_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Target for value fn regression</span></span><br><span class="line">    <span class="comment"># We update the vf towards the min of two Q-functions in order to</span></span><br><span class="line">    <span class="comment"># reduce overestimation bias from function approximation error.</span></span><br><span class="line">    v_backup = tf.stop_gradient(min_qf_pi - self.ent_coef * logp_pi)</span><br><span class="line">    value_loss = <span class="number">0.5</span> * tf.reduce_mean((value_fn - v_backup) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    values_losses = qf1_loss + qf2_loss + value_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Policy train op</span></span><br><span class="line">    <span class="comment"># (has to be separate from value train op, because min_qf_pi appears in policy_loss)</span></span><br><span class="line">    policy_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)</span><br><span class="line">    policy_train_op = policy_optimizer.minimize(policy_loss, var_list=tf_util.get_trainable_vars(<span class="string">'model/pi'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Value train op</span></span><br><span class="line">    value_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)</span><br><span class="line">    values_params = tf_util.get_trainable_vars(<span class="string">'model/values_fn'</span>)</span><br><span class="line"></span><br><span class="line">    source_params = tf_util.get_trainable_vars(<span class="string">"model/values_fn"</span>)</span><br><span class="line">    target_params = tf_util.get_trainable_vars(<span class="string">"target/values_fn"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Polyak averaging for target variables</span></span><br><span class="line">    self.target_update_op = [</span><br><span class="line">        tf.assign(target, (<span class="number">1</span> - self.tau) * target + self.tau * source)</span><br><span class="line">        <span class="keyword">for</span> target, source <span class="keyword">in</span> <span class="built_in">zip</span>(target_params, source_params)</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># Initializing target to match source variables</span></span><br><span class="line">    target_init_op = [</span><br><span class="line">        tf.assign(target, source)</span><br><span class="line">        <span class="keyword">for</span> target, source <span class="keyword">in</span> <span class="built_in">zip</span>(target_params, source_params)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Control flow is used because sess.run otherwise evaluates in nondeterministic order</span></span><br><span class="line">    <span class="comment"># and we first need to compute the policy action before computing q values losses</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([policy_train_op]):</span><br><span class="line">        train_values_op = value_optimizer.minimize(values_losses, var_list=values_params)</span><br><span class="line"></span><br><span class="line">        self.infos_names = [<span class="string">'policy_loss'</span>, <span class="string">'qf1_loss'</span>, <span class="string">'qf2_loss'</span>, <span class="string">'value_loss'</span>, <span class="string">'entropy'</span>]</span><br><span class="line">        <span class="comment"># All ops to call during one training step</span></span><br><span class="line">        self.step_ops = [policy_loss, qf1_loss, qf2_loss,</span><br><span class="line">                         value_loss, qf1, qf2, value_fn, logp_pi,</span><br><span class="line">                         self.entropy, policy_train_op, train_values_op]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add entropy coefficient optimization operation if needed</span></span><br><span class="line">        <span class="keyword">if</span> ent_coef_loss <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">with</span> tf.control_dependencies([train_values_op]):</span><br><span class="line">                ent_coef_op = entropy_optimizer.minimize(ent_coef_loss, var_list=self.log_ent_coef)</span><br><span class="line">                self.infos_names += [<span class="string">'ent_coef_loss'</span>, <span class="string">'ent_coef'</span>]</span><br><span class="line">                self.step_ops += [ent_coef_op, ent_coef_loss, self.ent_coef]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Monitor losses and entropy in tensorboard</span></span><br><span class="line">    tf.summary.scalar(<span class="string">'policy_loss'</span>, policy_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">'qf1_loss'</span>, qf1_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">'qf2_loss'</span>, qf2_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">'value_loss'</span>, value_loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">'entropy'</span>, self.entropy)</span><br><span class="line">    <span class="keyword">if</span> ent_coef_loss <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        tf.summary.scalar(<span class="string">'ent_coef_loss'</span>, ent_coef_loss)</span><br><span class="line">        tf.summary.scalar(<span class="string">'ent_coef'</span>, self.ent_coef)</span><br><span class="line"></span><br><span class="line">    tf.summary.scalar(<span class="string">'learning_rate'</span>, tf.reduce_mean(self.learning_rate_ph))</span><br></pre></td></tr></table></figure>
<p>注：</p>
<p>（1）openAI的SAC版本的critic网络有1个Value network（需要target）和两个Q-value network（不需要target）。</p>
<p>（2）value loss是想要value_fn与当前最小q value的min_qf_pi对应的soft value——v_backup（即在min_qf_pi基础上加上当前时刻动作分布的entropy）接近：*value_loss = 0.5 * tf.reduce_mean((value_fn - v_backup) ** 2)*</p>
<p>（3）Q value loss是想qf接近Q target（通过r+gamma*V_target求出Q_target）：</p>
<p>*qf1_loss = 0.5 * tf.reduce_mean((q_backup - qf1) ** 2)*</p>
<p>（3）这里默认的计算policy loss的方式是最大化Q network1对应的Q值同时最大化动作熵，<em>policy_kl_loss = tf.reduce_mean(self.ent_coef * logp_pi - qf1_pi)</em> 。当然也可以选为最大化min_qf_pi对应的Q值同时最大化动作熵。</p>
<p>（4）计算value loss的梯度时采用的是所有和value相关的loss：<em>values_losses = qf1_loss + qf2_loss + value_loss</em></p>
<p>（5）soft更新target参数的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.target_update_op = [</span><br><span class="line">    tf.assign(target, (<span class="number">1</span> - self.tau) * target + self.tau * source)</span><br><span class="line">    <span class="keyword">for</span> target, source <span class="keyword">in</span> <span class="built_in">zip</span>(target_params, source_params)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Xu Wan
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://waunx.github.io/2022/05/19/DRL-Loss/" title="SB中常见DRL算法的损失函数设置">https://waunx.github.io/2022/05/19/DRL-Loss/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Code/" rel="tag"># Code</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/05/01/QP-1/" rel="prev" title="Barrier Function学习笔记">
      <i class="fa fa-chevron-left"></i> Barrier Function学习笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/05/22/Inference%20Attack-1/" rel="next" title="Inference Attack学习笔记（1）">
      Inference Attack学习笔记（1） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AE%B9%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">内容概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dqn"><span class="nav-number">2.</span> <span class="nav-text">DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ppo"><span class="nav-number">3.</span> <span class="nav-text">PPO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sac"><span class="nav-number">4.</span> <span class="nav-text">SAC</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xu Wan"
      src="/images/photo.jpg">
  <p class="site-author-name" itemprop="name">Xu Wan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
      
      
      <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
      <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
      <div class="widget-wrap">
          <h3 class="widget-title">Tag Cloud</h3>
          <div id="myCanvasContainer" class="widget tagcloud">
              <canvas width="250" height="250" id="resCanvas" style="width=100%">
                  <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Adaptive-DRL/" rel="tag">Adaptive DRL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CBF/" rel="tag">CBF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Code/" rel="tag">Code</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inference-Attack/" rel="tag">Inference Attack</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Robust-DRL/" rel="tag">Robust DRL</a><span class="tag-list-count">5</span></li></ul>
              </canvas>
          </div>
      </div>
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xu Wan</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">56k</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
-->

<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共12.3k字</span>
  <span class="post-meta-divider">|</span>
  本站总访问量<span id="busuanzi_value_site_pv"></span>次
  <span class="post-meta-divider">|</span>
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
  <span class="post-meta-divider">|</span>
  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共12.3k字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
